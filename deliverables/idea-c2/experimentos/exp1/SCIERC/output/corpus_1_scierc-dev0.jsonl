{"id": "A94-1037", "text": " Methods developed for  spelling correction  for  languages  like  English  (see the review by Kukich (Kukich, 1992)) are not readily applicable to  agglutinative languages . This poster presents an approach to  spelling correction  in  agglutinative languages  that is based on  two-level morphology  and a  dynamic-programming based search algorithm . After an overview of our approach, we present results from experiments with  spelling correction  in  Turkish . ", "Comments": [], "entities": [{"id": 1089171, "label": "ENT", "start_offset": 1, "end_offset": 8}, {"id": 1089172, "label": "ENT", "start_offset": 24, "end_offset": 43}, {"id": 1089173, "label": "ENT", "start_offset": 50, "end_offset": 59}, {"id": 1089174, "label": "ENT", "start_offset": 67, "end_offset": 74}, {"id": 1089175, "label": "ENT", "start_offset": 149, "end_offset": 172}, {"id": 1089176, "label": "ENT", "start_offset": 199, "end_offset": 207}, {"id": 1089177, "label": "ENT", "start_offset": 212, "end_offset": 231}, {"id": 1089178, "label": "ENT", "start_offset": 237, "end_offset": 260}, {"id": 1089179, "label": "ENT", "start_offset": 280, "end_offset": 300}, {"id": 1089180, "label": "ENT", "start_offset": 309, "end_offset": 351}, {"id": 1089181, "label": "ENT", "start_offset": 379, "end_offset": 387}, {"id": 1089182, "label": "ENT", "start_offset": 431, "end_offset": 450}, {"id": 1089183, "label": "ENT", "start_offset": 456, "end_offset": 463}], "relations": [{"id": 173171, "from_id": 1089172, "to_id": 1089173, "type": "USED-FOR"}, {"id": 173172, "from_id": 1089178, "to_id": 1089177, "type": "USED-FOR"}, {"id": 173173, "from_id": 1089183, "to_id": 1089182, "type": "USED-FOR"}, {"id": 173174, "from_id": 1089174, "to_id": 1089173, "type": "HYPONYM-OF"}, {"id": 173175, "from_id": 1089177, "to_id": 1089172, "type": "COREF"}, {"id": 173176, "from_id": 1089179, "to_id": 1089177, "type": "USED-FOR"}, {"id": 173177, "from_id": 1089180, "to_id": 1089177, "type": "USED-FOR"}, {"id": 173178, "from_id": 1089171, "to_id": 1089172, "type": "USED-FOR"}, {"id": 173179, "from_id": 1089176, "to_id": 1089177, "type": "USED-FOR"}, {"id": 173180, "from_id": 1089179, "to_id": 1089180, "type": "CONJUNCTION"}, {"id": 173181, "from_id": 1089181, "to_id": 1089176, "type": "COREF"}, {"id": 173182, "from_id": 1089177, "to_id": 1089182, "type": "COREF"}]}
{"id": "CVPR_2007_21_abs", "text": "This paper describes our work on classification of outdoor scenes. First, images are partitioned into regions using one-class classification and patch-based clustering algorithms where one-class classifiers model the regions with relatively uniform color and texture properties, and clustering of patches aims to detect structures in the remaining regions. Next, the resulting regions are clustered to obtain a codebook of region types, and two models are constructed for scene representation: a \" bag of individual regions \" representation where each region is regarded separately , and a \" bag of region pairs \" representation where regions with particular spatial relationships are considered together. Given these representations, scene classification is done using Bayesian classifiers. We also propose a novel region selection algorithm that identifies region types that are frequently found in a particular class of scenes but rarely exist in other classes, and also consistently occur together in the same class of scenes. Experiments on the LabelMe data set showed that the proposed models significantly out-perform a baseline global feature-based approach.", "Comments": [], "entities": [{"id": 1089184, "label": "ENT", "start_offset": 33, "end_offset": 65}, {"id": 1089185, "label": "ENT", "start_offset": 116, "end_offset": 140}, {"id": 1089186, "label": "ENT", "start_offset": 145, "end_offset": 178}, {"id": 1089187, "label": "ENT", "start_offset": 185, "end_offset": 206}, {"id": 1089188, "label": "ENT", "start_offset": 241, "end_offset": 277}, {"id": 1089189, "label": "ENT", "start_offset": 283, "end_offset": 304}, {"id": 1089190, "label": "ENT", "start_offset": 411, "end_offset": 435}, {"id": 1089191, "label": "ENT", "start_offset": 445, "end_offset": 451}, {"id": 1089192, "label": "ENT", "start_offset": 472, "end_offset": 492}, {"id": 1089193, "label": "ENT", "start_offset": 659, "end_offset": 680}, {"id": 1089194, "label": "ENT", "start_offset": 735, "end_offset": 755}, {"id": 1089195, "label": "ENT", "start_offset": 770, "end_offset": 790}, {"id": 1089196, "label": "ENT", "start_offset": 816, "end_offset": 842}, {"id": 1089197, "label": "ENT", "start_offset": 1050, "end_offset": 1066}, {"id": 1089198, "label": "ENT", "start_offset": 1092, "end_offset": 1098}, {"id": 1089199, "label": "ENT", "start_offset": 1127, "end_offset": 1165}], "relations": [{"id": 173183, "from_id": 1089186, "to_id": 1089185, "type": "CONJUNCTION"}, {"id": 173184, "from_id": 1089188, "to_id": 1089187, "type": "USED-FOR"}, {"id": 173185, "from_id": 1089191, "to_id": 1089192, "type": "USED-FOR"}, {"id": 173186, "from_id": 1089195, "to_id": 1089194, "type": "USED-FOR"}, {"id": 173187, "from_id": 1089198, "to_id": 1089199, "type": "COMPARE"}, {"id": 173188, "from_id": 1089197, "to_id": 1089198, "type": "EVALUATE-FOR"}, {"id": 173189, "from_id": 1089198, "to_id": 1089196, "type": "COREF"}, {"id": 173190, "from_id": 1089197, "to_id": 1089199, "type": "EVALUATE-FOR"}]}
{"id": "INTERSPEECH_2013_21_abs", "text": "This work proposes a new research direction to address the lack of structures in traditional n-gram models. It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus. Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge. Bayesian inference then samples the rules, disambiguating and combining them to create complex tree structures that maximize a discriminative model's posterior on a target unlabeled corpus. This posterior encodes sparse se-lectional preferences between a head word and its dependents. The model is evaluated on English and Czech newspaper texts, and is then validated on French broadcast news transcriptions.", "Comments": [], "entities": [{"id": 1089200, "label": "ENT", "start_offset": 59, "end_offset": 77}, {"id": 1089201, "label": "ENT", "start_offset": 93, "end_offset": 106}, {"id": 1089202, "label": "ENT", "start_offset": 125, "end_offset": 160}, {"id": 1089203, "label": "ENT", "start_offset": 176, "end_offset": 189}, {"id": 1089204, "label": "ENT", "start_offset": 213, "end_offset": 238}, {"id": 1089205, "label": "ENT", "start_offset": 240, "end_offset": 252}, {"id": 1089206, "label": "ENT", "start_offset": 274, "end_offset": 292}, {"id": 1089207, "label": "ENT", "start_offset": 311, "end_offset": 330}, {"id": 1089208, "label": "ENT", "start_offset": 332, "end_offset": 350}, {"id": 1089209, "label": "ENT", "start_offset": 368, "end_offset": 373}, {"id": 1089210, "label": "ENT", "start_offset": 404, "end_offset": 408}, {"id": 1089211, "label": "ENT", "start_offset": 419, "end_offset": 442}, {"id": 1089212, "label": "ENT", "start_offset": 459, "end_offset": 491}, {"id": 1089213, "label": "ENT", "start_offset": 504, "end_offset": 520}, {"id": 1089214, "label": "ENT", "start_offset": 527, "end_offset": 536}, {"id": 1089215, "label": "ENT", "start_offset": 545, "end_offset": 576}, {"id": 1089216, "label": "ENT", "start_offset": 621, "end_offset": 626}, {"id": 1089217, "label": "ENT", "start_offset": 643, "end_offset": 676}, {"id": 1089218, "label": "ENT", "start_offset": 703, "end_offset": 739}], "relations": [{"id": 173191, "from_id": 1089202, "to_id": 1089203, "type": "USED-FOR"}, {"id": 173192, "from_id": 1089206, "to_id": 1089207, "type": "USED-FOR"}, {"id": 173193, "from_id": 1089209, "to_id": 1089210, "type": "COREF"}, {"id": 173194, "from_id": 1089210, "to_id": 1089211, "type": "USED-FOR"}, {"id": 173195, "from_id": 1089208, "to_id": 1089209, "type": "USED-FOR"}, {"id": 173196, "from_id": 1089211, "to_id": 1089212, "type": "USED-FOR"}, {"id": 173197, "from_id": 1089213, "to_id": 1089212, "type": "USED-FOR"}, {"id": 173198, "from_id": 1089212, "to_id": 1089214, "type": "COREF"}, {"id": 173199, "from_id": 1089214, "to_id": 1089215, "type": "USED-FOR"}, {"id": 173200, "from_id": 1089217, "to_id": 1089216, "type": "EVALUATE-FOR"}, {"id": 173201, "from_id": 1089218, "to_id": 1089216, "type": "EVALUATE-FOR"}]}
{"id": "C90-2032", "text": "   This paper proposes  document oriented preference sets(DoPS)  for the disambiguation of the  dependency structure  of  sentences  . The  DoPS system  extracts preference knowledge from a  target document  or other  documents  automatically.  Sentence ambiguities  can be resolved by using domain targeted preference knowledge without using complicated large  knowledgebases  .  Implementation  and  empirical results  are described for the the analysis of  dependency structures  of  Japanese patent claim sentences  . ", "Comments": [], "entities": [{"id": 1089219, "label": "ENT", "start_offset": 24, "end_offset": 63}, {"id": 1089220, "label": "ENT", "start_offset": 73, "end_offset": 116}, {"id": 1089221, "label": "ENT", "start_offset": 140, "end_offset": 151}, {"id": 1089222, "label": "ENT", "start_offset": 245, "end_offset": 265}, {"id": 1089223, "label": "ENT", "start_offset": 292, "end_offset": 328}, {"id": 1089224, "label": "ENT", "start_offset": 362, "end_offset": 376}, {"id": 1089225, "label": "ENT", "start_offset": 460, "end_offset": 481}, {"id": 1089226, "label": "ENT", "start_offset": 487, "end_offset": 518}], "relations": [{"id": 173202, "from_id": 1089225, "to_id": 1089226, "type": "FEATURE-OF"}, {"id": 173203, "from_id": 1089221, "to_id": 1089219, "type": "COREF"}, {"id": 173204, "from_id": 1089223, "to_id": 1089222, "type": "USED-FOR"}, {"id": 173205, "from_id": 1089223, "to_id": 1089224, "type": "COMPARE"}, {"id": 173206, "from_id": 1089219, "to_id": 1089220, "type": "USED-FOR"}]}
{"id": "C88-1044", "text": "   This paper presents necessary and sufficient conditions for the use of  demonstrative expressions  in  English  and discusses implications for current  discourse processing algorithms  . We examine a broad range of  texts  to show how the distribution of  demonstrative forms and functions  is  genre dependent  . This research is part of a larger study of  anaphoric expressions  , the results of which will be incorporated into a  natural language generation system  . ", "Comments": [], "entities": [{"id": 1089240, "label": "ENT", "start_offset": 75, "end_offset": 100}, {"id": 1089241, "label": "ENT", "start_offset": 106, "end_offset": 113}, {"id": 1089242, "label": "ENT", "start_offset": 129, "end_offset": 141}, {"id": 1089243, "label": "ENT", "start_offset": 155, "end_offset": 186}, {"id": 1089244, "label": "ENT", "start_offset": 259, "end_offset": 292}, {"id": 1089245, "label": "ENT", "start_offset": 361, "end_offset": 382}, {"id": 1089246, "label": "ENT", "start_offset": 436, "end_offset": 470}], "relations": [{"id": 173219, "from_id": 1089240, "to_id": 1089241, "type": "FEATURE-OF"}, {"id": 173220, "from_id": 1089245, "to_id": 1089246, "type": "USED-FOR"}, {"id": 173221, "from_id": 1089242, "to_id": 1089243, "type": "USED-FOR"}]}
{"id": "P05-3001", "text": " We describe a  dialogue system  that works with its interlocutor to identify objects. Our contributions include a concise,  modular architecture  with reversible processes of  understanding  and  generation , an  information-state model of reference , and flexible links between  semantics  and  collaborative problem solving . ", "Comments": [], "entities": [{"id": 1089247, "label": "ENT", "start_offset": 16, "end_offset": 31}, {"id": 1089248, "label": "ENT", "start_offset": 115, "end_offset": 145}, {"id": 1089249, "label": "ENT", "start_offset": 177, "end_offset": 190}, {"id": 1089250, "label": "ENT", "start_offset": 197, "end_offset": 207}, {"id": 1089251, "label": "ENT", "start_offset": 214, "end_offset": 250}, {"id": 1089252, "label": "ENT", "start_offset": 281, "end_offset": 290}, {"id": 1089253, "label": "ENT", "start_offset": 297, "end_offset": 326}], "relations": [{"id": 173222, "from_id": 1089249, "to_id": 1089250, "type": "CONJUNCTION"}, {"id": 173223, "from_id": 1089248, "to_id": 1089249, "type": "USED-FOR"}, {"id": 173224, "from_id": 1089248, "to_id": 1089250, "type": "USED-FOR"}]}
{"id": "P84-1020", "text": "   This abstract describes a  natural language system  which deals usefully with  ungrammatical input  and describes some actual and potential applications of it in  computer aided second language learning  . However, this is not the only area in which the principles of the system might be used, and the aim in building it was simply to demonstrate the workability of the general mechanism, and provide a framework for assessing developments of it. ", "Comments": [], "entities": [{"id": 1089254, "label": "ENT", "start_offset": 30, "end_offset": 53}, {"id": 1089255, "label": "ENT", "start_offset": 82, "end_offset": 101}, {"id": 1089256, "label": "ENT", "start_offset": 159, "end_offset": 161}, {"id": 1089257, "label": "ENT", "start_offset": 166, "end_offset": 205}, {"id": 1089258, "label": "ENT", "start_offset": 218, "end_offset": 222}, {"id": 1089259, "label": "ENT", "start_offset": 275, "end_offset": 281}, {"id": 1089260, "label": "ENT", "start_offset": 321, "end_offset": 323}, {"id": 1089261, "label": "ENT", "start_offset": 446, "end_offset": 448}], "relations": [{"id": 173225, "from_id": 1089254, "to_id": 1089255, "type": "USED-FOR"}, {"id": 173226, "from_id": 1089254, "to_id": 1089256, "type": "COREF"}, {"id": 173227, "from_id": 1089256, "to_id": 1089257, "type": "USED-FOR"}, {"id": 173228, "from_id": 1089254, "to_id": 1089259, "type": "COREF"}, {"id": 173229, "from_id": 1089257, "to_id": 1089258, "type": "COREF"}, {"id": 173230, "from_id": 1089259, "to_id": 1089258, "type": "USED-FOR"}, {"id": 173231, "from_id": 1089259, "to_id": 1089260, "type": "COREF"}, {"id": 173232, "from_id": 1089260, "to_id": 1089261, "type": "COREF"}]}
{"id": "NIPS_1996_15_abs", "text": "We have calculated analytical expressions for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with offline updates over trials in absorbing Markov chains using lookup table representations. We illustrate classes of learning curve behavior in various chains, and show the manner in which TD is sensitive to the choice of its step-size and eligibility trace parameters.", "Comments": [], "entities": [{"id": 1089262, "label": "ENT", "start_offset": 19, "end_offset": 41}, {"id": 1089263, "label": "ENT", "start_offset": 110, "end_offset": 157}, {"id": 1089264, "label": "ENT", "start_offset": 211, "end_offset": 224}, {"id": 1089265, "label": "ENT", "start_offset": 231, "end_offset": 259}, {"id": 1089266, "label": "ENT", "start_offset": 286, "end_offset": 309}, {"id": 1089267, "label": "ENT", "start_offset": 395, "end_offset": 437}], "relations": [{"id": 173233, "from_id": 1089265, "to_id": 1089262, "type": "USED-FOR"}]}
{"id": "ECCV_2012_30_abs", "text": "This paper considers the problem of reconstructing the motion of a 3D articulated tree from 2D point correspondences subject to some temporal prior. Hitherto, smooth motion has been encouraged using a trajectory basis, yielding a hard combinatorial problem with time complexity growing exponentially in the number of frames. Branch and bound strategies have previously attempted to curb this complexity whilst maintaining global optimality. However, they provide no guarantee of being more efficient than exhaustive search. Inspired by recent work which reconstructs general trajectories using compact high-pass filters, we develop a dynamic programming approach which scales linearly in the number of frames, leveraging the intrinsically local nature of filter interactions. Extension to affine projection enables reconstruction without estimating cameras.", "Comments": [], "entities": [{"id": 1089341, "label": "ENT", "start_offset": 36, "end_offset": 86}, {"id": 1089342, "label": "ENT", "start_offset": 92, "end_offset": 116}, {"id": 1089343, "label": "ENT", "start_offset": 133, "end_offset": 147}, {"id": 1089344, "label": "ENT", "start_offset": 159, "end_offset": 172}, {"id": 1089345, "label": "ENT", "start_offset": 201, "end_offset": 217}, {"id": 1089346, "label": "ENT", "start_offset": 230, "end_offset": 256}, {"id": 1089347, "label": "ENT", "start_offset": 262, "end_offset": 277}, {"id": 1089348, "label": "ENT", "start_offset": 325, "end_offset": 352}, {"id": 1089349, "label": "ENT", "start_offset": 392, "end_offset": 402}, {"id": 1089350, "label": "ENT", "start_offset": 422, "end_offset": 439}, {"id": 1089351, "label": "ENT", "start_offset": 450, "end_offset": 454}, {"id": 1089352, "label": "ENT", "start_offset": 505, "end_offset": 522}, {"id": 1089353, "label": "ENT", "start_offset": 594, "end_offset": 619}, {"id": 1089354, "label": "ENT", "start_offset": 634, "end_offset": 662}, {"id": 1089355, "label": "ENT", "start_offset": 755, "end_offset": 774}, {"id": 1089356, "label": "ENT", "start_offset": 789, "end_offset": 806}, {"id": 1089357, "label": "ENT", "start_offset": 815, "end_offset": 829}, {"id": 1089358, "label": "ENT", "start_offset": 838, "end_offset": 856}], "relations": [{"id": 173293, "from_id": 1089345, "to_id": 1089344, "type": "USED-FOR"}, {"id": 173294, "from_id": 1089348, "to_id": 1089349, "type": "USED-FOR"}, {"id": 173295, "from_id": 1089349, "to_id": 1089347, "type": "COREF"}, {"id": 173296, "from_id": 1089348, "to_id": 1089351, "type": "COREF"}, {"id": 173297, "from_id": 1089351, "to_id": 1089352, "type": "COMPARE"}, {"id": 173298, "from_id": 1089356, "to_id": 1089357, "type": "USED-FOR"}, {"id": 173299, "from_id": 1089350, "to_id": 1089348, "type": "FEATURE-OF"}, {"id": 173300, "from_id": 1089347, "to_id": 1089346, "type": "EVALUATE-FOR"}, {"id": 173301, "from_id": 1089342, "to_id": 1089341, "type": "USED-FOR"}, {"id": 173302, "from_id": 1089341, "to_id": 1089357, "type": "COREF"}]}
{"id": "J88-3002", "text": "   For  intelligent interactive systems  to communicate with  humans  in a natural manner, they must have knowledge about the  system users  . This paper explores the role of  user modeling  in such  systems  . It begins with a characterization of what a  user model  is and how it can be used. The types of information that a  user model  may be required to keep about a  user  are then identified and discussed.  User models  themselves can vary greatly depending on the requirements of the situation and the implementation, so several dimensions along which they can be classified are presented. Since acquiring the knowledge for a  user model  is a fundamental problem in  user modeling  , a section is devoted to this topic. Next, the benefits and costs of implementing a  user modeling component  for a system are weighed in light of several aspects of the  interaction requirements  that may be imposed by the system. Finally, the current state of research in  user modeling  is summarized, and future research topics that must be addressed in order to achieve powerful, general  user modeling systems  are assessed. ", "Comments": [], "entities": [{"id": 1089359, "label": "ENT", "start_offset": 8, "end_offset": 39}, {"id": 1089360, "label": "ENT", "start_offset": 91, "end_offset": 95}, {"id": 1089361, "label": "ENT", "start_offset": 176, "end_offset": 189}, {"id": 1089362, "label": "ENT", "start_offset": 200, "end_offset": 207}, {"id": 1089363, "label": "ENT", "start_offset": 256, "end_offset": 266}, {"id": 1089364, "label": "ENT", "start_offset": 279, "end_offset": 281}, {"id": 1089365, "label": "ENT", "start_offset": 328, "end_offset": 338}, {"id": 1089366, "label": "ENT", "start_offset": 415, "end_offset": 426}, {"id": 1089367, "label": "ENT", "start_offset": 561, "end_offset": 565}, {"id": 1089368, "label": "ENT", "start_offset": 636, "end_offset": 646}, {"id": 1089369, "label": "ENT", "start_offset": 677, "end_offset": 690}, {"id": 1089370, "label": "ENT", "start_offset": 778, "end_offset": 801}, {"id": 1089371, "label": "ENT", "start_offset": 809, "end_offset": 815}, {"id": 1089372, "label": "ENT", "start_offset": 917, "end_offset": 923}, {"id": 1089373, "label": "ENT", "start_offset": 968, "end_offset": 981}, {"id": 1089374, "label": "ENT", "start_offset": 1087, "end_offset": 1108}], "relations": [{"id": 173303, "from_id": 1089361, "to_id": 1089362, "type": "PART-OF"}, {"id": 173304, "from_id": 1089360, "to_id": 1089359, "type": "COREF"}, {"id": 173305, "from_id": 1089362, "to_id": 1089359, "type": "COREF"}, {"id": 173306, "from_id": 1089365, "to_id": 1089363, "type": "COREF"}, {"id": 173307, "from_id": 1089366, "to_id": 1089365, "type": "COREF"}, {"id": 173308, "from_id": 1089367, "to_id": 1089366, "type": "COREF"}, {"id": 173309, "from_id": 1089368, "to_id": 1089367, "type": "COREF"}, {"id": 173310, "from_id": 1089372, "to_id": 1089371, "type": "COREF"}, {"id": 173311, "from_id": 1089370, "to_id": 1089371, "type": "PART-OF"}, {"id": 173312, "from_id": 1089364, "to_id": 1089363, "type": "COREF"}, {"id": 173313, "from_id": 1089368, "to_id": 1089369, "type": "USED-FOR"}, {"id": 173314, "from_id": 1089369, "to_id": 1089361, "type": "COREF"}, {"id": 173315, "from_id": 1089373, "to_id": 1089369, "type": "COREF"}, {"id": 173316, "from_id": 1089374, "to_id": 1089368, "type": "COREF"}]}
{"id": "P02-1060", "text": " This paper proposes a  Hidden Markov Model (HMM)  and an  HMM-based chunk tagger , from which a  named entity (NE) recognition (NER) system  is built to recognize and classify  names ,  times and numerical quantities . Through the  HMM , our system is able to apply and integrate four types of internal and external evidences : 1) simple deterministic internal feature of the  words , such as  capitalization  and digitalization ; 2)  internal semantic feature  of important triggers ; 3)  internal gazetteer feature ; 4)  external macro context feature . In this way, the  NER problem  can be resolved effectively. Evaluation of our  system  on  MUC-6 and MUC-7 English NE tasks  achieves  F-measures  of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other  machine-learning system . Moreover, the  performance  is even consistently better than those based on  handcrafted rules . ", "Comments": [], "entities": [{"id": 1089414, "label": "ENT", "start_offset": 24, "end_offset": 49}, {"id": 1089415, "label": "ENT", "start_offset": 59, "end_offset": 81}, {"id": 1089416, "label": "ENT", "start_offset": 98, "end_offset": 140}, {"id": 1089417, "label": "ENT", "start_offset": 178, "end_offset": 183}, {"id": 1089418, "label": "ENT", "start_offset": 187, "end_offset": 217}, {"id": 1089419, "label": "ENT", "start_offset": 233, "end_offset": 236}, {"id": 1089420, "label": "ENT", "start_offset": 339, "end_offset": 383}, {"id": 1089421, "label": "ENT", "start_offset": 395, "end_offset": 409}, {"id": 1089422, "label": "ENT", "start_offset": 415, "end_offset": 429}, {"id": 1089423, "label": "ENT", "start_offset": 491, "end_offset": 517}, {"id": 1089424, "label": "ENT", "start_offset": 524, "end_offset": 554}, {"id": 1089425, "label": "ENT", "start_offset": 575, "end_offset": 586}, {"id": 1089426, "label": "ENT", "start_offset": 636, "end_offset": 642}, {"id": 1089427, "label": "ENT", "start_offset": 648, "end_offset": 680}, {"id": 1089428, "label": "ENT", "start_offset": 692, "end_offset": 702}, {"id": 1089429, "label": "ENT", "start_offset": 819, "end_offset": 842}, {"id": 1089430, "label": "ENT", "start_offset": 922, "end_offset": 939}], "relations": [{"id": 173349, "from_id": 1089415, "to_id": 1089416, "type": "USED-FOR"}, {"id": 173350, "from_id": 1089414, "to_id": 1089416, "type": "USED-FOR"}, {"id": 173351, "from_id": 1089414, "to_id": 1089415, "type": "CONJUNCTION"}, {"id": 173352, "from_id": 1089414, "to_id": 1089419, "type": "COREF"}, {"id": 173353, "from_id": 1089422, "to_id": 1089420, "type": "HYPONYM-OF"}, {"id": 173354, "from_id": 1089421, "to_id": 1089420, "type": "HYPONYM-OF"}, {"id": 173355, "from_id": 1089421, "to_id": 1089422, "type": "CONJUNCTION"}, {"id": 173356, "from_id": 1089428, "to_id": 1089426, "type": "EVALUATE-FOR"}, {"id": 173357, "from_id": 1089427, "to_id": 1089426, "type": "EVALUATE-FOR"}, {"id": 173358, "from_id": 1089416, "to_id": 1089426, "type": "COREF"}, {"id": 173359, "from_id": 1089416, "to_id": 1089417, "type": "USED-FOR"}, {"id": 173360, "from_id": 1089416, "to_id": 1089418, "type": "USED-FOR"}, {"id": 173361, "from_id": 1089417, "to_id": 1089418, "type": "CONJUNCTION"}]}
{"id": "E91-1043", "text": " In this paper I will argue for a  model of grammatical processing  that is based on  uniform processing  and  knowledge sources . The main  feature  of this model is to view  parsing  and  generation  as two strongly interleaved tasks performed by a single  parametrized deduction  process. It will be shown that this view supports flexible and efficient  natural language processing . ", "Comments": [], "entities": [{"id": 1089467, "label": "ENT", "start_offset": 35, "end_offset": 66}, {"id": 1089468, "label": "ENT", "start_offset": 86, "end_offset": 104}, {"id": 1089469, "label": "ENT", "start_offset": 111, "end_offset": 128}, {"id": 1089470, "label": "ENT", "start_offset": 158, "end_offset": 163}, {"id": 1089471, "label": "ENT", "start_offset": 176, "end_offset": 183}, {"id": 1089472, "label": "ENT", "start_offset": 190, "end_offset": 200}, {"id": 1089473, "label": "ENT", "start_offset": 230, "end_offset": 235}, {"id": 1089474, "label": "ENT", "start_offset": 259, "end_offset": 290}, {"id": 1089475, "label": "ENT", "start_offset": 357, "end_offset": 384}], "relations": [{"id": 173387, "from_id": 1089468, "to_id": 1089467, "type": "USED-FOR"}, {"id": 173388, "from_id": 1089471, "to_id": 1089472, "type": "CONJUNCTION"}, {"id": 173389, "from_id": 1089469, "to_id": 1089467, "type": "USED-FOR"}, {"id": 173390, "from_id": 1089469, "to_id": 1089468, "type": "CONJUNCTION"}, {"id": 173391, "from_id": 1089470, "to_id": 1089467, "type": "COREF"}, {"id": 173392, "from_id": 1089472, "to_id": 1089473, "type": "HYPONYM-OF"}, {"id": 173393, "from_id": 1089471, "to_id": 1089473, "type": "HYPONYM-OF"}, {"id": 173394, "from_id": 1089474, "to_id": 1089473, "type": "USED-FOR"}]}
{"id": "CVPR_2006_11_abs", "text": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD's. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.", "Comments": [], "entities": [{"id": 1089476, "label": "ENT", "start_offset": 2, "end_offset": 20}, {"id": 1089477, "label": "ENT", "start_offset": 92, "end_offset": 102}, {"id": 1089478, "label": "ENT", "start_offset": 107, "end_offset": 114}, {"id": 1089479, "label": "ENT", "start_offset": 168, "end_offset": 177}, {"id": 1089480, "label": "ENT", "start_offset": 185, "end_offset": 193}, {"id": 1089481, "label": "ENT", "start_offset": 203, "end_offset": 231}, {"id": 1089482, "label": "ENT", "start_offset": 237, "end_offset": 243}, {"id": 1089483, "label": "ENT", "start_offset": 278, "end_offset": 298}, {"id": 1089484, "label": "ENT", "start_offset": 314, "end_offset": 327}, {"id": 1089485, "label": "ENT", "start_offset": 346, "end_offset": 364}, {"id": 1089486, "label": "ENT", "start_offset": 369, "end_offset": 378}, {"id": 1089487, "label": "ENT", "start_offset": 384, "end_offset": 408}, {"id": 1089488, "label": "ENT", "start_offset": 443, "end_offset": 458}, {"id": 1089489, "label": "ENT", "start_offset": 464, "end_offset": 479}, {"id": 1089490, "label": "ENT", "start_offset": 619, "end_offset": 636}, {"id": 1089491, "label": "ENT", "start_offset": 694, "end_offset": 698}, {"id": 1089492, "label": "ENT", "start_offset": 720, "end_offset": 732}, {"id": 1089493, "label": "ENT", "start_offset": 738, "end_offset": 750}, {"id": 1089494, "label": "ENT", "start_offset": 759, "end_offset": 767}, {"id": 1089495, "label": "ENT", "start_offset": 840, "end_offset": 859}, {"id": 1089496, "label": "ENT", "start_offset": 881, "end_offset": 890}, {"id": 1089497, "label": "ENT", "start_offset": 896, "end_offset": 922}, {"id": 1089498, "label": "ENT", "start_offset": 949, "end_offset": 973}], "relations": [{"id": 173395, "from_id": 1089477, "to_id": 1089478, "type": "CONJUNCTION"}, {"id": 173396, "from_id": 1089476, "to_id": 1089482, "type": "COREF"}, {"id": 173397, "from_id": 1089483, "to_id": 1089482, "type": "USED-FOR"}, {"id": 173398, "from_id": 1089484, "to_id": 1089483, "type": "USED-FOR"}, {"id": 173399, "from_id": 1089488, "to_id": 1089487, "type": "USED-FOR"}, {"id": 173400, "from_id": 1089488, "to_id": 1089489, "type": "COREF"}, {"id": 173401, "from_id": 1089493, "to_id": 1089494, "type": "CONJUNCTION"}, {"id": 173402, "from_id": 1089497, "to_id": 1089496, "type": "USED-FOR"}, {"id": 173403, "from_id": 1089495, "to_id": 1089498, "type": "EVALUATE-FOR"}, {"id": 173404, "from_id": 1089496, "to_id": 1089495, "type": "EVALUATE-FOR"}, {"id": 173405, "from_id": 1089481, "to_id": 1089480, "type": "FEATURE-OF"}, {"id": 173406, "from_id": 1089482, "to_id": 1089485, "type": "USED-FOR"}, {"id": 173407, "from_id": 1089482, "to_id": 1089486, "type": "USED-FOR"}, {"id": 173408, "from_id": 1089485, "to_id": 1089486, "type": "CONJUNCTION"}]}
{"id": "ICCV_2013_33_abs", "text": "This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they \" attract \" people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or \" repel \" people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as \" dark matter \" , emanating \" dark energy \" that affects people's trajectories in the video. To detect \" dark matter \" and infer their \" dark energy \" field, we extend the La-grangian mechanics. People are treated as particle-agents with latent intents to approach \" dark matter \" and thus satisfy their needs, where their motions are subject to a composite \" dark energy \" field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended \" dark matter \" while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people's trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people's trajectories in unobserved parts of the video footage.", "Comments": [], "entities": [{"id": 1089548, "label": "ENT", "start_offset": 23, "end_offset": 31}, {"id": 1089549, "label": "ENT", "start_offset": 35, "end_offset": 64}, {"id": 1089550, "label": "ENT", "start_offset": 68, "end_offset": 87}, {"id": 1089551, "label": "ENT", "start_offset": 96, "end_offset": 112}, {"id": 1089552, "label": "ENT", "start_offset": 119, "end_offset": 142}, {"id": 1089553, "label": "ENT", "start_offset": 173, "end_offset": 191}, {"id": 1089554, "label": "ENT", "start_offset": 204, "end_offset": 239}, {"id": 1089555, "label": "ENT", "start_offset": 245, "end_offset": 249}, {"id": 1089556, "label": "ENT", "start_offset": 485, "end_offset": 503}, {"id": 1089557, "label": "ENT", "start_offset": 693, "end_offset": 714}, {"id": 1089558, "label": "ENT", "start_offset": 908, "end_offset": 926}, {"id": 1089559, "label": "ENT", "start_offset": 1075, "end_offset": 1093}, {"id": 1089560, "label": "ENT", "start_offset": 1130, "end_offset": 1163}, {"id": 1089561, "label": "ENT", "start_offset": 1165, "end_offset": 1192}, {"id": 1089562, "label": "ENT", "start_offset": 1198, "end_offset": 1229}, {"id": 1089563, "label": "ENT", "start_offset": 1233, "end_offset": 1284}, {"id": 1089564, "label": "ENT", "start_offset": 1297, "end_offset": 1306}, {"id": 1089565, "label": "ENT", "start_offset": 1326, "end_offset": 1365}, {"id": 1089566, "label": "ENT", "start_offset": 1400, "end_offset": 1429}, {"id": 1089567, "label": "ENT", "start_offset": 1434, "end_offset": 1466}, {"id": 1089568, "label": "ENT", "start_offset": 1494, "end_offset": 1507}], "relations": [{"id": 173455, "from_id": 1089550, "to_id": 1089549, "type": "USED-FOR"}, {"id": 173456, "from_id": 1089548, "to_id": 1089549, "type": "USED-FOR"}, {"id": 173457, "from_id": 1089552, "to_id": 1089551, "type": "FEATURE-OF"}, {"id": 173458, "from_id": 1089555, "to_id": 1089553, "type": "COREF"}, {"id": 173459, "from_id": 1089559, "to_id": 1089560, "type": "USED-FOR"}, {"id": 173460, "from_id": 1089559, "to_id": 1089561, "type": "USED-FOR"}, {"id": 173461, "from_id": 1089559, "to_id": 1089562, "type": "USED-FOR"}, {"id": 173462, "from_id": 1089560, "to_id": 1089561, "type": "CONJUNCTION"}, {"id": 173463, "from_id": 1089561, "to_id": 1089562, "type": "CONJUNCTION"}, {"id": 173464, "from_id": 1089563, "to_id": 1089564, "type": "USED-FOR"}, {"id": 173465, "from_id": 1089565, "to_id": 1089566, "type": "EVALUATE-FOR"}, {"id": 173466, "from_id": 1089565, "to_id": 1089567, "type": "EVALUATE-FOR"}, {"id": 173467, "from_id": 1089566, "to_id": 1089567, "type": "CONJUNCTION"}, {"id": 173468, "from_id": 1089566, "to_id": 1089549, "type": "COREF"}, {"id": 173469, "from_id": 1089548, "to_id": 1089559, "type": "COREF"}]}
{"id": "C04-1011", "text": " We consider the problem of computing the  Kullback-Leibler distance , also called the  relative entropy , between a  probabilistic context-free grammar  and a  probabilistic finite automaton . We show that there is a  closed-form (analytical) solution  for one part of the  Kullback-Leibler distance , viz the  cross-entropy . We discuss several applications of the result to the problem of  distributional approximation  of  probabilistic context-free grammars  by means of  probabilistic finite automata . ", "Comments": [], "entities": [{"id": 1089579, "label": "ENT", "start_offset": 43, "end_offset": 68}, {"id": 1089580, "label": "ENT", "start_offset": 88, "end_offset": 104}, {"id": 1089581, "label": "ENT", "start_offset": 118, "end_offset": 152}, {"id": 1089582, "label": "ENT", "start_offset": 161, "end_offset": 191}, {"id": 1089583, "label": "ENT", "start_offset": 219, "end_offset": 252}, {"id": 1089584, "label": "ENT", "start_offset": 275, "end_offset": 300}, {"id": 1089585, "label": "ENT", "start_offset": 312, "end_offset": 325}, {"id": 1089586, "label": "ENT", "start_offset": 393, "end_offset": 421}, {"id": 1089587, "label": "ENT", "start_offset": 427, "end_offset": 462}, {"id": 1089588, "label": "ENT", "start_offset": 477, "end_offset": 506}], "relations": [{"id": 173479, "from_id": 1089581, "to_id": 1089582, "type": "COMPARE"}, {"id": 173480, "from_id": 1089585, "to_id": 1089584, "type": "PART-OF"}, {"id": 173481, "from_id": 1089588, "to_id": 1089586, "type": "USED-FOR"}, {"id": 173482, "from_id": 1089580, "to_id": 1089579, "type": "COREF"}, {"id": 173483, "from_id": 1089588, "to_id": 1089582, "type": "COREF"}, {"id": 173484, "from_id": 1089583, "to_id": 1089584, "type": "USED-FOR"}, {"id": 173485, "from_id": 1089583, "to_id": 1089585, "type": "USED-FOR"}, {"id": 173486, "from_id": 1089584, "to_id": 1089579, "type": "COREF"}, {"id": 173487, "from_id": 1089586, "to_id": 1089587, "type": "FEATURE-OF"}, {"id": 173488, "from_id": 1089587, "to_id": 1089581, "type": "COREF"}]}
{"id": "CVPR_2003_10_abs", "text": "A \" graphics for vision \" approach is proposed to address the problem of reconstruction from a large and imperfect data set: reconstruction on demand by tensor voting, or ROD-TV. ROD-TV simultaneously delivers good efficiency and robust-ness, by adapting to a continuum of primitive connectivity, view dependence, and levels of detail (LOD). Locally inferred surface elements are robust to noise and better capture local shapes. By inferring per-vertex normals at sub-voxel precision on the fly, we can achieve interpolative shading. Since these missing details can be recovered at the current level of detail, our result is not upper bounded by the scanning resolution. By relaxing the mesh connectivity requirement, we extend ROD-TV and propose a simple but effective multiscale feature extraction algorithm. ROD-TV consists of a hierarchical data structure that encodes different levels of detail. The local reconstruction algorithm is tensor voting. It is applied on demand to the visible subset of data at a desired level of detail , by traversing the data hierarchy and collecting tensorial support in a neighborhood. We compare our approach and present encouraging results.", "Comments": [], "entities": [{"id": 1089589, "label": "ENT", "start_offset": 2, "end_offset": 34}, {"id": 1089590, "label": "ENT", "start_offset": 73, "end_offset": 87}, {"id": 1089591, "label": "ENT", "start_offset": 95, "end_offset": 123}, {"id": 1089592, "label": "ENT", "start_offset": 125, "end_offset": 139}, {"id": 1089593, "label": "ENT", "start_offset": 153, "end_offset": 166}, {"id": 1089594, "label": "ENT", "start_offset": 171, "end_offset": 177}, {"id": 1089595, "label": "ENT", "start_offset": 179, "end_offset": 185}, {"id": 1089596, "label": "ENT", "start_offset": 215, "end_offset": 225}, {"id": 1089597, "label": "ENT", "start_offset": 230, "end_offset": 241}, {"id": 1089598, "label": "ENT", "start_offset": 273, "end_offset": 295}, {"id": 1089599, "label": "ENT", "start_offset": 297, "end_offset": 312}, {"id": 1089600, "label": "ENT", "start_offset": 318, "end_offset": 340}, {"id": 1089601, "label": "ENT", "start_offset": 342, "end_offset": 375}, {"id": 1089602, "label": "ENT", "start_offset": 390, "end_offset": 395}, {"id": 1089603, "label": "ENT", "start_offset": 415, "end_offset": 427}, {"id": 1089604, "label": "ENT", "start_offset": 442, "end_offset": 460}, {"id": 1089605, "label": "ENT", "start_offset": 464, "end_offset": 483}, {"id": 1089606, "label": "ENT", "start_offset": 511, "end_offset": 532}, {"id": 1089607, "label": "ENT", "start_offset": 650, "end_offset": 669}, {"id": 1089608, "label": "ENT", "start_offset": 687, "end_offset": 716}, {"id": 1089609, "label": "ENT", "start_offset": 728, "end_offset": 734}, {"id": 1089610, "label": "ENT", "start_offset": 770, "end_offset": 809}, {"id": 1089611, "label": "ENT", "start_offset": 811, "end_offset": 817}, {"id": 1089612, "label": "ENT", "start_offset": 832, "end_offset": 859}, {"id": 1089613, "label": "ENT", "start_offset": 905, "end_offset": 935}, {"id": 1089614, "label": "ENT", "start_offset": 939, "end_offset": 952}, {"id": 1089615, "label": "ENT", "start_offset": 954, "end_offset": 956}, {"id": 1089616, "label": "ENT", "start_offset": 1042, "end_offset": 1071}, {"id": 1089617, "label": "ENT", "start_offset": 1076, "end_offset": 1104}, {"id": 1089618, "label": "ENT", "start_offset": 1139, "end_offset": 1147}], "relations": [{"id": 173489, "from_id": 1089593, "to_id": 1089592, "type": "USED-FOR"}, {"id": 173490, "from_id": 1089594, "to_id": 1089592, "type": "USED-FOR"}, {"id": 173491, "from_id": 1089597, "to_id": 1089596, "type": "CONJUNCTION"}, {"id": 173492, "from_id": 1089599, "to_id": 1089598, "type": "CONJUNCTION"}, {"id": 173493, "from_id": 1089601, "to_id": 1089603, "type": "USED-FOR"}, {"id": 173494, "from_id": 1089597, "to_id": 1089595, "type": "EVALUATE-FOR"}, {"id": 173495, "from_id": 1089604, "to_id": 1089606, "type": "USED-FOR"}, {"id": 173496, "from_id": 1089608, "to_id": 1089610, "type": "USED-FOR"}, {"id": 173497, "from_id": 1089609, "to_id": 1089610, "type": "USED-FOR"}, {"id": 173498, "from_id": 1089612, "to_id": 1089611, "type": "PART-OF"}, {"id": 173499, "from_id": 1089614, "to_id": 1089613, "type": "HYPONYM-OF"}, {"id": 173500, "from_id": 1089591, "to_id": 1089590, "type": "USED-FOR"}, {"id": 173501, "from_id": 1089593, "to_id": 1089594, "type": "CONJUNCTION"}, {"id": 173502, "from_id": 1089595, "to_id": 1089594, "type": "COREF"}, {"id": 173503, "from_id": 1089600, "to_id": 1089599, "type": "CONJUNCTION"}, {"id": 173504, "from_id": 1089596, "to_id": 1089595, "type": "EVALUATE-FOR"}, {"id": 173505, "from_id": 1089609, "to_id": 1089595, "type": "COREF"}, {"id": 173506, "from_id": 1089611, "to_id": 1089609, "type": "COREF"}, {"id": 173507, "from_id": 1089616, "to_id": 1089615, "type": "USED-FOR"}, {"id": 173508, "from_id": 1089617, "to_id": 1089615, "type": "USED-FOR"}, {"id": 173509, "from_id": 1089616, "to_id": 1089617, "type": "CONJUNCTION"}, {"id": 173510, "from_id": 1089589, "to_id": 1089590, "type": "USED-FOR"}, {"id": 173511, "from_id": 1089589, "to_id": 1089618, "type": "COREF"}, {"id": 173512, "from_id": 1089590, "to_id": 1089592, "type": "COREF"}, {"id": 173513, "from_id": 1089614, "to_id": 1089615, "type": "COREF"}, {"id": 173514, "from_id": 1089605, "to_id": 1089604, "type": "FEATURE-OF"}]}
{"id": "NIPS_2014_21_abs", "text": "In real-world action recognition problems, low-level features cannot adequately characterize the rich spatial-temporal structures in action videos. In this work, we encode actions based on attributes that describes actions as high-level concepts e.g., jump forward or motion in the air. We base our analysis on two types of action attributes. One type of action attributes is generated by humans. The second type is data-driven attributes, which are learned from data using dictionary learning methods. Attribute-based representation may exhibit high variance due to noisy and redundant attributes. We propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set. Three attribute selection criteria are proposed and formulated as a submodular optimization problem. A greedy optimization algorithm is presented and guaranteed to be at least (1-1/e)-approximation to the optimum. Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches.", "Comments": [], "entities": [{"id": 1089619, "label": "ENT", "start_offset": 3, "end_offset": 41}, {"id": 1089620, "label": "ENT", "start_offset": 43, "end_offset": 61}, {"id": 1089621, "label": "ENT", "start_offset": 97, "end_offset": 129}, {"id": 1089622, "label": "ENT", "start_offset": 133, "end_offset": 146}, {"id": 1089623, "label": "ENT", "start_offset": 226, "end_offset": 245}, {"id": 1089624, "label": "ENT", "start_offset": 324, "end_offset": 341}, {"id": 1089625, "label": "ENT", "start_offset": 355, "end_offset": 372}, {"id": 1089626, "label": "ENT", "start_offset": 416, "end_offset": 438}, {"id": 1089627, "label": "ENT", "start_offset": 474, "end_offset": 501}, {"id": 1089628, "label": "ENT", "start_offset": 503, "end_offset": 533}, {"id": 1089629, "label": "ENT", "start_offset": 567, "end_offset": 597}, {"id": 1089630, "label": "ENT", "start_offset": 612, "end_offset": 669}, {"id": 1089631, "label": "ENT", "start_offset": 695, "end_offset": 720}, {"id": 1089632, "label": "ENT", "start_offset": 755, "end_offset": 783}, {"id": 1089633, "label": "ENT", "start_offset": 817, "end_offset": 848}, {"id": 1089634, "label": "ENT", "start_offset": 852, "end_offset": 881}, {"id": 1089635, "label": "ENT", "start_offset": 991, "end_offset": 1025}, {"id": 1089636, "label": "ENT", "start_offset": 1056, "end_offset": 1086}, {"id": 1089637, "label": "ENT", "start_offset": 1130, "end_offset": 1159}, {"id": 1089638, "label": "ENT", "start_offset": 1198, "end_offset": 1220}], "relations": [{"id": 173515, "from_id": 1089621, "to_id": 1089622, "type": "FEATURE-OF"}, {"id": 173516, "from_id": 1089633, "to_id": 1089632, "type": "USED-FOR"}, {"id": 173517, "from_id": 1089635, "to_id": 1089636, "type": "EVALUATE-FOR"}, {"id": 173518, "from_id": 1089627, "to_id": 1089626, "type": "USED-FOR"}, {"id": 173519, "from_id": 1089631, "to_id": 1089630, "type": "USED-FOR"}, {"id": 173520, "from_id": 1089636, "to_id": 1089637, "type": "USED-FOR"}, {"id": 173521, "from_id": 1089637, "to_id": 1089638, "type": "COMPARE"}, {"id": 173522, "from_id": 1089636, "to_id": 1089628, "type": "COREF"}]}
{"id": "NIPS_2015_18_abs", "text": "Many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions. For example, nucleotides in a DNA sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions. In all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the DNA strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic. These dependencies are not naturally captured by the typical Dirichlet-multinomial formulation. Here, we leverage a logistic stick-breaking representation and recent innovations in P\u00f3lya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods, enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead.", "Comments": [], "entities": [{"id": 1089639, "label": "ENT", "start_offset": 15, "end_offset": 32}, {"id": 1089640, "label": "ENT", "start_offset": 41, "end_offset": 54}, {"id": 1089641, "label": "ENT", "start_offset": 95, "end_offset": 135}, {"id": 1089642, "label": "ENT", "start_offset": 150, "end_offset": 179}, {"id": 1089643, "label": "ENT", "start_offset": 229, "end_offset": 243}, {"id": 1089644, "label": "ENT", "start_offset": 274, "end_offset": 299}, {"id": 1089645, "label": "ENT", "start_offset": 381, "end_offset": 391}, {"id": 1089646, "label": "ENT", "start_offset": 415, "end_offset": 425}, {"id": 1089647, "label": "ENT", "start_offset": 444, "end_offset": 465}, {"id": 1089648, "label": "ENT", "start_offset": 636, "end_offset": 669}, {"id": 1089649, "label": "ENT", "start_offset": 691, "end_offset": 729}, {"id": 1089650, "label": "ENT", "start_offset": 756, "end_offset": 780}, {"id": 1089651, "label": "ENT", "start_offset": 801, "end_offset": 825}, {"id": 1089652, "label": "ENT", "start_offset": 838, "end_offset": 854}, {"id": 1089653, "label": "ENT", "start_offset": 860, "end_offset": 888}, {"id": 1089654, "label": "ENT", "start_offset": 933, "end_offset": 962}, {"id": 1089655, "label": "ENT", "start_offset": 967, "end_offset": 982}, {"id": 1089656, "label": "ENT", "start_offset": 988, "end_offset": 1004}], "relations": [{"id": 173523, "from_id": 1089644, "to_id": 1089641, "type": "HYPONYM-OF"}, {"id": 173524, "from_id": 1089644, "to_id": 1089642, "type": "USED-FOR"}, {"id": 173525, "from_id": 1089644, "to_id": 1089643, "type": "USED-FOR"}, {"id": 173526, "from_id": 1089654, "to_id": 1089655, "type": "USED-FOR"}, {"id": 173527, "from_id": 1089656, "to_id": 1089655, "type": "FEATURE-OF"}, {"id": 173528, "from_id": 1089650, "to_id": 1089651, "type": "USED-FOR"}, {"id": 173529, "from_id": 1089649, "to_id": 1089651, "type": "USED-FOR"}, {"id": 173530, "from_id": 1089653, "to_id": 1089652, "type": "FEATURE-OF"}, {"id": 173531, "from_id": 1089652, "to_id": 1089651, "type": "PART-OF"}, {"id": 173532, "from_id": 1089651, "to_id": 1089644, "type": "COREF"}, {"id": 173533, "from_id": 1089640, "to_id": 1089639, "type": "USED-FOR"}, {"id": 173534, "from_id": 1089641, "to_id": 1089639, "type": "USED-FOR"}]}
{"id": "ICASSP_2016_11_abs", "text": "The problem of blind separation of underdetermined instantaneous mixtures of independent signals is addressed through a method relying on nonstationarity of the original signals. The signals are assumed to be piecewise stationary with varying variances in different epochs. In comparison with previous works, in this paper it is assumed that the signals are not i.i.d. in each epoch, but obey a first-order autoregressive model. This model was shown to be more appropriate for blind separation of natural speech signals. A separation method is proposed that is nearly statistically efficient (approaching the corresponding Cram\u00e9r-Rao lower bound), if the separated signals obey the assumed model. In the case of natural speech signals , the method is shown to have separation accuracy better than the state-of-the-art methods.", "Comments": [], "entities": [{"id": 1089657, "label": "ENT", "start_offset": 15, "end_offset": 96}, {"id": 1089658, "label": "ENT", "start_offset": 120, "end_offset": 126}, {"id": 1089659, "label": "ENT", "start_offset": 138, "end_offset": 153}, {"id": 1089660, "label": "ENT", "start_offset": 161, "end_offset": 177}, {"id": 1089661, "label": "ENT", "start_offset": 183, "end_offset": 190}, {"id": 1089662, "label": "ENT", "start_offset": 346, "end_offset": 353}, {"id": 1089663, "label": "ENT", "start_offset": 395, "end_offset": 427}, {"id": 1089664, "label": "ENT", "start_offset": 434, "end_offset": 439}, {"id": 1089665, "label": "ENT", "start_offset": 477, "end_offset": 520}, {"id": 1089666, "label": "ENT", "start_offset": 523, "end_offset": 540}, {"id": 1089667, "label": "ENT", "start_offset": 623, "end_offset": 646}, {"id": 1089668, "label": "ENT", "start_offset": 682, "end_offset": 695}, {"id": 1089669, "label": "ENT", "start_offset": 712, "end_offset": 734}, {"id": 1089670, "label": "ENT", "start_offset": 741, "end_offset": 747}, {"id": 1089671, "label": "ENT", "start_offset": 765, "end_offset": 784}, {"id": 1089672, "label": "ENT", "start_offset": 818, "end_offset": 825}], "relations": [{"id": 173535, "from_id": 1089659, "to_id": 1089658, "type": "USED-FOR"}, {"id": 173536, "from_id": 1089661, "to_id": 1089660, "type": "COREF"}, {"id": 173537, "from_id": 1089658, "to_id": 1089657, "type": "USED-FOR"}, {"id": 173538, "from_id": 1089662, "to_id": 1089661, "type": "COREF"}, {"id": 173539, "from_id": 1089663, "to_id": 1089662, "type": "USED-FOR"}, {"id": 173540, "from_id": 1089664, "to_id": 1089663, "type": "COREF"}, {"id": 173541, "from_id": 1089664, "to_id": 1089665, "type": "USED-FOR"}, {"id": 173542, "from_id": 1089670, "to_id": 1089672, "type": "COMPARE"}, {"id": 173543, "from_id": 1089671, "to_id": 1089670, "type": "EVALUATE-FOR"}, {"id": 173544, "from_id": 1089671, "to_id": 1089672, "type": "EVALUATE-FOR"}, {"id": 173545, "from_id": 1089670, "to_id": 1089669, "type": "USED-FOR"}, {"id": 173546, "from_id": 1089672, "to_id": 1089669, "type": "USED-FOR"}, {"id": 173547, "from_id": 1089670, "to_id": 1089666, "type": "COREF"}, {"id": 173548, "from_id": 1089664, "to_id": 1089668, "type": "COREF"}, {"id": 173549, "from_id": 1089667, "to_id": 1089666, "type": "FEATURE-OF"}]}
{"id": "W02-1602", "text": "  Coedition  of a  natural language text  and its representation in some  interlingual form  seems the best and simplest way to share  text revision  across  languages . For various reasons,  UNL graphs  are the best candidates in this context. We are developing a  prototype  where, in the simplest  sharing scenario , naive users interact directly with the  text  in their  language (L0) , and indirectly with the associated  graph . The modified  graph  is then sent to the  UNL-L0 deconverter  and the result shown. If is is satisfactory, the errors were probably due to the  graph , not to the  deconverter , and the  graph  is sent to  deconverters  in other  languages . Versions in some other  languages  known by the user may be displayed, so that improvement sharing is visible and encouraging. As new versions are added with appropriate  tags  and  attributes  in the  original multilingual document , nothing is ever lost, and cooperative working on a  document  is rendered feasible. On the internal side, liaisons are established between elements of the  text  and the  graph  by using broadly available resources such as a  LO-English or better a L0-UNL dictionary , a  morphosyntactic parser of L0 , and a  canonical graph2tree transformation . Establishing a \"best\" correspondence between the \" UNL-tree+L0 \" and the \" MS-L0 structure \", a  lattice , may be done using the  dictionary  and trying to align the  tree  and the selected  trajectory  with as few  crossing liaisons  as possible. A central goal of this research is to merge approaches from  pivot MT ,  interactive MT , and  multilingual text authoring . ", "Comments": [], "entities": [{"id": 1089691, "label": "ENT", "start_offset": 2, "end_offset": 11}, {"id": 1089692, "label": "ENT", "start_offset": 19, "end_offset": 40}, {"id": 1089693, "label": "ENT", "start_offset": 135, "end_offset": 148}, {"id": 1089694, "label": "ENT", "start_offset": 158, "end_offset": 167}, {"id": 1089695, "label": "ENT", "start_offset": 192, "end_offset": 202}, {"id": 1089696, "label": "ENT", "start_offset": 428, "end_offset": 433}, {"id": 1089697, "label": "ENT", "start_offset": 450, "end_offset": 455}, {"id": 1089698, "label": "ENT", "start_offset": 478, "end_offset": 496}, {"id": 1089699, "label": "ENT", "start_offset": 580, "end_offset": 585}, {"id": 1089700, "label": "ENT", "start_offset": 600, "end_offset": 611}, {"id": 1089701, "label": "ENT", "start_offset": 623, "end_offset": 628}, {"id": 1089702, "label": "ENT", "start_offset": 642, "end_offset": 654}, {"id": 1089703, "label": "ENT", "start_offset": 880, "end_offset": 910}, {"id": 1089704, "label": "ENT", "start_offset": 1019, "end_offset": 1027}, {"id": 1089705, "label": "ENT", "start_offset": 1118, "end_offset": 1127}, {"id": 1089706, "label": "ENT", "start_offset": 1139, "end_offset": 1179}, {"id": 1089707, "label": "ENT", "start_offset": 1185, "end_offset": 1213}, {"id": 1089708, "label": "ENT", "start_offset": 1223, "end_offset": 1258}, {"id": 1089709, "label": "ENT", "start_offset": 1312, "end_offset": 1323}, {"id": 1089710, "label": "ENT", "start_offset": 1336, "end_offset": 1351}, {"id": 1089711, "label": "ENT", "start_offset": 1358, "end_offset": 1365}, {"id": 1089712, "label": "ENT", "start_offset": 1391, "end_offset": 1401}, {"id": 1089713, "label": "ENT", "start_offset": 1477, "end_offset": 1494}, {"id": 1089714, "label": "ENT", "start_offset": 1570, "end_offset": 1578}, {"id": 1089715, "label": "ENT", "start_offset": 1582, "end_offset": 1596}, {"id": 1089716, "label": "ENT", "start_offset": 1604, "end_offset": 1631}], "relations": [{"id": 173566, "from_id": 1089691, "to_id": 1089693, "type": "USED-FOR"}, {"id": 173567, "from_id": 1089706, "to_id": 1089707, "type": "CONJUNCTION"}, {"id": 173568, "from_id": 1089707, "to_id": 1089708, "type": "CONJUNCTION"}, {"id": 173569, "from_id": 1089709, "to_id": 1089710, "type": "CONJUNCTION"}, {"id": 173570, "from_id": 1089697, "to_id": 1089698, "type": "USED-FOR"}, {"id": 173571, "from_id": 1089692, "to_id": 1089691, "type": "USED-FOR"}, {"id": 173572, "from_id": 1089698, "to_id": 1089700, "type": "COREF"}, {"id": 173573, "from_id": 1089700, "to_id": 1089702, "type": "COREF"}, {"id": 173574, "from_id": 1089715, "to_id": 1089716, "type": "CONJUNCTION"}, {"id": 173575, "from_id": 1089714, "to_id": 1089715, "type": "CONJUNCTION"}, {"id": 173576, "from_id": 1089706, "to_id": 1089705, "type": "HYPONYM-OF"}, {"id": 173577, "from_id": 1089707, "to_id": 1089705, "type": "HYPONYM-OF"}, {"id": 173578, "from_id": 1089708, "to_id": 1089705, "type": "HYPONYM-OF"}, {"id": 173579, "from_id": 1089705, "to_id": 1089704, "type": "USED-FOR"}, {"id": 173580, "from_id": 1089712, "to_id": 1089711, "type": "USED-FOR"}]}
{"id": "J82-3002", "text": "   This paper gives an overall account of a prototype  natural language question answering system  , called  Chat-80  .  Chat-80  has been designed to be both efficient and easily adaptable to a variety of applications. The system is implemented entirely in  Prolog  , a  programming language  based on  logic  . With the aid of a  logic-based grammar formalism  called  extraposition grammars  ,  Chat-80  translates  English questions  into the  Prolog   subset of logic  . The resulting  logical expression  is then transformed by a  planning algorithm  into efficient  Prolog  , cf.  query optimisation  in a  relational database  . Finally, the  Prolog form  is executed to yield the answer. ", "Comments": [], "entities": [{"id": 1089717, "label": "ENT", "start_offset": 55, "end_offset": 97}, {"id": 1089718, "label": "ENT", "start_offset": 109, "end_offset": 116}, {"id": 1089719, "label": "ENT", "start_offset": 121, "end_offset": 128}, {"id": 1089720, "label": "ENT", "start_offset": 224, "end_offset": 230}, {"id": 1089721, "label": "ENT", "start_offset": 259, "end_offset": 265}, {"id": 1089722, "label": "ENT", "start_offset": 272, "end_offset": 292}, {"id": 1089723, "label": "ENT", "start_offset": 304, "end_offset": 309}, {"id": 1089724, "label": "ENT", "start_offset": 332, "end_offset": 361}, {"id": 1089725, "label": "ENT", "start_offset": 371, "end_offset": 393}, {"id": 1089726, "label": "ENT", "start_offset": 398, "end_offset": 405}, {"id": 1089727, "label": "ENT", "start_offset": 448, "end_offset": 472}, {"id": 1089728, "label": "ENT", "start_offset": 491, "end_offset": 509}, {"id": 1089729, "label": "ENT", "start_offset": 537, "end_offset": 555}, {"id": 1089730, "label": "ENT", "start_offset": 573, "end_offset": 579}, {"id": 1089731, "label": "ENT", "start_offset": 588, "end_offset": 606}, {"id": 1089732, "label": "ENT", "start_offset": 614, "end_offset": 633}, {"id": 1089733, "label": "ENT", "start_offset": 651, "end_offset": 662}], "relations": [{"id": 173581, "from_id": 1089723, "to_id": 1089722, "type": "USED-FOR"}, {"id": 173582, "from_id": 1089718, "to_id": 1089717, "type": "HYPONYM-OF"}, {"id": 173583, "from_id": 1089719, "to_id": 1089718, "type": "COREF"}, {"id": 173584, "from_id": 1089720, "to_id": 1089719, "type": "COREF"}, {"id": 173585, "from_id": 1089721, "to_id": 1089720, "type": "USED-FOR"}, {"id": 173586, "from_id": 1089721, "to_id": 1089722, "type": "HYPONYM-OF"}, {"id": 173587, "from_id": 1089725, "to_id": 1089724, "type": "HYPONYM-OF"}, {"id": 173588, "from_id": 1089726, "to_id": 1089720, "type": "COREF"}, {"id": 173589, "from_id": 1089729, "to_id": 1089728, "type": "USED-FOR"}, {"id": 173590, "from_id": 1089732, "to_id": 1089731, "type": "USED-FOR"}, {"id": 173591, "from_id": 1089725, "to_id": 1089726, "type": "USED-FOR"}]}
{"id": "I05-5004", "text": " Towards deep analysis of  compositional classes of paraphrases , we have examined a  class-oriented framework  for collecting  paraphrase examples , in which  sentential paraphrases  are collected for each  paraphrase class  separately by means of  automatic candidate generation  and  manual judgement . Our preliminary experiments on building a  paraphrase corpus  have so far been producing promising results, which we have evaluated according to  cost-efficiency ,  exhaustiveness , and  reliability . ", "Comments": [], "entities": [{"id": 1089767, "label": "ENT", "start_offset": 27, "end_offset": 63}, {"id": 1089768, "label": "ENT", "start_offset": 86, "end_offset": 110}, {"id": 1089769, "label": "ENT", "start_offset": 128, "end_offset": 147}, {"id": 1089770, "label": "ENT", "start_offset": 160, "end_offset": 182}, {"id": 1089771, "label": "ENT", "start_offset": 250, "end_offset": 280}, {"id": 1089772, "label": "ENT", "start_offset": 287, "end_offset": 303}, {"id": 1089773, "label": "ENT", "start_offset": 349, "end_offset": 366}], "relations": [{"id": 173629, "from_id": 1089768, "to_id": 1089769, "type": "USED-FOR"}, {"id": 173630, "from_id": 1089771, "to_id": 1089772, "type": "CONJUNCTION"}, {"id": 173631, "from_id": 1089768, "to_id": 1089767, "type": "USED-FOR"}, {"id": 173632, "from_id": 1089771, "to_id": 1089770, "type": "USED-FOR"}, {"id": 173633, "from_id": 1089772, "to_id": 1089770, "type": "USED-FOR"}, {"id": 173634, "from_id": 1089770, "to_id": 1089769, "type": "COREF"}]}
{"id": "I05-2021", "text": " We present the first known  empirical test  of an increasingly common speculative claim, by evaluating a representative  Chinese-to-English SMT model  directly on  word sense disambiguation performance  , using standard  WSD evaluation methodology  and  datasets  from the  Senseval-3 Chinese lexical sample task  . Much effort has been put in designing and evaluating dedicated  word sense disambiguation (WSD) models  , in particular with the  Senseval  series of workshops. At the same time, the recent improvements in the  BLEU scores  of  statistical machine translation (SMT)  suggests that  SMT models  are good at predicting the right  translation  of the  words  in  source language sentences  . Surprisingly however, the  WSD   accuracy  of  SMT models  has never been evaluated and compared with that of the dedicated  WSD models  . We present controlled experiments showing the  WSD   accuracy  of current typical  SMT models  to be significantly lower than that of all the dedicated  WSD models  considered. This tends to support the view that despite recent speculative claims to the contrary, current  SMT models  do have limitations in comparison with dedicated  WSD models  , and that  SMT  should benefit from the better predictions made by the  WSD models  . ", "Comments": [], "entities": [{"id": 1089774, "label": "ENT", "start_offset": 122, "end_offset": 150}, {"id": 1089775, "label": "ENT", "start_offset": 165, "end_offset": 190}, {"id": 1089776, "label": "ENT", "start_offset": 222, "end_offset": 248}, {"id": 1089777, "label": "ENT", "start_offset": 275, "end_offset": 313}, {"id": 1089778, "label": "ENT", "start_offset": 370, "end_offset": 419}, {"id": 1089779, "label": "ENT", "start_offset": 447, "end_offset": 476}, {"id": 1089780, "label": "ENT", "start_offset": 528, "end_offset": 539}, {"id": 1089781, "label": "ENT", "start_offset": 545, "end_offset": 582}, {"id": 1089782, "label": "ENT", "start_offset": 599, "end_offset": 609}, {"id": 1089783, "label": "ENT", "start_offset": 645, "end_offset": 656}, {"id": 1089784, "label": "ENT", "start_offset": 733, "end_offset": 747}, {"id": 1089785, "label": "ENT", "start_offset": 753, "end_offset": 763}, {"id": 1089786, "label": "ENT", "start_offset": 808, "end_offset": 812}, {"id": 1089787, "label": "ENT", "start_offset": 820, "end_offset": 841}, {"id": 1089788, "label": "ENT", "start_offset": 892, "end_offset": 906}, {"id": 1089789, "label": "ENT", "start_offset": 928, "end_offset": 938}, {"id": 1089790, "label": "ENT", "start_offset": 971, "end_offset": 975}, {"id": 1089791, "label": "ENT", "start_offset": 987, "end_offset": 1008}, {"id": 1089792, "label": "ENT", "start_offset": 1118, "end_offset": 1128}, {"id": 1089793, "label": "ENT", "start_offset": 1169, "end_offset": 1190}, {"id": 1089794, "label": "ENT", "start_offset": 1204, "end_offset": 1207}, {"id": 1089795, "label": "ENT", "start_offset": 1265, "end_offset": 1275}], "relations": [{"id": 173635, "from_id": 1089775, "to_id": 1089774, "type": "EVALUATE-FOR"}, {"id": 173636, "from_id": 1089780, "to_id": 1089781, "type": "EVALUATE-FOR"}, {"id": 173637, "from_id": 1089782, "to_id": 1089783, "type": "USED-FOR"}, {"id": 173638, "from_id": 1089794, "to_id": 1089792, "type": "COREF"}, {"id": 173639, "from_id": 1089792, "to_id": 1089789, "type": "COREF"}, {"id": 173640, "from_id": 1089789, "to_id": 1089785, "type": "COREF"}, {"id": 173641, "from_id": 1089777, "to_id": 1089774, "type": "EVALUATE-FOR"}, {"id": 173642, "from_id": 1089776, "to_id": 1089774, "type": "EVALUATE-FOR"}, {"id": 173643, "from_id": 1089784, "to_id": 1089785, "type": "EVALUATE-FOR"}, {"id": 173644, "from_id": 1089785, "to_id": 1089782, "type": "COREF"}, {"id": 173645, "from_id": 1089788, "to_id": 1089789, "type": "EVALUATE-FOR"}, {"id": 173646, "from_id": 1089787, "to_id": 1089791, "type": "COREF"}, {"id": 173647, "from_id": 1089793, "to_id": 1089791, "type": "COREF"}, {"id": 173648, "from_id": 1089792, "to_id": 1089793, "type": "COMPARE"}, {"id": 173649, "from_id": 1089795, "to_id": 1089794, "type": "USED-FOR"}, {"id": 173650, "from_id": 1089779, "to_id": 1089778, "type": "EVALUATE-FOR"}, {"id": 173651, "from_id": 1089787, "to_id": 1089778, "type": "COREF"}, {"id": 173652, "from_id": 1089795, "to_id": 1089793, "type": "COREF"}, {"id": 173653, "from_id": 1089790, "to_id": 1089788, "type": "COMPARE"}, {"id": 173654, "from_id": 1089786, "to_id": 1089784, "type": "COMPARE"}]}
{"id": "P01-1008", "text": " While  paraphrasing  is critical both for  interpretation and generation of natural language  , current systems use manual or semi-automatic methods to collect  paraphrases  . We present an  unsupervised learning algorithm  for  identification of paraphrases  from a  corpus of multiple English translations  of the same  source text  . Our approach yields  phrasal and single word lexical paraphrases  as well as  syntactic paraphrases  . ", "Comments": [], "entities": [{"id": 1089829, "label": "ENT", "start_offset": 8, "end_offset": 20}, {"id": 1089830, "label": "ENT", "start_offset": 44, "end_offset": 93}, {"id": 1089831, "label": "ENT", "start_offset": 105, "end_offset": 112}, {"id": 1089832, "label": "ENT", "start_offset": 117, "end_offset": 149}, {"id": 1089833, "label": "ENT", "start_offset": 162, "end_offset": 173}, {"id": 1089834, "label": "ENT", "start_offset": 192, "end_offset": 223}, {"id": 1089835, "label": "ENT", "start_offset": 230, "end_offset": 259}, {"id": 1089836, "label": "ENT", "start_offset": 269, "end_offset": 308}, {"id": 1089837, "label": "ENT", "start_offset": 342, "end_offset": 350}, {"id": 1089838, "label": "ENT", "start_offset": 359, "end_offset": 402}, {"id": 1089839, "label": "ENT", "start_offset": 416, "end_offset": 437}], "relations": [{"id": 173675, "from_id": 1089829, "to_id": 1089830, "type": "USED-FOR"}, {"id": 173676, "from_id": 1089834, "to_id": 1089835, "type": "USED-FOR"}, {"id": 173677, "from_id": 1089832, "to_id": 1089831, "type": "USED-FOR"}, {"id": 173678, "from_id": 1089831, "to_id": 1089833, "type": "USED-FOR"}, {"id": 173679, "from_id": 1089837, "to_id": 1089838, "type": "USED-FOR"}, {"id": 173680, "from_id": 1089837, "to_id": 1089839, "type": "USED-FOR"}, {"id": 173681, "from_id": 1089838, "to_id": 1089839, "type": "CONJUNCTION"}, {"id": 173682, "from_id": 1089836, "to_id": 1089835, "type": "USED-FOR"}, {"id": 173683, "from_id": 1089834, "to_id": 1089837, "type": "COREF"}]}
{"id": "IJCAI_2016_423_abs", "text": "Learning video representation is not a trivial task, as video is an information-intensive media where each frame does not exist independently. Locally, a video frame is visually and semantically similar with its adjacent frames. Holistically, a video has its inherent structure\u2014the correlations among video frames. For example, even the frames far from each other may also hold similar semantics. Such context information is therefore important to characterize the intrinsic representation of a video frame. In this paper, we present a novel approach to learn the deep video representation by exploring both local and holistic contexts. Specifically, we propose a triplet sampling mechanism to encode the local temporal relationship of adjacent frames based on their deep representations. In addition, we incorporate the graph structure of the video, as a priori, to holistically preserve the inherent correlations among video frames. Our approach is fully unsupervised and trained in an end-to-end deep convolutional neu-ral network architecture. By extensive experiments, we show that our learned representation can significantly boost several video recognition tasks (retrieval , classification, and highlight detection) over traditional video representations.", "Comments": [], "entities": [{"id": 1089883, "label": "ENT", "start_offset": 0, "end_offset": 29}, {"id": 1089884, "label": "ENT", "start_offset": 386, "end_offset": 395}, {"id": 1089885, "label": "ENT", "start_offset": 402, "end_offset": 421}, {"id": 1089886, "label": "ENT", "start_offset": 465, "end_offset": 506}, {"id": 1089887, "label": "ENT", "start_offset": 542, "end_offset": 550}, {"id": 1089888, "label": "ENT", "start_offset": 564, "end_offset": 589}, {"id": 1089889, "label": "ENT", "start_offset": 608, "end_offset": 635}, {"id": 1089890, "label": "ENT", "start_offset": 664, "end_offset": 690}, {"id": 1089891, "label": "ENT", "start_offset": 705, "end_offset": 751}, {"id": 1089892, "label": "ENT", "start_offset": 767, "end_offset": 787}, {"id": 1089893, "label": "ENT", "start_offset": 821, "end_offset": 849}, {"id": 1089894, "label": "ENT", "start_offset": 856, "end_offset": 862}, {"id": 1089895, "label": "ENT", "start_offset": 939, "end_offset": 947}, {"id": 1089896, "label": "ENT", "start_offset": 988, "end_offset": 1046}, {"id": 1089897, "label": "ENT", "start_offset": 1091, "end_offset": 1113}, {"id": 1089898, "label": "ENT", "start_offset": 1146, "end_offset": 1169}, {"id": 1089899, "label": "ENT", "start_offset": 1171, "end_offset": 1180}, {"id": 1089900, "label": "ENT", "start_offset": 1183, "end_offset": 1197}, {"id": 1089901, "label": "ENT", "start_offset": 1203, "end_offset": 1222}, {"id": 1089902, "label": "ENT", "start_offset": 1241, "end_offset": 1262}], "relations": [{"id": 173716, "from_id": 1089885, "to_id": 1089886, "type": "USED-FOR"}, {"id": 173717, "from_id": 1089887, "to_id": 1089888, "type": "USED-FOR"}, {"id": 173718, "from_id": 1089889, "to_id": 1089887, "type": "USED-FOR"}, {"id": 173719, "from_id": 1089890, "to_id": 1089891, "type": "USED-FOR"}, {"id": 173720, "from_id": 1089893, "to_id": 1089894, "type": "USED-FOR"}, {"id": 173721, "from_id": 1089887, "to_id": 1089895, "type": "COREF"}, {"id": 173722, "from_id": 1089896, "to_id": 1089895, "type": "USED-FOR"}, {"id": 173723, "from_id": 1089899, "to_id": 1089898, "type": "HYPONYM-OF"}, {"id": 173724, "from_id": 1089900, "to_id": 1089898, "type": "HYPONYM-OF"}, {"id": 173725, "from_id": 1089901, "to_id": 1089898, "type": "HYPONYM-OF"}, {"id": 173726, "from_id": 1089899, "to_id": 1089900, "type": "CONJUNCTION"}, {"id": 173727, "from_id": 1089900, "to_id": 1089901, "type": "CONJUNCTION"}, {"id": 173728, "from_id": 1089897, "to_id": 1089902, "type": "COMPARE"}, {"id": 173729, "from_id": 1089898, "to_id": 1089897, "type": "EVALUATE-FOR"}, {"id": 173730, "from_id": 1089898, "to_id": 1089902, "type": "EVALUATE-FOR"}, {"id": 173731, "from_id": 1089888, "to_id": 1089897, "type": "COREF"}, {"id": 173732, "from_id": 1089888, "to_id": 1089892, "type": "COREF"}, {"id": 173733, "from_id": 1089892, "to_id": 1089890, "type": "USED-FOR"}]}
{"id": "CVPR_1992_10_abs", "text": "This paper presents an algorithm for labeling curvilinear structure at multiple scales in line drawings and edge images Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data. Tokens are computed via a small-to-large scale grouping procedure employing a \" greedy \" , best-first, strategy for choosing the support of new tokens. The resulting image description is rich and redundant in that a given segment of image contour may be described by multiple tokens at different scales, and by more than one token at any given scale. This property facilitates selection and characterization of portions of the image based on local CURVE-ELEMENT attributes. ", "Comments": [], "entities": [{"id": 1089909, "label": "ENT", "start_offset": 23, "end_offset": 32}, {"id": 1089910, "label": "ENT", "start_offset": 37, "end_offset": 67}, {"id": 1089911, "label": "ENT", "start_offset": 90, "end_offset": 103}, {"id": 1089912, "label": "ENT", "start_offset": 108, "end_offset": 119}, {"id": 1089913, "label": "ENT", "start_offset": 129, "end_offset": 149}, {"id": 1089914, "label": "ENT", "start_offset": 164, "end_offset": 214}, {"id": 1089915, "label": "ENT", "start_offset": 243, "end_offset": 253}, {"id": 1089916, "label": "ENT", "start_offset": 281, "end_offset": 320}, {"id": 1089917, "label": "ENT", "start_offset": 421, "end_offset": 438}, {"id": 1089918, "label": "ENT", "start_offset": 488, "end_offset": 501}, {"id": 1089919, "label": "ENT", "start_offset": 697, "end_offset": 727}], "relations": [{"id": 173739, "from_id": 1089909, "to_id": 1089910, "type": "USED-FOR"}, {"id": 173740, "from_id": 1089911, "to_id": 1089910, "type": "FEATURE-OF"}, {"id": 173741, "from_id": 1089912, "to_id": 1089910, "type": "FEATURE-OF"}, {"id": 173742, "from_id": 1089911, "to_id": 1089912, "type": "CONJUNCTION"}, {"id": 173743, "from_id": 1089913, "to_id": 1089914, "type": "PART-OF"}]}
{"id": "ICCV_2013_50_abs", "text": "Regression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives: (1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively. (2) Rather than learning from only labelled data, the abundant unlabelled data are exploited. (3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.", "Comments": [], "entities": [{"id": 1089920, "label": "ENT", "start_offset": 0, "end_offset": 27}, {"id": 1089921, "label": "ENT", "start_offset": 61, "end_offset": 94}, {"id": 1089922, "label": "ENT", "start_offset": 119, "end_offset": 129}, {"id": 1089923, "label": "ENT", "start_offset": 162, "end_offset": 177}, {"id": 1089924, "label": "ENT", "start_offset": 182, "end_offset": 196}, {"id": 1089925, "label": "ENT", "start_offset": 341, "end_offset": 359}, {"id": 1089926, "label": "ENT", "start_offset": 377, "end_offset": 387}, {"id": 1089927, "label": "ENT", "start_offset": 451, "end_offset": 464}, {"id": 1089928, "label": "ENT", "start_offset": 470, "end_offset": 494}, {"id": 1089929, "label": "ENT", "start_offset": 595, "end_offset": 610}, {"id": 1089930, "label": "ENT", "start_offset": 649, "end_offset": 704}, {"id": 1089931, "label": "ENT", "start_offset": 729, "end_offset": 746}, {"id": 1089932, "label": "ENT", "start_offset": 777, "end_offset": 814}, {"id": 1089933, "label": "ENT", "start_offset": 819, "end_offset": 836}, {"id": 1089934, "label": "ENT", "start_offset": 894, "end_offset": 902}], "relations": [{"id": 173744, "from_id": 1089923, "to_id": 1089924, "type": "USED-FOR"}, {"id": 173745, "from_id": 1089923, "to_id": 1089922, "type": "USED-FOR"}, {"id": 173746, "from_id": 1089923, "to_id": 1089926, "type": "COREF"}, {"id": 173747, "from_id": 1089930, "to_id": 1089931, "type": "USED-FOR"}, {"id": 173748, "from_id": 1089930, "to_id": 1089934, "type": "COREF"}, {"id": 173749, "from_id": 1089933, "to_id": 1089932, "type": "USED-FOR"}, {"id": 173750, "from_id": 1089932, "to_id": 1089930, "type": "USED-FOR"}, {"id": 173751, "from_id": 1089926, "to_id": 1089929, "type": "COREF"}, {"id": 173752, "from_id": 1089920, "to_id": 1089921, "type": "USED-FOR"}, {"id": 173753, "from_id": 1089927, "to_id": 1089928, "type": "COMPARE"}]}
{"id": "IJCNLP_2005_3_abs", "text": "Automatic image annotation is a newly developed and promising technique to provide semantic image retrieval via text descriptions. It concerns a process of automatically labeling the image contents with a pre-defined set of keywords which are exploited to represent the image semantics. A Maximum Entropy Model-based approach to the task of automatic image annotation is proposed in this paper. In the phase of training, a basic visual vocabulary consisting of blob-tokens to describe the image content is generated at first; then the statistical relationship is modeled between the blob-tokens and keywords by a Maximum Entropy Model constructed from the training set of labeled images. In the phase of annotation, for an unlabeled image, the most likely associated keywords are predicted in terms of the blob-token set extracted from the given image. We carried out experiments on a medium-sized image collection with about 5000 images from Corel Photo CDs. The experimental results demonstrated that the annotation performance of this method outperforms some traditional annotation methods by about 8% in mean precision, showing a potential of the Maximum Entropy Model in the task of automatic image annotation.", "Comments": [], "entities": [{"id": 1089950, "label": "ENT", "start_offset": 0, "end_offset": 26}, {"id": 1089951, "label": "ENT", "start_offset": 83, "end_offset": 107}, {"id": 1089952, "label": "ENT", "start_offset": 112, "end_offset": 129}, {"id": 1089953, "label": "ENT", "start_offset": 156, "end_offset": 197}, {"id": 1089954, "label": "ENT", "start_offset": 224, "end_offset": 232}, {"id": 1089955, "label": "ENT", "start_offset": 270, "end_offset": 285}, {"id": 1089956, "label": "ENT", "start_offset": 289, "end_offset": 325}, {"id": 1089957, "label": "ENT", "start_offset": 341, "end_offset": 367}, {"id": 1089958, "label": "ENT", "start_offset": 411, "end_offset": 419}, {"id": 1089959, "label": "ENT", "start_offset": 429, "end_offset": 446}, {"id": 1089960, "label": "ENT", "start_offset": 461, "end_offset": 472}, {"id": 1089961, "label": "ENT", "start_offset": 489, "end_offset": 502}, {"id": 1089962, "label": "ENT", "start_offset": 535, "end_offset": 559}, {"id": 1089963, "label": "ENT", "start_offset": 583, "end_offset": 594}, {"id": 1089964, "label": "ENT", "start_offset": 599, "end_offset": 607}, {"id": 1089965, "label": "ENT", "start_offset": 613, "end_offset": 634}, {"id": 1089966, "label": "ENT", "start_offset": 704, "end_offset": 714}, {"id": 1089967, "label": "ENT", "start_offset": 767, "end_offset": 775}, {"id": 1089968, "label": "ENT", "start_offset": 806, "end_offset": 820}, {"id": 1089969, "label": "ENT", "start_offset": 885, "end_offset": 914}, {"id": 1089970, "label": "ENT", "start_offset": 943, "end_offset": 958}, {"id": 1089971, "label": "ENT", "start_offset": 1007, "end_offset": 1017}, {"id": 1089972, "label": "ENT", "start_offset": 1038, "end_offset": 1044}, {"id": 1089973, "label": "ENT", "start_offset": 1074, "end_offset": 1092}, {"id": 1089974, "label": "ENT", "start_offset": 1108, "end_offset": 1122}, {"id": 1089975, "label": "ENT", "start_offset": 1151, "end_offset": 1172}, {"id": 1089976, "label": "ENT", "start_offset": 1188, "end_offset": 1214}], "relations": [{"id": 173768, "from_id": 1089956, "to_id": 1089957, "type": "USED-FOR"}, {"id": 173769, "from_id": 1089954, "to_id": 1089955, "type": "USED-FOR"}, {"id": 173770, "from_id": 1089952, "to_id": 1089951, "type": "USED-FOR"}, {"id": 173771, "from_id": 1089968, "to_id": 1089967, "type": "USED-FOR"}, {"id": 173772, "from_id": 1089959, "to_id": 1089961, "type": "USED-FOR"}, {"id": 173773, "from_id": 1089965, "to_id": 1089962, "type": "USED-FOR"}, {"id": 173774, "from_id": 1089975, "to_id": 1089976, "type": "USED-FOR"}, {"id": 173775, "from_id": 1089974, "to_id": 1089973, "type": "EVALUATE-FOR"}, {"id": 173776, "from_id": 1089970, "to_id": 1089969, "type": "USED-FOR"}, {"id": 173777, "from_id": 1089950, "to_id": 1089951, "type": "USED-FOR"}, {"id": 173778, "from_id": 1089954, "to_id": 1089953, "type": "USED-FOR"}, {"id": 173779, "from_id": 1089950, "to_id": 1089957, "type": "COREF"}, {"id": 173780, "from_id": 1089960, "to_id": 1089959, "type": "PART-OF"}, {"id": 173781, "from_id": 1089972, "to_id": 1089973, "type": "COMPARE"}, {"id": 173782, "from_id": 1089957, "to_id": 1089966, "type": "COREF"}, {"id": 173783, "from_id": 1089966, "to_id": 1089971, "type": "COREF"}, {"id": 173784, "from_id": 1089972, "to_id": 1089971, "type": "USED-FOR"}, {"id": 173785, "from_id": 1089973, "to_id": 1089971, "type": "USED-FOR"}, {"id": 173786, "from_id": 1089965, "to_id": 1089975, "type": "COREF"}]}
{"id": "C92-4199", "text": "    Word Identification  has been an important and active issue in  Chinese Natural Language Processing  . In this paper, a new mechanism, based on the concept of  sublanguage  , is proposed for identifying  unknown words  , especially  personal names  , in  Chinese newspapers  . The proposed mechanism includes  title-driven name recognition  ,  adaptive dynamic word formation  ,  identification of 2-character and 3-character Chinese names without title  . We will show the experimental results for two  corpora  and compare them with the results by the  NTHU's statistic-based system  , the only system that we know has attacked the same problem. The experimental results have shown significant improvements over the  WI systems  without the  name identification  capability. ", "Comments": [], "entities": [{"id": 1089977, "label": "ENT", "start_offset": 4, "end_offset": 23}, {"id": 1089978, "label": "ENT", "start_offset": 68, "end_offset": 103}, {"id": 1089979, "label": "ENT", "start_offset": 128, "end_offset": 137}, {"id": 1089980, "label": "ENT", "start_offset": 164, "end_offset": 175}, {"id": 1089981, "label": "ENT", "start_offset": 208, "end_offset": 221}, {"id": 1089982, "label": "ENT", "start_offset": 237, "end_offset": 251}, {"id": 1089983, "label": "ENT", "start_offset": 259, "end_offset": 277}, {"id": 1089984, "label": "ENT", "start_offset": 294, "end_offset": 303}, {"id": 1089985, "label": "ENT", "start_offset": 314, "end_offset": 343}, {"id": 1089986, "label": "ENT", "start_offset": 348, "end_offset": 379}, {"id": 1089987, "label": "ENT", "start_offset": 384, "end_offset": 457}, {"id": 1089988, "label": "ENT", "start_offset": 559, "end_offset": 588}, {"id": 1089989, "label": "ENT", "start_offset": 601, "end_offset": 607}, {"id": 1089990, "label": "ENT", "start_offset": 723, "end_offset": 733}, {"id": 1089991, "label": "ENT", "start_offset": 748, "end_offset": 767}], "relations": [{"id": 173787, "from_id": 1089977, "to_id": 1089978, "type": "HYPONYM-OF"}, {"id": 173788, "from_id": 1089979, "to_id": 1089981, "type": "USED-FOR"}, {"id": 173789, "from_id": 1089980, "to_id": 1089979, "type": "USED-FOR"}, {"id": 173790, "from_id": 1089982, "to_id": 1089981, "type": "HYPONYM-OF"}, {"id": 173791, "from_id": 1089984, "to_id": 1089979, "type": "COREF"}, {"id": 173792, "from_id": 1089985, "to_id": 1089984, "type": "PART-OF"}, {"id": 173793, "from_id": 1089986, "to_id": 1089984, "type": "PART-OF"}, {"id": 173794, "from_id": 1089987, "to_id": 1089984, "type": "PART-OF"}, {"id": 173795, "from_id": 1089989, "to_id": 1089988, "type": "COREF"}, {"id": 173796, "from_id": 1089983, "to_id": 1089979, "type": "USED-FOR"}, {"id": 173797, "from_id": 1089985, "to_id": 1089986, "type": "CONJUNCTION"}, {"id": 173798, "from_id": 1089986, "to_id": 1089987, "type": "CONJUNCTION"}]}
{"id": "ICCV_2015_403_abs", "text": "In this paper, we aim to automatically render aging faces in a personalized way. Basically, a set of age-group specific dictionaries are learned, where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups, and a linear combination of these patterns expresses a particular personalized aging process. Moreover, two factors are taken into consideration in the dictionary learning process. First, beyond the aging dictionaries, each subject may have extra personalized facial characteristics, e.g. mole, which are invariant in the aging process. Second, it is challenging or even impossible to collect faces of all age groups for a particular subject, yet much easier and more practical to get face pairs from neighboring age groups. Thus a personality-aware coupled reconstruction loss is utilized to learn the dictionaries based on face pairs from neighboring age groups. Extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts in term of personalized aging progression, as well as the performance gain for cross-age face verification by synthesizing aging faces.", "Comments": [], "entities": [{"id": 1090027, "label": "ENT", "start_offset": 46, "end_offset": 57}, {"id": 1090028, "label": "ENT", "start_offset": 101, "end_offset": 132}, {"id": 1090029, "label": "ENT", "start_offset": 156, "end_offset": 172}, {"id": 1090030, "label": "ENT", "start_offset": 255, "end_offset": 276}, {"id": 1090031, "label": "ENT", "start_offset": 311, "end_offset": 329}, {"id": 1090032, "label": "ENT", "start_offset": 339, "end_offset": 347}, {"id": 1090033, "label": "ENT", "start_offset": 371, "end_offset": 397}, {"id": 1090034, "label": "ENT", "start_offset": 457, "end_offset": 484}, {"id": 1090035, "label": "ENT", "start_offset": 504, "end_offset": 522}, {"id": 1090036, "label": "ENT", "start_offset": 552, "end_offset": 587}, {"id": 1090037, "label": "ENT", "start_offset": 594, "end_offset": 598}, {"id": 1090038, "label": "ENT", "start_offset": 627, "end_offset": 640}, {"id": 1090039, "label": "ENT", "start_offset": 837, "end_offset": 882}, {"id": 1090040, "label": "ENT", "start_offset": 908, "end_offset": 920}, {"id": 1090041, "label": "ENT", "start_offset": 1040, "end_offset": 1048}, {"id": 1090042, "label": "ENT", "start_offset": 1060, "end_offset": 1077}, {"id": 1090043, "label": "ENT", "start_offset": 1089, "end_offset": 1119}, {"id": 1090044, "label": "ENT", "start_offset": 1157, "end_offset": 1184}, {"id": 1090045, "label": "ENT", "start_offset": 1188, "end_offset": 1212}], "relations": [{"id": 173820, "from_id": 1090030, "to_id": 1090032, "type": "COREF"}, {"id": 173821, "from_id": 1090031, "to_id": 1090033, "type": "USED-FOR"}, {"id": 173822, "from_id": 1090032, "to_id": 1090031, "type": "USED-FOR"}, {"id": 173823, "from_id": 1090029, "to_id": 1090028, "type": "PART-OF"}, {"id": 173824, "from_id": 1090037, "to_id": 1090036, "type": "HYPONYM-OF"}, {"id": 173825, "from_id": 1090041, "to_id": 1090042, "type": "COMPARE"}, {"id": 173826, "from_id": 1090045, "to_id": 1090044, "type": "USED-FOR"}, {"id": 173827, "from_id": 1090041, "to_id": 1090043, "type": "USED-FOR"}, {"id": 173828, "from_id": 1090042, "to_id": 1090043, "type": "USED-FOR"}, {"id": 173829, "from_id": 1090033, "to_id": 1090038, "type": "COREF"}, {"id": 173830, "from_id": 1090039, "to_id": 1090040, "type": "USED-FOR"}, {"id": 173831, "from_id": 1090028, "to_id": 1090035, "type": "COREF"}, {"id": 173832, "from_id": 1090035, "to_id": 1090040, "type": "COREF"}]}
{"id": "C94-1091", "text": " This paper presents an algorithm for selecting an appropriate  classifier word  for a  noun . In  Thai language , it frequently happens that there is fluctuation in the choice of  classifier  for a given  concrete noun , both from the point of view of the whole  speech community  and  individual speakers . Basically, there is no exact rule for  classifier selection . As far as we can do in the  rule-based approach  is to give a  default rule  to pick up a corresponding  classifier  of each  noun . Registration of  classifier  for each  noun  is limited to the  type of unit classifier  because other types are open due to the meaning of representation. We propose a  corpus-based method  (Biber,1993; Nagao,1993; Smadja,1993) which generates  Noun Classifier Associations (NCA)  to overcome the problems in  classifier assignment  and  semantic construction of noun phrase . The  NCA  is created statistically from a large  corpus  and recomposed under  concept hierarchy constraints  and  frequency of occurrences .  ", "Comments": [], "entities": [{"id": 1090085, "label": "ENT", "start_offset": 99, "end_offset": 112}, {"id": 1090086, "label": "ENT", "start_offset": 181, "end_offset": 191}, {"id": 1090087, "label": "ENT", "start_offset": 348, "end_offset": 368}, {"id": 1090088, "label": "ENT", "start_offset": 399, "end_offset": 418}, {"id": 1090089, "label": "ENT", "start_offset": 476, "end_offset": 486}, {"id": 1090090, "label": "ENT", "start_offset": 521, "end_offset": 531}, {"id": 1090091, "label": "ENT", "start_offset": 568, "end_offset": 591}, {"id": 1090092, "label": "ENT", "start_offset": 674, "end_offset": 693}, {"id": 1090093, "label": "ENT", "start_offset": 750, "end_offset": 784}, {"id": 1090094, "label": "ENT", "start_offset": 815, "end_offset": 836}, {"id": 1090095, "label": "ENT", "start_offset": 843, "end_offset": 879}, {"id": 1090096, "label": "ENT", "start_offset": 887, "end_offset": 890}, {"id": 1090097, "label": "ENT", "start_offset": 961, "end_offset": 990}, {"id": 1090098, "label": "ENT", "start_offset": 997, "end_offset": 1021}], "relations": [{"id": 173862, "from_id": 1090092, "to_id": 1090094, "type": "USED-FOR"}, {"id": 173863, "from_id": 1090092, "to_id": 1090095, "type": "USED-FOR"}, {"id": 173864, "from_id": 1090096, "to_id": 1090093, "type": "COREF"}, {"id": 173865, "from_id": 1090093, "to_id": 1090094, "type": "USED-FOR"}, {"id": 173866, "from_id": 1090092, "to_id": 1090093, "type": "USED-FOR"}, {"id": 173867, "from_id": 1090093, "to_id": 1090095, "type": "USED-FOR"}, {"id": 173868, "from_id": 1090097, "to_id": 1090096, "type": "USED-FOR"}, {"id": 173869, "from_id": 1090098, "to_id": 1090096, "type": "USED-FOR"}, {"id": 173870, "from_id": 1090094, "to_id": 1090095, "type": "CONJUNCTION"}]}
{"id": "ECCV_2014_47_abs", "text": "The problem of predicting image or video interestingness from their low-level feature representations has received increasing interest. As a highly subjective visual attribute, annotating the interesting-ness value of training data for learning a prediction model is challenging. To make the annotation less subjective and more reliable, recent studies employ crowdsourcing tools to collect pairwise comparisons \u2013 relying on majority voting to prune the annotation outliers/errors. In this paper, we propose a more principled way to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem, tackling both the outlier detection and interestingness prediction tasks jointly. Extensive experiments on both image and video interestingness benchmark datasets demonstrate that our new approach significantly outperforms state-of-the-art alternatives.", "Comments": [], "entities": [{"id": 1090140, "label": "ENT", "start_offset": 15, "end_offset": 56}, {"id": 1090141, "label": "ENT", "start_offset": 68, "end_offset": 101}, {"id": 1090142, "label": "ENT", "start_offset": 148, "end_offset": 175}, {"id": 1090143, "label": "ENT", "start_offset": 192, "end_offset": 214}, {"id": 1090144, "label": "ENT", "start_offset": 247, "end_offset": 263}, {"id": 1090145, "label": "ENT", "start_offset": 360, "end_offset": 379}, {"id": 1090146, "label": "ENT", "start_offset": 391, "end_offset": 411}, {"id": 1090147, "label": "ENT", "start_offset": 425, "end_offset": 440}, {"id": 1090148, "label": "ENT", "start_offset": 454, "end_offset": 480}, {"id": 1090149, "label": "ENT", "start_offset": 526, "end_offset": 529}, {"id": 1090150, "label": "ENT", "start_offset": 542, "end_offset": 561}, {"id": 1090151, "label": "ENT", "start_offset": 581, "end_offset": 612}, {"id": 1090152, "label": "ENT", "start_offset": 618, "end_offset": 641}, {"id": 1090153, "label": "ENT", "start_offset": 645, "end_offset": 657}, {"id": 1090154, "label": "ENT", "start_offset": 677, "end_offset": 694}, {"id": 1090155, "label": "ENT", "start_offset": 699, "end_offset": 731}, {"id": 1090156, "label": "ENT", "start_offset": 771, "end_offset": 821}, {"id": 1090157, "label": "ENT", "start_offset": 847, "end_offset": 855}, {"id": 1090158, "label": "ENT", "start_offset": 882, "end_offset": 911}], "relations": [{"id": 173896, "from_id": 1090141, "to_id": 1090140, "type": "USED-FOR"}, {"id": 173897, "from_id": 1090145, "to_id": 1090146, "type": "USED-FOR"}, {"id": 173898, "from_id": 1090147, "to_id": 1090148, "type": "USED-FOR"}, {"id": 173899, "from_id": 1090147, "to_id": 1090145, "type": "USED-FOR"}, {"id": 173900, "from_id": 1090152, "to_id": 1090151, "type": "USED-FOR"}, {"id": 173901, "from_id": 1090151, "to_id": 1090150, "type": "USED-FOR"}, {"id": 173902, "from_id": 1090152, "to_id": 1090153, "type": "USED-FOR"}, {"id": 173903, "from_id": 1090157, "to_id": 1090158, "type": "COMPARE"}, {"id": 173904, "from_id": 1090149, "to_id": 1090150, "type": "USED-FOR"}, {"id": 173905, "from_id": 1090149, "to_id": 1090154, "type": "USED-FOR"}, {"id": 173906, "from_id": 1090149, "to_id": 1090155, "type": "USED-FOR"}, {"id": 173907, "from_id": 1090154, "to_id": 1090155, "type": "CONJUNCTION"}, {"id": 173908, "from_id": 1090149, "to_id": 1090157, "type": "COREF"}, {"id": 173909, "from_id": 1090156, "to_id": 1090157, "type": "EVALUATE-FOR"}]}
{"id": "INTERSPEECH_2000_490_abs", "text": "Currently, N-gram models are the most common and widely used models for statistical language modeling. In this paper, we investigated an alternative way to build language models, i.e., using artificial neural networks to learn the language model. Our experiment result shows that the neural network can learn a language model that has performance even better than standard statistical methods.", "Comments": [], "entities": [{"id": 1090159, "label": "ENT", "start_offset": 11, "end_offset": 24}, {"id": 1090160, "label": "ENT", "start_offset": 72, "end_offset": 101}, {"id": 1090161, "label": "ENT", "start_offset": 162, "end_offset": 177}, {"id": 1090162, "label": "ENT", "start_offset": 191, "end_offset": 217}, {"id": 1090163, "label": "ENT", "start_offset": 231, "end_offset": 245}, {"id": 1090164, "label": "ENT", "start_offset": 284, "end_offset": 298}, {"id": 1090165, "label": "ENT", "start_offset": 311, "end_offset": 325}, {"id": 1090166, "label": "ENT", "start_offset": 373, "end_offset": 392}], "relations": [{"id": 173910, "from_id": 1090159, "to_id": 1090160, "type": "USED-FOR"}, {"id": 173911, "from_id": 1090163, "to_id": 1090161, "type": "COREF"}, {"id": 173912, "from_id": 1090162, "to_id": 1090163, "type": "USED-FOR"}, {"id": 173913, "from_id": 1090164, "to_id": 1090162, "type": "COREF"}, {"id": 173914, "from_id": 1090164, "to_id": 1090165, "type": "USED-FOR"}, {"id": 173915, "from_id": 1090164, "to_id": 1090166, "type": "COMPARE"}]}
{"id": "C88-2166", "text": "   Although every  natural language system  needs a  computational lexicon  , each system puts different amounts and types of information into its  lexicon  according to its individual needs. However, some of the information needed across systems is shared or identical information. This paper presents our experience in planning and building  COMPLEX  , a  computational lexicon  designed to be a repository of  shared lexical information  for use by  Natural Language Processing (NLP) systems  . We have drawn primarily on explicit and implicit information from  machine-readable dictionaries (MRD's)  to create a  broad coverage lexicon  . ", "Comments": [], "entities": [{"id": 1090172, "label": "ENT", "start_offset": 19, "end_offset": 42}, {"id": 1090173, "label": "ENT", "start_offset": 53, "end_offset": 74}, {"id": 1090174, "label": "ENT", "start_offset": 83, "end_offset": 89}, {"id": 1090175, "label": "ENT", "start_offset": 344, "end_offset": 351}, {"id": 1090176, "label": "ENT", "start_offset": 358, "end_offset": 379}, {"id": 1090177, "label": "ENT", "start_offset": 413, "end_offset": 439}, {"id": 1090178, "label": "ENT", "start_offset": 453, "end_offset": 494}, {"id": 1090179, "label": "ENT", "start_offset": 565, "end_offset": 602}], "relations": [{"id": 173920, "from_id": 1090173, "to_id": 1090172, "type": "USED-FOR"}, {"id": 173921, "from_id": 1090174, "to_id": 1090172, "type": "COREF"}, {"id": 173922, "from_id": 1090175, "to_id": 1090176, "type": "HYPONYM-OF"}, {"id": 173923, "from_id": 1090175, "to_id": 1090178, "type": "USED-FOR"}, {"id": 173924, "from_id": 1090176, "to_id": 1090173, "type": "COREF"}]}
{"id": "INTERSPEECH_2007_21_abs", "text": "Conventional HMMs have weak duration constraints. In noisy conditions, the mismatch between corrupted speech signals and models trained on clean speech may cause the decoder to produce word matches with unrealistic durations. This paper presents a simple way to incorporate word duration constraints by unrolling HMMs to form a lattice where word duration probabilities can be applied directly to state transitions. The expanded HMMs are compatible with conventional Viterbi decoding. Experiments on connected-digit recognition show that when using explicit duration constraints the decoder generates word matches with more reasonable durations, and word error rates are significantly reduced across a broad range of noise conditions .", "Comments": [], "entities": [{"id": 1090180, "label": "ENT", "start_offset": 13, "end_offset": 17}, {"id": 1090181, "label": "ENT", "start_offset": 23, "end_offset": 48}, {"id": 1090182, "label": "ENT", "start_offset": 92, "end_offset": 116}, {"id": 1090183, "label": "ENT", "start_offset": 121, "end_offset": 127}, {"id": 1090184, "label": "ENT", "start_offset": 139, "end_offset": 151}, {"id": 1090185, "label": "ENT", "start_offset": 166, "end_offset": 173}, {"id": 1090186, "label": "ENT", "start_offset": 185, "end_offset": 197}, {"id": 1090187, "label": "ENT", "start_offset": 203, "end_offset": 224}, {"id": 1090188, "label": "ENT", "start_offset": 274, "end_offset": 299}, {"id": 1090189, "label": "ENT", "start_offset": 303, "end_offset": 317}, {"id": 1090190, "label": "ENT", "start_offset": 328, "end_offset": 335}, {"id": 1090191, "label": "ENT", "start_offset": 342, "end_offset": 369}, {"id": 1090192, "label": "ENT", "start_offset": 397, "end_offset": 414}, {"id": 1090193, "label": "ENT", "start_offset": 429, "end_offset": 433}, {"id": 1090194, "label": "ENT", "start_offset": 467, "end_offset": 483}, {"id": 1090195, "label": "ENT", "start_offset": 500, "end_offset": 527}, {"id": 1090196, "label": "ENT", "start_offset": 558, "end_offset": 578}, {"id": 1090197, "label": "ENT", "start_offset": 583, "end_offset": 590}, {"id": 1090198, "label": "ENT", "start_offset": 601, "end_offset": 613}, {"id": 1090199, "label": "ENT", "start_offset": 650, "end_offset": 666}, {"id": 1090200, "label": "ENT", "start_offset": 717, "end_offset": 733}], "relations": [{"id": 173925, "from_id": 1090181, "to_id": 1090180, "type": "FEATURE-OF"}, {"id": 173926, "from_id": 1090184, "to_id": 1090183, "type": "USED-FOR"}, {"id": 173927, "from_id": 1090187, "to_id": 1090186, "type": "FEATURE-OF"}, {"id": 173928, "from_id": 1090185, "to_id": 1090186, "type": "USED-FOR"}, {"id": 173929, "from_id": 1090189, "to_id": 1090188, "type": "USED-FOR"}, {"id": 173930, "from_id": 1090189, "to_id": 1090190, "type": "USED-FOR"}, {"id": 173931, "from_id": 1090191, "to_id": 1090192, "type": "USED-FOR"}, {"id": 173932, "from_id": 1090194, "to_id": 1090193, "type": "CONJUNCTION"}, {"id": 173933, "from_id": 1090197, "to_id": 1090198, "type": "USED-FOR"}, {"id": 173934, "from_id": 1090196, "to_id": 1090197, "type": "USED-FOR"}, {"id": 173935, "from_id": 1090195, "to_id": 1090197, "type": "USED-FOR"}]}
{"id": "N03-2036", "text": " In this paper, we describe a  phrase-based unigram model  for  statistical machine translation  that uses a much simpler set of  model parameters  than similar  phrase-based models  . The  units of translation  are  blocks  - pairs of  phrases  . During  decoding  , we use a  block unigram model  and a  word-based trigram language model  . During  training  , the  blocks  are learned from  source interval projections  using an underlying  word alignment  . We show experimental results on  block selection criteria  based on  unigram  counts and  phrase  length. ", "Comments": [], "entities": [{"id": 1090237, "label": "ENT", "start_offset": 31, "end_offset": 57}, {"id": 1090238, "label": "ENT", "start_offset": 64, "end_offset": 95}, {"id": 1090239, "label": "ENT", "start_offset": 130, "end_offset": 146}, {"id": 1090240, "label": "ENT", "start_offset": 162, "end_offset": 181}, {"id": 1090241, "label": "ENT", "start_offset": 217, "end_offset": 223}, {"id": 1090242, "label": "ENT", "start_offset": 256, "end_offset": 264}, {"id": 1090243, "label": "ENT", "start_offset": 278, "end_offset": 297}, {"id": 1090244, "label": "ENT", "start_offset": 306, "end_offset": 339}, {"id": 1090245, "label": "ENT", "start_offset": 351, "end_offset": 359}, {"id": 1090246, "label": "ENT", "start_offset": 368, "end_offset": 374}, {"id": 1090247, "label": "ENT", "start_offset": 394, "end_offset": 421}, {"id": 1090248, "label": "ENT", "start_offset": 444, "end_offset": 458}, {"id": 1090249, "label": "ENT", "start_offset": 495, "end_offset": 519}, {"id": 1090250, "label": "ENT", "start_offset": 531, "end_offset": 546}, {"id": 1090251, "label": "ENT", "start_offset": 552, "end_offset": 566}], "relations": [{"id": 173953, "from_id": 1090237, "to_id": 1090238, "type": "USED-FOR"}, {"id": 173954, "from_id": 1090247, "to_id": 1090246, "type": "USED-FOR"}, {"id": 173955, "from_id": 1090239, "to_id": 1090237, "type": "USED-FOR"}, {"id": 173956, "from_id": 1090237, "to_id": 1090240, "type": "COMPARE"}, {"id": 173957, "from_id": 1090243, "to_id": 1090242, "type": "USED-FOR"}, {"id": 173958, "from_id": 1090244, "to_id": 1090242, "type": "USED-FOR"}, {"id": 173959, "from_id": 1090244, "to_id": 1090243, "type": "CONJUNCTION"}, {"id": 173960, "from_id": 1090248, "to_id": 1090247, "type": "USED-FOR"}, {"id": 173961, "from_id": 1090250, "to_id": 1090251, "type": "CONJUNCTION"}, {"id": 173962, "from_id": 1090250, "to_id": 1090249, "type": "USED-FOR"}, {"id": 173963, "from_id": 1090251, "to_id": 1090249, "type": "USED-FOR"}, {"id": 173964, "from_id": 1090246, "to_id": 1090241, "type": "COREF"}]}
{"id": "W03-2907", "text": " This paper presents an approach to the  unsupervised learning  of  parts of speech  which uses both  morphological and syntactic information . While the  model  is more complex than those which have been employed for  unsupervised learning  of  POS tags in English , which use only  syntactic information , the variety of  languages  in the world requires that we consider  morphology  as well. In many  languages ,  morphology  provides better clues to a word's category than  word order . We present the  computational model  for  POS learning , and present results for applying it to  Bulgarian , a  Slavic language  with relatively  free word order  and  rich morphology . ", "Comments": [], "entities": [{"id": 1090286, "label": "ENT", "start_offset": 24, "end_offset": 32}, {"id": 1090287, "label": "ENT", "start_offset": 41, "end_offset": 83}, {"id": 1090288, "label": "ENT", "start_offset": 102, "end_offset": 141}, {"id": 1090289, "label": "ENT", "start_offset": 155, "end_offset": 160}, {"id": 1090290, "label": "ENT", "start_offset": 183, "end_offset": 188}, {"id": 1090291, "label": "ENT", "start_offset": 219, "end_offset": 265}, {"id": 1090292, "label": "ENT", "start_offset": 284, "end_offset": 305}, {"id": 1090293, "label": "ENT", "start_offset": 375, "end_offset": 385}, {"id": 1090294, "label": "ENT", "start_offset": 418, "end_offset": 428}, {"id": 1090295, "label": "ENT", "start_offset": 479, "end_offset": 489}, {"id": 1090296, "label": "ENT", "start_offset": 508, "end_offset": 527}, {"id": 1090297, "label": "ENT", "start_offset": 534, "end_offset": 546}, {"id": 1090298, "label": "ENT", "start_offset": 582, "end_offset": 584}, {"id": 1090299, "label": "ENT", "start_offset": 589, "end_offset": 598}, {"id": 1090300, "label": "ENT", "start_offset": 604, "end_offset": 619}, {"id": 1090301, "label": "ENT", "start_offset": 638, "end_offset": 653}, {"id": 1090302, "label": "ENT", "start_offset": 660, "end_offset": 675}], "relations": [{"id": 173990, "from_id": 1090296, "to_id": 1090297, "type": "USED-FOR"}, {"id": 173991, "from_id": 1090299, "to_id": 1090300, "type": "HYPONYM-OF"}, {"id": 173992, "from_id": 1090294, "to_id": 1090295, "type": "COMPARE"}, {"id": 173993, "from_id": 1090301, "to_id": 1090302, "type": "CONJUNCTION"}, {"id": 173994, "from_id": 1090286, "to_id": 1090287, "type": "USED-FOR"}, {"id": 173995, "from_id": 1090288, "to_id": 1090286, "type": "USED-FOR"}, {"id": 173996, "from_id": 1090289, "to_id": 1090286, "type": "COREF"}, {"id": 173997, "from_id": 1090289, "to_id": 1090290, "type": "COMPARE"}, {"id": 173998, "from_id": 1090290, "to_id": 1090291, "type": "USED-FOR"}, {"id": 173999, "from_id": 1090292, "to_id": 1090290, "type": "USED-FOR"}, {"id": 174000, "from_id": 1090296, "to_id": 1090289, "type": "COREF"}, {"id": 174001, "from_id": 1090299, "to_id": 1090298, "type": "USED-FOR"}, {"id": 174002, "from_id": 1090301, "to_id": 1090299, "type": "FEATURE-OF"}, {"id": 174003, "from_id": 1090302, "to_id": 1090299, "type": "FEATURE-OF"}, {"id": 174004, "from_id": 1090298, "to_id": 1090296, "type": "COREF"}]}
{"id": "C92-4207", "text": "   This paper describes the understanding process of the  spatial descriptions  in  Japanese  . In order to understand the described  world  , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space. It is done by an experimental  computer program   SPRINT  , which takes  natural language texts  and produces a  model  of the described  world  . To reconstruct the  model  , the authors extract the  qualitative spatial constraints  from the  text  , and represent them as the  numerical constraints  on the  spatial attributes  of the  entities  . This makes it possible to express the vagueness of the  spatial concepts  and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints. The interpretation reflects the  temporary belief  about the  world  . ", "Comments": [], "entities": [{"id": 1090328, "label": "ENT", "start_offset": 58, "end_offset": 78}, {"id": 1090329, "label": "ENT", "start_offset": 84, "end_offset": 92}, {"id": 1090330, "label": "ENT", "start_offset": 178, "end_offset": 193}, {"id": 1090331, "label": "ENT", "start_offset": 291, "end_offset": 316}, {"id": 1090332, "label": "ENT", "start_offset": 333, "end_offset": 355}, {"id": 1090333, "label": "ENT", "start_offset": 373, "end_offset": 378}, {"id": 1090334, "label": "ENT", "start_offset": 427, "end_offset": 432}, {"id": 1090335, "label": "ENT", "start_offset": 461, "end_offset": 492}, {"id": 1090336, "label": "ENT", "start_offset": 539, "end_offset": 560}, {"id": 1090337, "label": "ENT", "start_offset": 570, "end_offset": 606}, {"id": 1090338, "label": "ENT", "start_offset": 666, "end_offset": 682}], "relations": [{"id": 174019, "from_id": 1090329, "to_id": 1090328, "type": "FEATURE-OF"}, {"id": 174020, "from_id": 1090334, "to_id": 1090333, "type": "COREF"}, {"id": 174021, "from_id": 1090337, "to_id": 1090336, "type": "USED-FOR"}]}
{"id": "C80-1039", "text": "   In order to meet the needs of a publication of papers in English, many systems to run off texts have been developed. In this paper, we report a system  FROFF  which can make a fair copy of not only texts but also graphs and tables indispensable to our papers. Its selection of  fonts  , specification of  character  size are dynamically changeable, and the  typing location  can be also changed in lateral or longitudinal directions. Each  character  has its own width and a line length is counted by the sum of each  character  . By using commands or  rules  which are defined to facilitate the construction of format expected or some  mathematical expressions  , elaborate and pretty documents can be successfully obtained. ", "Comments": [], "entities": [{"id": 1090354, "label": "ENT", "start_offset": 147, "end_offset": 153}, {"id": 1090355, "label": "ENT", "start_offset": 155, "end_offset": 160}, {"id": 1090356, "label": "ENT", "start_offset": 361, "end_offset": 376}, {"id": 1090357, "label": "ENT", "start_offset": 543, "end_offset": 551}, {"id": 1090358, "label": "ENT", "start_offset": 556, "end_offset": 561}, {"id": 1090359, "label": "ENT", "start_offset": 640, "end_offset": 664}], "relations": [{"id": 174035, "from_id": 1090358, "to_id": 1090359, "type": "USED-FOR"}, {"id": 174036, "from_id": 1090355, "to_id": 1090354, "type": "COREF"}, {"id": 174037, "from_id": 1090357, "to_id": 1090359, "type": "USED-FOR"}, {"id": 174038, "from_id": 1090357, "to_id": 1090358, "type": "CONJUNCTION"}]}
{"id": "NIPS_2016_560_abs", "text": "Joint matrix triangularization is often used for estimating the joint eigenstructure of a set M of matrices, with applications in signal processing and machine learning. We consider the problem of approximate joint matrix triangularization when the matrices in M are jointly diagonalizable and real, but we only observe a set M' of noise perturbed versions of the matrices in M. Our main result is a first-order upper bound on the distance between any approximate joint triangularizer of the matrices in M' and any exact joint triangularizer of the matrices in M. The bound depends only on the observable matrices in M' and the noise level. In particular, it does not depend on optimization specific properties of the triangularizer, such as its proximity to critical points, that are typical of existing bounds in the literature. To our knowledge, this is the first a posteriori bound for joint matrix decomposition. We demonstrate the bound on synthetic data for which the ground truth is known.", "Comments": [], "entities": [{"id": 1090384, "label": "ENT", "start_offset": 0, "end_offset": 30}, {"id": 1090385, "label": "ENT", "start_offset": 64, "end_offset": 84}, {"id": 1090386, "label": "ENT", "start_offset": 130, "end_offset": 147}, {"id": 1090387, "label": "ENT", "start_offset": 152, "end_offset": 168}, {"id": 1090388, "label": "ENT", "start_offset": 197, "end_offset": 239}, {"id": 1090389, "label": "ENT", "start_offset": 400, "end_offset": 423}, {"id": 1090390, "label": "ENT", "start_offset": 452, "end_offset": 484}, {"id": 1090391, "label": "ENT", "start_offset": 515, "end_offset": 541}, {"id": 1090392, "label": "ENT", "start_offset": 568, "end_offset": 573}, {"id": 1090393, "label": "ENT", "start_offset": 718, "end_offset": 732}, {"id": 1090394, "label": "ENT", "start_offset": 869, "end_offset": 885}, {"id": 1090395, "label": "ENT", "start_offset": 890, "end_offset": 916}, {"id": 1090396, "label": "ENT", "start_offset": 946, "end_offset": 960}], "relations": [{"id": 174067, "from_id": 1090384, "to_id": 1090385, "type": "USED-FOR"}, {"id": 174068, "from_id": 1090386, "to_id": 1090387, "type": "CONJUNCTION"}, {"id": 174069, "from_id": 1090385, "to_id": 1090386, "type": "USED-FOR"}, {"id": 174070, "from_id": 1090385, "to_id": 1090387, "type": "USED-FOR"}, {"id": 174071, "from_id": 1090388, "to_id": 1090384, "type": "HYPONYM-OF"}, {"id": 174072, "from_id": 1090390, "to_id": 1090391, "type": "CONJUNCTION"}, {"id": 174073, "from_id": 1090392, "to_id": 1090389, "type": "COREF"}, {"id": 174074, "from_id": 1090394, "to_id": 1090395, "type": "USED-FOR"}]}
{"id": "E85-1004", "text": " This paper reports a completed stage of ongoing research at the University of York. Landsbergen's advocacy of  analytical inverses  for  compositional syntax rules  encourages the application of  Definite Clause Grammar techniques  to the construction of a  parser  returning  Montague analysis trees . A  parser MDCC  is presented which implements an  augmented Friedman - Warren algorithm  permitting  post referencing * and interfaces with a language of  intenslonal logic translator LILT  so as to display the  derivational history  of corresponding  reduced IL formulae . Some familiarity with  Montague's PTQ  and the  basic DCG mechanism  is assumed. ", "Comments": [], "entities": [{"id": 1090436, "label": "ENT", "start_offset": 112, "end_offset": 131}, {"id": 1090437, "label": "ENT", "start_offset": 138, "end_offset": 164}, {"id": 1090438, "label": "ENT", "start_offset": 197, "end_offset": 231}, {"id": 1090439, "label": "ENT", "start_offset": 259, "end_offset": 301}, {"id": 1090440, "label": "ENT", "start_offset": 307, "end_offset": 318}, {"id": 1090441, "label": "ENT", "start_offset": 354, "end_offset": 391}, {"id": 1090442, "label": "ENT", "start_offset": 405, "end_offset": 421}, {"id": 1090443, "label": "ENT", "start_offset": 459, "end_offset": 492}, {"id": 1090444, "label": "ENT", "start_offset": 516, "end_offset": 536}, {"id": 1090445, "label": "ENT", "start_offset": 556, "end_offset": 575}, {"id": 1090446, "label": "ENT", "start_offset": 601, "end_offset": 615}, {"id": 1090447, "label": "ENT", "start_offset": 626, "end_offset": 645}], "relations": [{"id": 174109, "from_id": 1090436, "to_id": 1090437, "type": "USED-FOR"}, {"id": 174110, "from_id": 1090441, "to_id": 1090440, "type": "USED-FOR"}, {"id": 174111, "from_id": 1090445, "to_id": 1090444, "type": "FEATURE-OF"}, {"id": 174112, "from_id": 1090442, "to_id": 1090441, "type": "FEATURE-OF"}, {"id": 174113, "from_id": 1090444, "to_id": 1090443, "type": "USED-FOR"}, {"id": 174114, "from_id": 1090447, "to_id": 1090438, "type": "COREF"}, {"id": 174115, "from_id": 1090438, "to_id": 1090439, "type": "USED-FOR"}, {"id": 174116, "from_id": 1090446, "to_id": 1090447, "type": "CONJUNCTION"}, {"id": 174117, "from_id": 1090436, "to_id": 1090438, "type": "USED-FOR"}]}
{"id": "ICML_2016_21_abs", "text": "We propose a novel limited-memory stochastic block BFGS update for incorporating enriched curvature information in stochastic approximation methods. In our method, the estimate of the inverse Hessian matrix that is maintained by it, is updated at each iteration using a sketch of the Hessian, i.e., a randomly generated compressed form of the Hessian. We propose several sketching strategies, present a new quasi-Newton method that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute batch stochastic gradients, and prove linear convergence of the resulting method. Numerical tests on large-scale logistic regression problems reveal that our method is more robust and substantially outperforms current state-of-the-art methods.", "Comments": [], "entities": [{"id": 1090448, "label": "ENT", "start_offset": 19, "end_offset": 62}, {"id": 1090449, "label": "ENT", "start_offset": 67, "end_offset": 147}, {"id": 1090450, "label": "ENT", "start_offset": 156, "end_offset": 162}, {"id": 1090451, "label": "ENT", "start_offset": 184, "end_offset": 206}, {"id": 1090452, "label": "ENT", "start_offset": 229, "end_offset": 231}, {"id": 1090453, "label": "ENT", "start_offset": 284, "end_offset": 291}, {"id": 1090454, "label": "ENT", "start_offset": 301, "end_offset": 350}, {"id": 1090455, "label": "ENT", "start_offset": 371, "end_offset": 391}, {"id": 1090456, "label": "ENT", "start_offset": 407, "end_offset": 426}, {"id": 1090457, "label": "ENT", "start_offset": 437, "end_offset": 466}, {"id": 1090458, "label": "ENT", "start_offset": 485, "end_offset": 517}, {"id": 1090459, "label": "ENT", "start_offset": 529, "end_offset": 555}, {"id": 1090460, "label": "ENT", "start_offset": 567, "end_offset": 585}, {"id": 1090461, "label": "ENT", "start_offset": 603, "end_offset": 609}, {"id": 1090462, "label": "ENT", "start_offset": 630, "end_offset": 670}, {"id": 1090463, "label": "ENT", "start_offset": 687, "end_offset": 693}, {"id": 1090464, "label": "ENT", "start_offset": 747, "end_offset": 771}], "relations": [{"id": 174118, "from_id": 1090448, "to_id": 1090450, "type": "COREF"}, {"id": 174119, "from_id": 1090452, "to_id": 1090450, "type": "COREF"}, {"id": 174120, "from_id": 1090452, "to_id": 1090451, "type": "USED-FOR"}, {"id": 174121, "from_id": 1090454, "to_id": 1090453, "type": "HYPONYM-OF"}, {"id": 174122, "from_id": 1090453, "to_id": 1090452, "type": "USED-FOR"}, {"id": 174123, "from_id": 1090457, "to_id": 1090458, "type": "CONJUNCTION"}, {"id": 174124, "from_id": 1090457, "to_id": 1090456, "type": "USED-FOR"}, {"id": 174125, "from_id": 1090458, "to_id": 1090456, "type": "USED-FOR"}, {"id": 174126, "from_id": 1090456, "to_id": 1090459, "type": "USED-FOR"}, {"id": 174127, "from_id": 1090448, "to_id": 1090463, "type": "COREF"}, {"id": 174128, "from_id": 1090462, "to_id": 1090463, "type": "EVALUATE-FOR"}, {"id": 174129, "from_id": 1090462, "to_id": 1090464, "type": "EVALUATE-FOR"}, {"id": 174130, "from_id": 1090463, "to_id": 1090464, "type": "COMPARE"}, {"id": 174131, "from_id": 1090448, "to_id": 1090449, "type": "USED-FOR"}, {"id": 174132, "from_id": 1090456, "to_id": 1090461, "type": "COREF"}, {"id": 174133, "from_id": 1090460, "to_id": 1090461, "type": "FEATURE-OF"}, {"id": 174134, "from_id": 1090455, "to_id": 1090463, "type": "HYPONYM-OF"}, {"id": 174135, "from_id": 1090456, "to_id": 1090463, "type": "HYPONYM-OF"}]}
{"id": "E99-1029", "text": " One of the claimed benefits of  Tree Adjoining Grammars  is that they have an  extended domain of locality (EDOL) . We consider how this can be exploited to limit the need for  feature structure unification  during  parsing . We compare two wide-coverage  lexicalized grammars of English ,  LEXSYS  and  XTAG , finding that the two  grammars  exploit  EDOL  in different ways. ", "Comments": [], "entities": [{"id": 1090473, "label": "ENT", "start_offset": 33, "end_offset": 56}, {"id": 1090474, "label": "ENT", "start_offset": 66, "end_offset": 70}, {"id": 1090475, "label": "ENT", "start_offset": 80, "end_offset": 114}, {"id": 1090476, "label": "ENT", "start_offset": 178, "end_offset": 207}, {"id": 1090477, "label": "ENT", "start_offset": 217, "end_offset": 224}, {"id": 1090478, "label": "ENT", "start_offset": 257, "end_offset": 288}, {"id": 1090479, "label": "ENT", "start_offset": 292, "end_offset": 298}, {"id": 1090480, "label": "ENT", "start_offset": 305, "end_offset": 309}, {"id": 1090481, "label": "ENT", "start_offset": 334, "end_offset": 342}, {"id": 1090482, "label": "ENT", "start_offset": 353, "end_offset": 357}], "relations": [{"id": 174143, "from_id": 1090473, "to_id": 1090475, "type": "FEATURE-OF"}, {"id": 174144, "from_id": 1090476, "to_id": 1090477, "type": "USED-FOR"}, {"id": 174145, "from_id": 1090479, "to_id": 1090480, "type": "COMPARE"}, {"id": 174146, "from_id": 1090481, "to_id": 1090482, "type": "USED-FOR"}, {"id": 174147, "from_id": 1090479, "to_id": 1090478, "type": "HYPONYM-OF"}, {"id": 174148, "from_id": 1090480, "to_id": 1090478, "type": "HYPONYM-OF"}, {"id": 174149, "from_id": 1090481, "to_id": 1090478, "type": "COREF"}, {"id": 174150, "from_id": 1090482, "to_id": 1090475, "type": "COREF"}, {"id": 174151, "from_id": 1090474, "to_id": 1090473, "type": "COREF"}]}
{"id": "E87-1043", "text": " The  verb forms  are often claimed to convey two kinds of  information  : 1. whether the  event  described in a  sentence  is  present ,  past  or  future  (=  deictic information ) 2. whether the  event  described in a  sentence  is presented as completed, going on, just starting or being finished (=  aspectual information ). It will be demonstrated in this paper that one has to add a third component to the analysis of  verb form meanings , namely whether or not they express  habituality . The framework of the analysis is  model-theoretic semantics . ", "Comments": [], "entities": [{"id": 1090483, "label": "ENT", "start_offset": 6, "end_offset": 16}, {"id": 1090484, "label": "ENT", "start_offset": 60, "end_offset": 71}, {"id": 1090485, "label": "ENT", "start_offset": 161, "end_offset": 180}, {"id": 1090486, "label": "ENT", "start_offset": 305, "end_offset": 326}, {"id": 1090487, "label": "ENT", "start_offset": 413, "end_offset": 444}, {"id": 1090488, "label": "ENT", "start_offset": 518, "end_offset": 526}, {"id": 1090489, "label": "ENT", "start_offset": 531, "end_offset": 556}], "relations": [{"id": 174152, "from_id": 1090485, "to_id": 1090484, "type": "HYPONYM-OF"}, {"id": 174153, "from_id": 1090486, "to_id": 1090484, "type": "HYPONYM-OF"}, {"id": 174154, "from_id": 1090485, "to_id": 1090486, "type": "CONJUNCTION"}, {"id": 174155, "from_id": 1090488, "to_id": 1090487, "type": "COREF"}, {"id": 174156, "from_id": 1090489, "to_id": 1090488, "type": "USED-FOR"}]}
{"id": "ECCV_2016_212_abs", "text": "Person re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)'s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples , for approximating the geodesic distance. From this point of view, selecting suitable positive (i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has large intra-class variations. In this paper, we propose a novel moderate positive sample mining method to train robust CNN for person re-identification, dealing with the problem of large variation. In addition, we improve the learning by a metric weight constraint, so that the learned metric has a better generalization ability. Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification, and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification. Therefore, the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification.", "Comments": [], "entities": [{"id": 1090531, "label": "ENT", "start_offset": 0, "end_offset": 24}, {"id": 1090532, "label": "ENT", "start_offset": 71, "end_offset": 75}, {"id": 1090533, "label": "ENT", "start_offset": 77, "end_offset": 89}, {"id": 1090534, "label": "ENT", "start_offset": 91, "end_offset": 100}, {"id": 1090535, "label": "ENT", "start_offset": 105, "end_offset": 116}, {"id": 1090536, "label": "ENT", "start_offset": 149, "end_offset": 164}, {"id": 1090537, "label": "ENT", "start_offset": 183, "end_offset": 206}, {"id": 1090538, "label": "ENT", "start_offset": 214, "end_offset": 227}, {"id": 1090539, "label": "ENT", "start_offset": 249, "end_offset": 284}, {"id": 1090540, "label": "ENT", "start_offset": 301, "end_offset": 319}, {"id": 1090541, "label": "ENT", "start_offset": 389, "end_offset": 406}, {"id": 1090542, "label": "ENT", "start_offset": 460, "end_offset": 482}, {"id": 1090543, "label": "ENT", "start_offset": 491, "end_offset": 509}, {"id": 1090544, "label": "ENT", "start_offset": 560, "end_offset": 585}, {"id": 1090545, "label": "ENT", "start_offset": 605, "end_offset": 623}, {"id": 1090546, "label": "ENT", "start_offset": 631, "end_offset": 642}, {"id": 1090547, "label": "ENT", "start_offset": 663, "end_offset": 685}, {"id": 1090548, "label": "ENT", "start_offset": 726, "end_offset": 743}, {"id": 1090549, "label": "ENT", "start_offset": 843, "end_offset": 854}, {"id": 1090550, "label": "ENT", "start_offset": 884, "end_offset": 897}, {"id": 1090551, "label": "ENT", "start_offset": 919, "end_offset": 923}, {"id": 1090552, "label": "ENT", "start_offset": 934, "end_offset": 956}, {"id": 1090553, "label": "ENT", "start_offset": 992, "end_offset": 1030}, {"id": 1090554, "label": "ENT", "start_offset": 1040, "end_offset": 1050}, {"id": 1090555, "label": "ENT", "start_offset": 1055, "end_offset": 1079}, {"id": 1090556, "label": "ENT", "start_offset": 1154, "end_offset": 1162}, {"id": 1090557, "label": "ENT", "start_offset": 1168, "end_offset": 1192}, {"id": 1090558, "label": "ENT", "start_offset": 1206, "end_offset": 1220}, {"id": 1090559, "label": "ENT", "start_offset": 1234, "end_offset": 1256}, {"id": 1090560, "label": "ENT", "start_offset": 1327, "end_offset": 1346}, {"id": 1090561, "label": "ENT", "start_offset": 1351, "end_offset": 1375}, {"id": 1090562, "label": "ENT", "start_offset": 1397, "end_offset": 1407}, {"id": 1090563, "label": "ENT", "start_offset": 1438, "end_offset": 1462}, {"id": 1090564, "label": "ENT", "start_offset": 1488, "end_offset": 1512}, {"id": 1090565, "label": "ENT", "start_offset": 1601, "end_offset": 1612}, {"id": 1090566, "label": "ENT", "start_offset": 1617, "end_offset": 1641}], "relations": [{"id": 174188, "from_id": 1090539, "to_id": 1090540, "type": "USED-FOR"}, {"id": 174189, "from_id": 1090537, "to_id": 1090536, "type": "USED-FOR"}, {"id": 174190, "from_id": 1090538, "to_id": 1090537, "type": "FEATURE-OF"}, {"id": 174191, "from_id": 1090543, "to_id": 1090542, "type": "USED-FOR"}, {"id": 174192, "from_id": 1090545, "to_id": 1090544, "type": "USED-FOR"}, {"id": 174193, "from_id": 1090546, "to_id": 1090545, "type": "FEATURE-OF"}, {"id": 174194, "from_id": 1090545, "to_id": 1090548, "type": "USED-FOR"}, {"id": 174195, "from_id": 1090547, "to_id": 1090548, "type": "USED-FOR"}, {"id": 174196, "from_id": 1090545, "to_id": 1090547, "type": "CONJUNCTION"}, {"id": 174197, "from_id": 1090553, "to_id": 1090554, "type": "USED-FOR"}, {"id": 174198, "from_id": 1090554, "to_id": 1090555, "type": "USED-FOR"}, {"id": 174199, "from_id": 1090557, "to_id": 1090556, "type": "USED-FOR"}, {"id": 174200, "from_id": 1090559, "to_id": 1090558, "type": "FEATURE-OF"}, {"id": 174201, "from_id": 1090560, "to_id": 1090561, "type": "USED-FOR"}, {"id": 174202, "from_id": 1090562, "to_id": 1090563, "type": "COMPARE"}, {"id": 174203, "from_id": 1090562, "to_id": 1090564, "type": "USED-FOR"}, {"id": 174204, "from_id": 1090563, "to_id": 1090564, "type": "USED-FOR"}, {"id": 174205, "from_id": 1090565, "to_id": 1090566, "type": "USED-FOR"}, {"id": 174206, "from_id": 1090566, "to_id": 1090564, "type": "COREF"}, {"id": 174207, "from_id": 1090564, "to_id": 1090555, "type": "COREF"}, {"id": 174208, "from_id": 1090555, "to_id": 1090531, "type": "COREF"}, {"id": 174209, "from_id": 1090552, "to_id": 1090551, "type": "FEATURE-OF"}]}
{"id": "P05-1028", "text": " This paper presents a  corpus study  that explores the extent to which captions contribute to recognizing the intended message of an  information graphic . It then presents an implemented  graphic interpretation system  that takes into account a variety of  communicative signals , and an evaluation study showing that evidence obtained from  shallow processing  of the graphic's caption has a significant impact on the system's success. This work is part of a larger project whose goal is to provide  sight-impaired users  with effective access to  information graphics . ", "Comments": [], "entities": [{"id": 1090585, "label": "ENT", "start_offset": 24, "end_offset": 36}, {"id": 1090586, "label": "ENT", "start_offset": 135, "end_offset": 154}, {"id": 1090587, "label": "ENT", "start_offset": 190, "end_offset": 219}, {"id": 1090588, "label": "ENT", "start_offset": 259, "end_offset": 280}, {"id": 1090589, "label": "ENT", "start_offset": 344, "end_offset": 362}, {"id": 1090590, "label": "ENT", "start_offset": 371, "end_offset": 388}, {"id": 1090591, "label": "ENT", "start_offset": 421, "end_offset": 427}, {"id": 1090592, "label": "ENT", "start_offset": 503, "end_offset": 523}, {"id": 1090593, "label": "ENT", "start_offset": 551, "end_offset": 571}], "relations": [{"id": 174218, "from_id": 1090588, "to_id": 1090587, "type": "USED-FOR"}, {"id": 174219, "from_id": 1090590, "to_id": 1090589, "type": "USED-FOR"}, {"id": 174220, "from_id": 1090589, "to_id": 1090591, "type": "USED-FOR"}, {"id": 174221, "from_id": 1090587, "to_id": 1090591, "type": "COREF"}]}
{"id": "P06-1053", "text": " The  psycholinguistic literature  provides evidence for  syntactic priming , i.e., the tendency to repeat structures. This paper describes a method for incorporating  priming  into an  incremental probabilistic parser . Three models are compared, which involve  priming  of  rules  between  sentences , within  sentences , and within  coordinate structures . These models simulate the reading time advantage for  parallel structures  found in  human data , and also yield a small increase in overall  parsing accuracy . ", "Comments": [], "entities": [{"id": 1090614, "label": "ENT", "start_offset": 6, "end_offset": 33}, {"id": 1090615, "label": "ENT", "start_offset": 58, "end_offset": 75}, {"id": 1090616, "label": "ENT", "start_offset": 142, "end_offset": 148}, {"id": 1090617, "label": "ENT", "start_offset": 168, "end_offset": 175}, {"id": 1090618, "label": "ENT", "start_offset": 186, "end_offset": 218}, {"id": 1090619, "label": "ENT", "start_offset": 263, "end_offset": 270}, {"id": 1090620, "label": "ENT", "start_offset": 276, "end_offset": 281}, {"id": 1090621, "label": "ENT", "start_offset": 336, "end_offset": 357}, {"id": 1090622, "label": "ENT", "start_offset": 414, "end_offset": 433}, {"id": 1090623, "label": "ENT", "start_offset": 445, "end_offset": 455}, {"id": 1090624, "label": "ENT", "start_offset": 502, "end_offset": 518}], "relations": [{"id": 174242, "from_id": 1090617, "to_id": 1090618, "type": "USED-FOR"}, {"id": 174243, "from_id": 1090622, "to_id": 1090623, "type": "PART-OF"}, {"id": 174244, "from_id": 1090614, "to_id": 1090615, "type": "USED-FOR"}, {"id": 174245, "from_id": 1090616, "to_id": 1090617, "type": "USED-FOR"}, {"id": 174246, "from_id": 1090615, "to_id": 1090617, "type": "COREF"}]}
{"id": "P02-1059", "text": " The paper proposes and empirically motivates an integration of  supervised learning  with  unsupervised learning  to deal with human biases in  summarization . In particular, we explore the use of  probabilistic decision tree  within the clustering framework to account for the variation as well as regularity in  human created summaries . The  corpus  of human created extracts is created from a  newspaper corpus  and used as a test set. We build  probabilistic decision trees  of different flavors and integrate each of them with the clustering framework. Experiments with the  corpus  demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either ofthe two is considered alone. ", "Comments": [], "entities": [{"id": 1090632, "label": "ENT", "start_offset": 65, "end_offset": 84}, {"id": 1090633, "label": "ENT", "start_offset": 92, "end_offset": 113}, {"id": 1090634, "label": "ENT", "start_offset": 128, "end_offset": 158}, {"id": 1090635, "label": "ENT", "start_offset": 199, "end_offset": 226}, {"id": 1090636, "label": "ENT", "start_offset": 239, "end_offset": 259}, {"id": 1090637, "label": "ENT", "start_offset": 315, "end_offset": 338}, {"id": 1090638, "label": "ENT", "start_offset": 346, "end_offset": 379}, {"id": 1090639, "label": "ENT", "start_offset": 399, "end_offset": 415}, {"id": 1090640, "label": "ENT", "start_offset": 451, "end_offset": 479}, {"id": 1090641, "label": "ENT", "start_offset": 524, "end_offset": 528}, {"id": 1090642, "label": "ENT", "start_offset": 538, "end_offset": 558}, {"id": 1090643, "label": "ENT", "start_offset": 582, "end_offset": 588}], "relations": [{"id": 174253, "from_id": 1090633, "to_id": 1090632, "type": "CONJUNCTION"}, {"id": 174254, "from_id": 1090633, "to_id": 1090634, "type": "USED-FOR"}, {"id": 174255, "from_id": 1090632, "to_id": 1090634, "type": "USED-FOR"}, {"id": 174256, "from_id": 1090636, "to_id": 1090635, "type": "FEATURE-OF"}, {"id": 174257, "from_id": 1090639, "to_id": 1090638, "type": "USED-FOR"}, {"id": 174258, "from_id": 1090638, "to_id": 1090643, "type": "COREF"}, {"id": 174259, "from_id": 1090640, "to_id": 1090641, "type": "COREF"}, {"id": 174260, "from_id": 1090642, "to_id": 1090641, "type": "CONJUNCTION"}]}
{"id": "AAAI_2008_262_abs", "text": "We describe Yoopick, a combinatorial sports prediction market that implements a flexible betting language, and in turn facilitates fine-grained probabilistic estimation of outcomes.", "Comments": [], "entities": [{"id": 1090644, "label": "ENT", "start_offset": 12, "end_offset": 19}, {"id": 1090645, "label": "ENT", "start_offset": 23, "end_offset": 61}, {"id": 1090646, "label": "ENT", "start_offset": 80, "end_offset": 105}, {"id": 1090647, "label": "ENT", "start_offset": 131, "end_offset": 180}], "relations": [{"id": 174261, "from_id": 1090644, "to_id": 1090645, "type": "HYPONYM-OF"}, {"id": 174262, "from_id": 1090646, "to_id": 1090644, "type": "USED-FOR"}, {"id": 174263, "from_id": 1090644, "to_id": 1090647, "type": "USED-FOR"}]}
{"id": "NIPS_2015_10_abs", "text": "We propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including Stochastic Variance Reduced Gradient (SVRG) and Stochastic Dual Coordinate Ascent (SDCA). For a large family of penalized empirical risk minimization problems , our methods exploit data dependent local smoothness of the loss functions near the optimum, while maintaining convergence guarantees. Our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better. Empirically, we provide thorough numerical results to back up our theory. Additionally we present algorithms exploiting local smoothness in more aggressive ways, which perform even better in practice.", "Comments": [], "entities": [{"id": 1090648, "label": "ENT", "start_offset": 23, "end_offset": 54}, {"id": 1090649, "label": "ENT", "start_offset": 87, "end_offset": 121}, {"id": 1090650, "label": "ENT", "start_offset": 127, "end_offset": 145}, {"id": 1090651, "label": "ENT", "start_offset": 156, "end_offset": 199}, {"id": 1090652, "label": "ENT", "start_offset": 204, "end_offset": 244}, {"id": 1090653, "label": "ENT", "start_offset": 268, "end_offset": 314}, {"id": 1090654, "label": "ENT", "start_offset": 321, "end_offset": 328}, {"id": 1090655, "label": "ENT", "start_offset": 337, "end_offset": 368}, {"id": 1090656, "label": "ENT", "start_offset": 376, "end_offset": 390}, {"id": 1090657, "label": "ENT", "start_offset": 400, "end_offset": 407}, {"id": 1090658, "label": "ENT", "start_offset": 427, "end_offset": 449}, {"id": 1090659, "label": "ENT", "start_offset": 514, "end_offset": 530}, {"id": 1090660, "label": "ENT", "start_offset": 659, "end_offset": 665}, {"id": 1090661, "label": "ENT", "start_offset": 691, "end_offset": 701}, {"id": 1090662, "label": "ENT", "start_offset": 713, "end_offset": 729}], "relations": [{"id": 174264, "from_id": 1090650, "to_id": 1090649, "type": "FEATURE-OF"}, {"id": 174265, "from_id": 1090651, "to_id": 1090652, "type": "CONJUNCTION"}, {"id": 174266, "from_id": 1090651, "to_id": 1090649, "type": "HYPONYM-OF"}, {"id": 174267, "from_id": 1090652, "to_id": 1090649, "type": "HYPONYM-OF"}, {"id": 174268, "from_id": 1090648, "to_id": 1090649, "type": "USED-FOR"}, {"id": 174269, "from_id": 1090655, "to_id": 1090656, "type": "FEATURE-OF"}, {"id": 174270, "from_id": 1090662, "to_id": 1090661, "type": "USED-FOR"}, {"id": 174271, "from_id": 1090655, "to_id": 1090654, "type": "USED-FOR"}, {"id": 174272, "from_id": 1090654, "to_id": 1090653, "type": "USED-FOR"}, {"id": 174273, "from_id": 1090654, "to_id": 1090648, "type": "COREF"}, {"id": 174274, "from_id": 1090660, "to_id": 1090654, "type": "COREF"}]}
{"id": "ICML_2006_111_abs", "text": "Boosting methods are known not to usually overfit training data even as the size of the generated classifiers becomes large. Schapire et al. attempted to explain this phenomenon in terms of the margins the classifier achieves on training examples. Later, however, Breiman cast serious doubt on this explanation by introducing a boosting algorithm, arc-gv, that can generate a higher margins distribution than AdaBoost and yet performs worse. In this paper, we take a close look at Breiman's compelling but puzzling results. Although we can reproduce his main finding, we find that the poorer performance of arc-gv can be explained by the increased complexity of the base classifiers it uses, an explanation supported by our experiments and entirely consistent with the margins theory. Thus, we find maximizing the margins is desirable, but not necessarily at the expense of other factors, especially base-classifier complexity.", "Comments": [], "entities": [{"id": 1090663, "label": "ENT", "start_offset": 0, "end_offset": 16}, {"id": 1090664, "label": "ENT", "start_offset": 98, "end_offset": 109}, {"id": 1090665, "label": "ENT", "start_offset": 206, "end_offset": 216}, {"id": 1090666, "label": "ENT", "start_offset": 328, "end_offset": 346}, {"id": 1090667, "label": "ENT", "start_offset": 348, "end_offset": 354}, {"id": 1090668, "label": "ENT", "start_offset": 383, "end_offset": 403}, {"id": 1090669, "label": "ENT", "start_offset": 409, "end_offset": 417}, {"id": 1090670, "label": "ENT", "start_offset": 607, "end_offset": 613}, {"id": 1090671, "label": "ENT", "start_offset": 648, "end_offset": 658}, {"id": 1090672, "label": "ENT", "start_offset": 666, "end_offset": 682}, {"id": 1090673, "label": "ENT", "start_offset": 769, "end_offset": 783}, {"id": 1090674, "label": "ENT", "start_offset": 900, "end_offset": 926}], "relations": [{"id": 174275, "from_id": 1090667, "to_id": 1090666, "type": "HYPONYM-OF"}, {"id": 174276, "from_id": 1090667, "to_id": 1090669, "type": "COMPARE"}, {"id": 174277, "from_id": 1090667, "to_id": 1090668, "type": "USED-FOR"}, {"id": 174278, "from_id": 1090672, "to_id": 1090670, "type": "HYPONYM-OF"}, {"id": 174279, "from_id": 1090670, "to_id": 1090667, "type": "COREF"}, {"id": 174280, "from_id": 1090671, "to_id": 1090672, "type": "EVALUATE-FOR"}]}
{"id": "L08-1260", "text": " The project presented here is a part of a long term research program aiming at a full  lexicon grammar for Polish (SyntLex) . The main of this project is  computer-assisted acquisition and morpho-syntactic description of verb-noun collocations  in  Polish . We present methodology and resources obtained in three main project phases which are:  dictionary-based acquisition  of  collocation lexicon , feasibility study for  corpus-based lexicon enlargement  phase,  corpus-based lexicon enlargement  and  collocation description . In this paper we focus on the results of the third phase. The presented here  corpus-based approach  permitted us to triple the size the  verb-noun collocation dictionary for Polish . In the paper we describe the  SyntLex Dictionary of Collocations  and announce some future research intended to be a separate project continuation. ", "Comments": [], "entities": [{"id": 1090686, "label": "ENT", "start_offset": 88, "end_offset": 124}, {"id": 1090687, "label": "ENT", "start_offset": 156, "end_offset": 244}, {"id": 1090688, "label": "ENT", "start_offset": 250, "end_offset": 256}, {"id": 1090689, "label": "ENT", "start_offset": 327, "end_offset": 333}, {"id": 1090690, "label": "ENT", "start_offset": 346, "end_offset": 399}, {"id": 1090691, "label": "ENT", "start_offset": 402, "end_offset": 419}, {"id": 1090692, "label": "ENT", "start_offset": 425, "end_offset": 464}, {"id": 1090693, "label": "ENT", "start_offset": 467, "end_offset": 529}, {"id": 1090694, "label": "ENT", "start_offset": 610, "end_offset": 631}, {"id": 1090695, "label": "ENT", "start_offset": 670, "end_offset": 702}, {"id": 1090696, "label": "ENT", "start_offset": 707, "end_offset": 713}, {"id": 1090697, "label": "ENT", "start_offset": 746, "end_offset": 780}], "relations": [{"id": 174294, "from_id": 1090688, "to_id": 1090687, "type": "USED-FOR"}, {"id": 174295, "from_id": 1090690, "to_id": 1090689, "type": "HYPONYM-OF"}, {"id": 174296, "from_id": 1090693, "to_id": 1090689, "type": "HYPONYM-OF"}, {"id": 174297, "from_id": 1090696, "to_id": 1090695, "type": "FEATURE-OF"}, {"id": 174298, "from_id": 1090694, "to_id": 1090695, "type": "USED-FOR"}, {"id": 174299, "from_id": 1090691, "to_id": 1090692, "type": "USED-FOR"}, {"id": 174300, "from_id": 1090691, "to_id": 1090689, "type": "HYPONYM-OF"}, {"id": 174301, "from_id": 1090690, "to_id": 1090691, "type": "CONJUNCTION"}, {"id": 174302, "from_id": 1090693, "to_id": 1090691, "type": "CONJUNCTION"}]}
{"id": "CVPR_2010_10_abs", "text": "We present a new approach for building an efficient and robust classifier for the two class problem, that localizes objects that may appear in the image under different orien-tations. In contrast to other works that address this problem using multiple classifiers, each one specialized for a specific orientation, we propose a simple two-step approach with an estimation stage and a classification stage. The estimator yields an initial set of potential object poses that are then validated by the classifier. This methodology allows reducing the time complexity of the algorithm while classification results remain high. The classifier we use in both stages is based on a boosted combination of Random Ferns over local histograms of oriented gradients (HOGs), which we compute during a pre-processing step. Both the use of supervised learning and working on the gradient space makes our approach robust while being efficient at run-time. We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations, and with challenging conditions such as cluttered backgrounds, changing illumination conditions and partial occlusions.", "Comments": [], "entities": [{"id": 1090714, "label": "ENT", "start_offset": 17, "end_offset": 25}, {"id": 1090715, "label": "ENT", "start_offset": 63, "end_offset": 73}, {"id": 1090716, "label": "ENT", "start_offset": 86, "end_offset": 99}, {"id": 1090717, "label": "ENT", "start_offset": 147, "end_offset": 152}, {"id": 1090718, "label": "ENT", "start_offset": 169, "end_offset": 182}, {"id": 1090719, "label": "ENT", "start_offset": 229, "end_offset": 236}, {"id": 1090720, "label": "ENT", "start_offset": 252, "end_offset": 263}, {"id": 1090721, "label": "ENT", "start_offset": 343, "end_offset": 351}, {"id": 1090722, "label": "ENT", "start_offset": 360, "end_offset": 376}, {"id": 1090723, "label": "ENT", "start_offset": 383, "end_offset": 403}, {"id": 1090724, "label": "ENT", "start_offset": 409, "end_offset": 418}, {"id": 1090725, "label": "ENT", "start_offset": 454, "end_offset": 466}, {"id": 1090726, "label": "ENT", "start_offset": 498, "end_offset": 508}, {"id": 1090727, "label": "ENT", "start_offset": 547, "end_offset": 562}, {"id": 1090728, "label": "ENT", "start_offset": 570, "end_offset": 579}, {"id": 1090729, "label": "ENT", "start_offset": 586, "end_offset": 600}, {"id": 1090730, "label": "ENT", "start_offset": 626, "end_offset": 636}, {"id": 1090731, "label": "ENT", "start_offset": 673, "end_offset": 708}, {"id": 1090732, "label": "ENT", "start_offset": 714, "end_offset": 759}, {"id": 1090733, "label": "ENT", "start_offset": 787, "end_offset": 806}, {"id": 1090734, "label": "ENT", "start_offset": 824, "end_offset": 843}, {"id": 1090735, "label": "ENT", "start_offset": 863, "end_offset": 877}, {"id": 1090736, "label": "ENT", "start_offset": 888, "end_offset": 896}, {"id": 1090737, "label": "ENT", "start_offset": 1019, "end_offset": 1027}, {"id": 1090738, "label": "ENT", "start_offset": 1036, "end_offset": 1069}, {"id": 1090739, "label": "ENT", "start_offset": 1092, "end_offset": 1102}, {"id": 1090740, "label": "ENT", "start_offset": 1111, "end_offset": 1132}, {"id": 1090741, "label": "ENT", "start_offset": 1134, "end_offset": 1166}, {"id": 1090742, "label": "ENT", "start_offset": 1171, "end_offset": 1189}], "relations": [{"id": 174316, "from_id": 1090721, "to_id": 1090714, "type": "COREF"}, {"id": 174317, "from_id": 1090728, "to_id": 1090721, "type": "COREF"}, {"id": 174318, "from_id": 1090736, "to_id": 1090721, "type": "COREF"}, {"id": 174319, "from_id": 1090724, "to_id": 1090722, "type": "COREF"}, {"id": 174320, "from_id": 1090726, "to_id": 1090723, "type": "COREF"}, {"id": 174321, "from_id": 1090730, "to_id": 1090723, "type": "COREF"}, {"id": 174322, "from_id": 1090714, "to_id": 1090715, "type": "USED-FOR"}, {"id": 174323, "from_id": 1090739, "to_id": 1090737, "type": "FEATURE-OF"}, {"id": 174324, "from_id": 1090740, "to_id": 1090739, "type": "HYPONYM-OF"}, {"id": 174325, "from_id": 1090741, "to_id": 1090739, "type": "HYPONYM-OF"}, {"id": 174326, "from_id": 1090742, "to_id": 1090739, "type": "HYPONYM-OF"}, {"id": 174327, "from_id": 1090740, "to_id": 1090741, "type": "CONJUNCTION"}, {"id": 174328, "from_id": 1090741, "to_id": 1090742, "type": "CONJUNCTION"}, {"id": 174329, "from_id": 1090722, "to_id": 1090723, "type": "CONJUNCTION"}, {"id": 174330, "from_id": 1090715, "to_id": 1090716, "type": "USED-FOR"}, {"id": 174331, "from_id": 1090719, "to_id": 1090716, "type": "COREF"}, {"id": 174332, "from_id": 1090722, "to_id": 1090721, "type": "PART-OF"}, {"id": 174333, "from_id": 1090723, "to_id": 1090721, "type": "PART-OF"}, {"id": 174334, "from_id": 1090726, "to_id": 1090725, "type": "USED-FOR"}, {"id": 174335, "from_id": 1090727, "to_id": 1090728, "type": "EVALUATE-FOR"}, {"id": 174336, "from_id": 1090731, "to_id": 1090730, "type": "USED-FOR"}, {"id": 174337, "from_id": 1090732, "to_id": 1090731, "type": "FEATURE-OF"}, {"id": 174338, "from_id": 1090733, "to_id": 1090732, "type": "USED-FOR"}, {"id": 174339, "from_id": 1090734, "to_id": 1090736, "type": "USED-FOR"}, {"id": 174340, "from_id": 1090735, "to_id": 1090736, "type": "USED-FOR"}, {"id": 174341, "from_id": 1090738, "to_id": 1090737, "type": "FEATURE-OF"}]}
{"id": "P03-1002", "text": " In this paper we present a novel, customizable :  IE paradigm  that takes advantage of  predicate-argument structures  . We also introduce a new way of automatically identifying  predicate argument structures  , which is central to our  IE paradigm  . It is based on: (1) an extended set of  features  ; and (2)  inductive decision tree learning  . The experimental results prove our claim that accurate  predicate-argument structures  enable high quality  IE  results. ", "Comments": [], "entities": [{"id": 1090743, "label": "ENT", "start_offset": 51, "end_offset": 62}, {"id": 1090744, "label": "ENT", "start_offset": 89, "end_offset": 118}, {"id": 1090745, "label": "ENT", "start_offset": 153, "end_offset": 209}, {"id": 1090746, "label": "ENT", "start_offset": 238, "end_offset": 249}, {"id": 1090747, "label": "ENT", "start_offset": 253, "end_offset": 255}, {"id": 1090748, "label": "ENT", "start_offset": 293, "end_offset": 301}, {"id": 1090749, "label": "ENT", "start_offset": 314, "end_offset": 346}, {"id": 1090750, "label": "ENT", "start_offset": 406, "end_offset": 435}, {"id": 1090751, "label": "ENT", "start_offset": 458, "end_offset": 460}], "relations": [{"id": 174342, "from_id": 1090744, "to_id": 1090743, "type": "USED-FOR"}, {"id": 174343, "from_id": 1090748, "to_id": 1090749, "type": "CONJUNCTION"}, {"id": 174344, "from_id": 1090747, "to_id": 1090745, "type": "COREF"}, {"id": 174345, "from_id": 1090748, "to_id": 1090747, "type": "USED-FOR"}, {"id": 174346, "from_id": 1090749, "to_id": 1090747, "type": "USED-FOR"}, {"id": 174347, "from_id": 1090750, "to_id": 1090751, "type": "USED-FOR"}]}
{"id": "CVPR_2011_293_abs", "text": "Color is known to be highly discriminative for many object recognition tasks, but is difficult to infer from uncontrolled images in which the illuminant is not known. Traditional methods for color constancy can improve surface re-flectance estimates from such uncalibrated images, but their output depends significantly on the background scene. In many recognition and retrieval applications, we have access to image sets that contain multiple views of the same object in different environments; we show in this paper that correspondences between these images provide important constraints that can improve color constancy. We introduce the multi-view color constancy problem, and present a method to recover estimates of underlying surface re-flectance based on joint estimation of these surface properties and the illuminants present in multiple images. The method can exploit image correspondences obtained by various alignment techniques, and we show examples based on matching local region features. Our results show that multi-view constraints can significantly improve estimates of both scene illuminants and object color (surface reflectance) when compared to a baseline single-view method.", "Comments": [], "entities": [{"id": 1090752, "label": "ENT", "start_offset": 52, "end_offset": 76}, {"id": 1090753, "label": "ENT", "start_offset": 109, "end_offset": 128}, {"id": 1090754, "label": "ENT", "start_offset": 142, "end_offset": 152}, {"id": 1090755, "label": "ENT", "start_offset": 179, "end_offset": 186}, {"id": 1090756, "label": "ENT", "start_offset": 191, "end_offset": 206}, {"id": 1090757, "label": "ENT", "start_offset": 219, "end_offset": 249}, {"id": 1090758, "label": "ENT", "start_offset": 260, "end_offset": 279}, {"id": 1090759, "label": "ENT", "start_offset": 327, "end_offset": 343}, {"id": 1090760, "label": "ENT", "start_offset": 353, "end_offset": 391}, {"id": 1090761, "label": "ENT", "start_offset": 607, "end_offset": 622}, {"id": 1090762, "label": "ENT", "start_offset": 641, "end_offset": 675}, {"id": 1090763, "label": "ENT", "start_offset": 691, "end_offset": 697}, {"id": 1090764, "label": "ENT", "start_offset": 709, "end_offset": 753}, {"id": 1090765, "label": "ENT", "start_offset": 789, "end_offset": 807}, {"id": 1090766, "label": "ENT", "start_offset": 816, "end_offset": 827}, {"id": 1090767, "label": "ENT", "start_offset": 860, "end_offset": 866}, {"id": 1090768, "label": "ENT", "start_offset": 879, "end_offset": 900}, {"id": 1090769, "label": "ENT", "start_offset": 921, "end_offset": 941}, {"id": 1090770, "label": "ENT", "start_offset": 973, "end_offset": 1003}, {"id": 1090771, "label": "ENT", "start_offset": 1027, "end_offset": 1049}, {"id": 1090772, "label": "ENT", "start_offset": 1076, "end_offset": 1150}, {"id": 1090773, "label": "ENT", "start_offset": 1170, "end_offset": 1197}], "relations": [{"id": 174348, "from_id": 1090755, "to_id": 1090756, "type": "USED-FOR"}, {"id": 174349, "from_id": 1090755, "to_id": 1090757, "type": "USED-FOR"}, {"id": 174350, "from_id": 1090758, "to_id": 1090757, "type": "USED-FOR"}, {"id": 174351, "from_id": 1090763, "to_id": 1090764, "type": "USED-FOR"}, {"id": 174352, "from_id": 1090767, "to_id": 1090768, "type": "USED-FOR"}, {"id": 174353, "from_id": 1090769, "to_id": 1090768, "type": "USED-FOR"}, {"id": 174354, "from_id": 1090771, "to_id": 1090772, "type": "USED-FOR"}, {"id": 174355, "from_id": 1090773, "to_id": 1090771, "type": "COMPARE"}, {"id": 174356, "from_id": 1090756, "to_id": 1090761, "type": "COREF"}, {"id": 174357, "from_id": 1090767, "to_id": 1090763, "type": "COREF"}]}
{"id": "P05-1067", "text": "  Syntax-based statistical machine translation (MT)  aims at applying  statistical models  to  structured data  . In this paper, we present a  syntax-based statistical machine translation system  based on a  probabilistic synchronous dependency insertion grammar  .  Synchronous dependency insertion grammars  are a version of  synchronous grammars  defined on  dependency trees  . We first introduce our approach to inducing such a  grammar  from  parallel corpora  . Second, we describe the  graphical model  for the  machine translation task  , which can also be viewed as a  stochastic tree-to-tree transducer  . We introduce a  polynomial time decoding algorithm  for the  model  . We evaluate the outputs of our  MT system  using the  NIST and Bleu automatic MT evaluation software  . The result shows that our system outperforms the  baseline system  based on the  IBM models  in both  translation speed and quality  . ", "Comments": [], "entities": [{"id": 1090805, "label": "ENT", "start_offset": 2, "end_offset": 51}, {"id": 1090806, "label": "ENT", "start_offset": 71, "end_offset": 89}, {"id": 1090807, "label": "ENT", "start_offset": 95, "end_offset": 110}, {"id": 1090808, "label": "ENT", "start_offset": 143, "end_offset": 194}, {"id": 1090809, "label": "ENT", "start_offset": 208, "end_offset": 262}, {"id": 1090810, "label": "ENT", "start_offset": 267, "end_offset": 308}, {"id": 1090811, "label": "ENT", "start_offset": 328, "end_offset": 348}, {"id": 1090812, "label": "ENT", "start_offset": 362, "end_offset": 378}, {"id": 1090813, "label": "ENT", "start_offset": 405, "end_offset": 413}, {"id": 1090814, "label": "ENT", "start_offset": 434, "end_offset": 441}, {"id": 1090815, "label": "ENT", "start_offset": 449, "end_offset": 465}, {"id": 1090816, "label": "ENT", "start_offset": 494, "end_offset": 509}, {"id": 1090817, "label": "ENT", "start_offset": 520, "end_offset": 544}, {"id": 1090818, "label": "ENT", "start_offset": 579, "end_offset": 613}, {"id": 1090819, "label": "ENT", "start_offset": 633, "end_offset": 667}, {"id": 1090820, "label": "ENT", "start_offset": 678, "end_offset": 683}, {"id": 1090821, "label": "ENT", "start_offset": 719, "end_offset": 728}, {"id": 1090822, "label": "ENT", "start_offset": 741, "end_offset": 787}, {"id": 1090823, "label": "ENT", "start_offset": 817, "end_offset": 823}, {"id": 1090824, "label": "ENT", "start_offset": 841, "end_offset": 856}, {"id": 1090825, "label": "ENT", "start_offset": 872, "end_offset": 882}, {"id": 1090826, "label": "ENT", "start_offset": 893, "end_offset": 922}], "relations": [{"id": 174380, "from_id": 1090807, "to_id": 1090806, "type": "USED-FOR"}, {"id": 174381, "from_id": 1090809, "to_id": 1090808, "type": "USED-FOR"}, {"id": 174382, "from_id": 1090816, "to_id": 1090817, "type": "USED-FOR"}, {"id": 174383, "from_id": 1090825, "to_id": 1090824, "type": "USED-FOR"}, {"id": 174384, "from_id": 1090806, "to_id": 1090805, "type": "USED-FOR"}, {"id": 174385, "from_id": 1090805, "to_id": 1090808, "type": "COREF"}, {"id": 174386, "from_id": 1090810, "to_id": 1090811, "type": "HYPONYM-OF"}, {"id": 174387, "from_id": 1090810, "to_id": 1090814, "type": "COREF"}, {"id": 174388, "from_id": 1090813, "to_id": 1090814, "type": "USED-FOR"}, {"id": 174389, "from_id": 1090815, "to_id": 1090814, "type": "USED-FOR"}, {"id": 174390, "from_id": 1090818, "to_id": 1090816, "type": "USED-FOR"}, {"id": 174391, "from_id": 1090816, "to_id": 1090820, "type": "COREF"}, {"id": 174392, "from_id": 1090819, "to_id": 1090820, "type": "USED-FOR"}, {"id": 174393, "from_id": 1090821, "to_id": 1090805, "type": "COREF"}, {"id": 174394, "from_id": 1090822, "to_id": 1090821, "type": "USED-FOR"}, {"id": 174395, "from_id": 1090821, "to_id": 1090823, "type": "COREF"}, {"id": 174396, "from_id": 1090823, "to_id": 1090824, "type": "COMPARE"}, {"id": 174397, "from_id": 1090826, "to_id": 1090824, "type": "EVALUATE-FOR"}, {"id": 174398, "from_id": 1090826, "to_id": 1090823, "type": "EVALUATE-FOR"}, {"id": 174399, "from_id": 1090809, "to_id": 1090810, "type": "COREF"}, {"id": 174400, "from_id": 1090812, "to_id": 1090810, "type": "FEATURE-OF"}]}
{"id": "ICCV_2005_50_abs", "text": "Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate the models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets.", "Comments": [], "entities": [{"id": 1090827, "label": "ENT", "start_offset": 8, "end_offset": 18}, {"id": 1090828, "label": "ENT", "start_offset": 22, "end_offset": 49}, {"id": 1090829, "label": "ENT", "start_offset": 58, "end_offset": 66}, {"id": 1090830, "label": "ENT", "start_offset": 135, "end_offset": 146}, {"id": 1090831, "label": "ENT", "start_offset": 162, "end_offset": 170}, {"id": 1090832, "label": "ENT", "start_offset": 189, "end_offset": 204}, {"id": 1090833, "label": "ENT", "start_offset": 256, "end_offset": 276}, {"id": 1090834, "label": "ENT", "start_offset": 321, "end_offset": 326}, {"id": 1090835, "label": "ENT", "start_offset": 328, "end_offset": 336}, {"id": 1090836, "label": "ENT", "start_offset": 352, "end_offset": 356}, {"id": 1090837, "label": "ENT", "start_offset": 372, "end_offset": 384}, {"id": 1090838, "label": "ENT", "start_offset": 397, "end_offset": 416}, {"id": 1090839, "label": "ENT", "start_offset": 466, "end_offset": 474}, {"id": 1090840, "label": "ENT", "start_offset": 495, "end_offset": 518}, {"id": 1090841, "label": "ENT", "start_offset": 543, "end_offset": 559}, {"id": 1090842, "label": "ENT", "start_offset": 572, "end_offset": 586}, {"id": 1090843, "label": "ENT", "start_offset": 604, "end_offset": 610}, {"id": 1090844, "label": "ENT", "start_offset": 623, "end_offset": 632}, {"id": 1090845, "label": "ENT", "start_offset": 680, "end_offset": 687}, {"id": 1090846, "label": "ENT", "start_offset": 699, "end_offset": 721}], "relations": [{"id": 174401, "from_id": 1090827, "to_id": 1090828, "type": "USED-FOR"}, {"id": 174402, "from_id": 1090831, "to_id": 1090832, "type": "USED-FOR"}, {"id": 174403, "from_id": 1090829, "to_id": 1090827, "type": "USED-FOR"}, {"id": 174404, "from_id": 1090831, "to_id": 1090834, "type": "COREF"}, {"id": 174405, "from_id": 1090834, "to_id": 1090835, "type": "COREF"}, {"id": 174406, "from_id": 1090836, "to_id": 1090835, "type": "USED-FOR"}, {"id": 174407, "from_id": 1090836, "to_id": 1090837, "type": "USED-FOR"}, {"id": 174408, "from_id": 1090835, "to_id": 1090839, "type": "COREF"}, {"id": 174409, "from_id": 1090839, "to_id": 1090840, "type": "USED-FOR"}, {"id": 174410, "from_id": 1090839, "to_id": 1090841, "type": "USED-FOR"}, {"id": 174411, "from_id": 1090840, "to_id": 1090841, "type": "CONJUNCTION"}, {"id": 174412, "from_id": 1090842, "to_id": 1090841, "type": "USED-FOR"}, {"id": 174413, "from_id": 1090833, "to_id": 1090842, "type": "COREF"}, {"id": 174414, "from_id": 1090839, "to_id": 1090843, "type": "COREF"}, {"id": 174415, "from_id": 1090844, "to_id": 1090843, "type": "EVALUATE-FOR"}, {"id": 174416, "from_id": 1090846, "to_id": 1090845, "type": "USED-FOR"}, {"id": 174417, "from_id": 1090845, "to_id": 1090843, "type": "COMPARE"}, {"id": 174418, "from_id": 1090844, "to_id": 1090845, "type": "EVALUATE-FOR"}, {"id": 174419, "from_id": 1090838, "to_id": 1090835, "type": "PART-OF"}]}
{"id": "P06-2110", "text": "   This paper examines what kind of  similarity  between  words  can be represented by what kind of  word vectors  in the  vector space model  . Through two experiments, three  methods for constructing word vectors  , i.e.,  LSA-based, cooccurrence-based and dictionary-based methods  , were compared in terms of the ability to represent two kinds of  similarity  , i.e.,  taxonomic similarity  and  associative similarity  . The result of the comparison was that the  dictionary-based word vectors  better reflect  taxonomic similarity  , while the  LSA-based and the cooccurrence-based word vectors  better reflect  associative similarity  . ", "Comments": [], "entities": [{"id": 1090874, "label": "ENT", "start_offset": 37, "end_offset": 63}, {"id": 1090875, "label": "ENT", "start_offset": 101, "end_offset": 113}, {"id": 1090876, "label": "ENT", "start_offset": 123, "end_offset": 141}, {"id": 1090877, "label": "ENT", "start_offset": 177, "end_offset": 184}, {"id": 1090878, "label": "ENT", "start_offset": 189, "end_offset": 214}, {"id": 1090879, "label": "ENT", "start_offset": 225, "end_offset": 283}, {"id": 1090880, "label": "ENT", "start_offset": 352, "end_offset": 362}, {"id": 1090881, "label": "ENT", "start_offset": 373, "end_offset": 393}, {"id": 1090882, "label": "ENT", "start_offset": 400, "end_offset": 422}, {"id": 1090883, "label": "ENT", "start_offset": 469, "end_offset": 498}, {"id": 1090884, "label": "ENT", "start_offset": 516, "end_offset": 536}, {"id": 1090885, "label": "ENT", "start_offset": 551, "end_offset": 600}, {"id": 1090886, "label": "ENT", "start_offset": 618, "end_offset": 640}], "relations": [{"id": 174444, "from_id": 1090879, "to_id": 1090880, "type": "USED-FOR"}, {"id": 174445, "from_id": 1090883, "to_id": 1090884, "type": "USED-FOR"}, {"id": 174446, "from_id": 1090885, "to_id": 1090886, "type": "USED-FOR"}, {"id": 174447, "from_id": 1090875, "to_id": 1090874, "type": "USED-FOR"}, {"id": 174448, "from_id": 1090876, "to_id": 1090875, "type": "USED-FOR"}, {"id": 174449, "from_id": 1090877, "to_id": 1090878, "type": "USED-FOR"}, {"id": 174450, "from_id": 1090881, "to_id": 1090880, "type": "HYPONYM-OF"}, {"id": 174451, "from_id": 1090882, "to_id": 1090880, "type": "HYPONYM-OF"}, {"id": 174452, "from_id": 1090879, "to_id": 1090877, "type": "COREF"}, {"id": 174453, "from_id": 1090881, "to_id": 1090882, "type": "CONJUNCTION"}]}
{"id": "C04-1058", "text": " Empirical experience and observations have shown us when powerful and highly tunable  classifiers  such as  maximum entropy classifiers ,  boosting  and  SVMs  are applied to  language processing tasks , it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point. To further improve performance, various  error correction mechanisms  have been developed, but in practice, most of them cannot be relied on to predictably improve performance on  unseen data ; indeed, depending upon the  test set , they are as likely to degrade accuracy as to improve it. This problem is especially severe if the  base classifier  has already been finely tuned. In recent work, we introduced  N-fold Templated Piped Correction, or NTPC (\"nitpick\") , an intriguing  error corrector  that is designed to work in these extreme operating conditions. Despite its simplicity, it consistently and robustly improves the accuracy of existing highly accurate  base models . This paper investigates some of the more surprising claims made by  NTPC , and presents experiments supporting an  Occam's Razor argument  that more complex models are damaging or unnecessary in practice. ", "Comments": [], "entities": [{"id": 1090887, "label": "ENT", "start_offset": 87, "end_offset": 98}, {"id": 1090888, "label": "ENT", "start_offset": 109, "end_offset": 136}, {"id": 1090889, "label": "ENT", "start_offset": 140, "end_offset": 148}, {"id": 1090890, "label": "ENT", "start_offset": 155, "end_offset": 159}, {"id": 1090891, "label": "ENT", "start_offset": 177, "end_offset": 202}, {"id": 1090892, "label": "ENT", "start_offset": 373, "end_offset": 400}, {"id": 1090893, "label": "ENT", "start_offset": 664, "end_offset": 679}, {"id": 1090894, "label": "ENT", "start_offset": 743, "end_offset": 797}, {"id": 1090895, "label": "ENT", "start_offset": 815, "end_offset": 830}, {"id": 1090896, "label": "ENT", "start_offset": 920, "end_offset": 922}, {"id": 1090897, "label": "ENT", "start_offset": 1000, "end_offset": 1011}, {"id": 1090898, "label": "ENT", "start_offset": 1082, "end_offset": 1086}, {"id": 1090899, "label": "ENT", "start_offset": 1129, "end_offset": 1151}], "relations": [{"id": 174454, "from_id": 1090888, "to_id": 1090887, "type": "HYPONYM-OF"}, {"id": 174455, "from_id": 1090889, "to_id": 1090887, "type": "HYPONYM-OF"}, {"id": 174456, "from_id": 1090890, "to_id": 1090887, "type": "HYPONYM-OF"}, {"id": 174457, "from_id": 1090887, "to_id": 1090891, "type": "USED-FOR"}, {"id": 174458, "from_id": 1090894, "to_id": 1090895, "type": "HYPONYM-OF"}, {"id": 174459, "from_id": 1090895, "to_id": 1090892, "type": "COREF"}, {"id": 174460, "from_id": 1090896, "to_id": 1090894, "type": "COREF"}, {"id": 174461, "from_id": 1090896, "to_id": 1090897, "type": "COMPARE"}, {"id": 174462, "from_id": 1090897, "to_id": 1090893, "type": "COREF"}, {"id": 174463, "from_id": 1090898, "to_id": 1090894, "type": "COREF"}, {"id": 174464, "from_id": 1090888, "to_id": 1090889, "type": "CONJUNCTION"}, {"id": 174465, "from_id": 1090889, "to_id": 1090890, "type": "CONJUNCTION"}]}
{"id": "ICCV_2001_47_abs", "text": "We propose to incorporate a priori geometric constraints in a 3\u2013D stereo reconstruction scheme to cope with the many cases where image information alone is not sufficient to accurately recover 3\u2013D shape. Our approach is based on the iterative deformation of a 3\u2013D surface mesh to minimize an objective function. We show that combining anisotropic meshing with a non-quadratic approach to regularization enables us to obtain satisfactory reconstruction results using triangulations with few vertices. Structural or numerical constraints can then be added locally to the reconstruction process through a constrained optimization scheme. They improve the reconstruction results and enforce their consistency with a priori knowledge about object shape. The strong description and modeling properties of differential features make them useful tools that can be efficiently used as constraints for 3\u2013D reconstruction .", "Comments": [], "entities": [{"id": 1090906, "label": "ENT", "start_offset": 28, "end_offset": 56}, {"id": 1090907, "label": "ENT", "start_offset": 62, "end_offset": 94}, {"id": 1090908, "label": "ENT", "start_offset": 129, "end_offset": 146}, {"id": 1090909, "label": "ENT", "start_offset": 193, "end_offset": 202}, {"id": 1090910, "label": "ENT", "start_offset": 208, "end_offset": 216}, {"id": 1090911, "label": "ENT", "start_offset": 233, "end_offset": 276}, {"id": 1090912, "label": "ENT", "start_offset": 292, "end_offset": 310}, {"id": 1090913, "label": "ENT", "start_offset": 335, "end_offset": 354}, {"id": 1090914, "label": "ENT", "start_offset": 362, "end_offset": 384}, {"id": 1090915, "label": "ENT", "start_offset": 388, "end_offset": 402}, {"id": 1090916, "label": "ENT", "start_offset": 437, "end_offset": 451}, {"id": 1090917, "label": "ENT", "start_offset": 466, "end_offset": 480}, {"id": 1090918, "label": "ENT", "start_offset": 490, "end_offset": 498}, {"id": 1090919, "label": "ENT", "start_offset": 500, "end_offset": 535}, {"id": 1090920, "label": "ENT", "start_offset": 569, "end_offset": 591}, {"id": 1090921, "label": "ENT", "start_offset": 602, "end_offset": 633}, {"id": 1090922, "label": "ENT", "start_offset": 635, "end_offset": 639}, {"id": 1090923, "label": "ENT", "start_offset": 652, "end_offset": 666}, {"id": 1090924, "label": "ENT", "start_offset": 712, "end_offset": 728}, {"id": 1090925, "label": "ENT", "start_offset": 735, "end_offset": 747}, {"id": 1090926, "label": "ENT", "start_offset": 776, "end_offset": 795}, {"id": 1090927, "label": "ENT", "start_offset": 799, "end_offset": 820}, {"id": 1090928, "label": "ENT", "start_offset": 826, "end_offset": 830}, {"id": 1090929, "label": "ENT", "start_offset": 892, "end_offset": 910}], "relations": [{"id": 174470, "from_id": 1090908, "to_id": 1090909, "type": "USED-FOR"}, {"id": 174471, "from_id": 1090911, "to_id": 1090910, "type": "USED-FOR"}, {"id": 174472, "from_id": 1090911, "to_id": 1090912, "type": "USED-FOR"}, {"id": 174473, "from_id": 1090913, "to_id": 1090914, "type": "CONJUNCTION"}, {"id": 174474, "from_id": 1090914, "to_id": 1090915, "type": "USED-FOR"}, {"id": 174475, "from_id": 1090906, "to_id": 1090907, "type": "PART-OF"}, {"id": 174476, "from_id": 1090913, "to_id": 1090916, "type": "USED-FOR"}, {"id": 174477, "from_id": 1090914, "to_id": 1090916, "type": "USED-FOR"}, {"id": 174478, "from_id": 1090917, "to_id": 1090916, "type": "USED-FOR"}, {"id": 174479, "from_id": 1090919, "to_id": 1090920, "type": "USED-FOR"}, {"id": 174480, "from_id": 1090921, "to_id": 1090919, "type": "USED-FOR"}, {"id": 174481, "from_id": 1090923, "to_id": 1090916, "type": "COREF"}, {"id": 174482, "from_id": 1090919, "to_id": 1090922, "type": "COREF"}, {"id": 174483, "from_id": 1090922, "to_id": 1090923, "type": "USED-FOR"}, {"id": 174484, "from_id": 1090925, "to_id": 1090924, "type": "FEATURE-OF"}, {"id": 174485, "from_id": 1090928, "to_id": 1090922, "type": "COREF"}, {"id": 174486, "from_id": 1090928, "to_id": 1090929, "type": "USED-FOR"}, {"id": 174487, "from_id": 1090929, "to_id": 1090923, "type": "COREF"}]}
{"id": "A00-1024", "text": " This paper introduces a  system for categorizing unknown words . The  system  is based on a  multi-component architecture  where each  component  is responsible for identifying one class of  unknown words . The focus of this paper is the  components  that identify  names  and  spelling errors . Each  component  uses a  decision tree architecture  to combine multiple types of  evidence  about the  unknown word . The  system  is evaluated using data from  live closed captions  - a genre replete with a wide variety of  unknown words . ", "Comments": [], "entities": [{"id": 1090980, "label": "ENT", "start_offset": 26, "end_offset": 32}, {"id": 1090981, "label": "ENT", "start_offset": 37, "end_offset": 63}, {"id": 1090982, "label": "ENT", "start_offset": 71, "end_offset": 77}, {"id": 1090983, "label": "ENT", "start_offset": 94, "end_offset": 122}, {"id": 1090984, "label": "ENT", "start_offset": 136, "end_offset": 145}, {"id": 1090985, "label": "ENT", "start_offset": 192, "end_offset": 205}, {"id": 1090986, "label": "ENT", "start_offset": 240, "end_offset": 250}, {"id": 1090987, "label": "ENT", "start_offset": 267, "end_offset": 272}, {"id": 1090988, "label": "ENT", "start_offset": 279, "end_offset": 294}, {"id": 1090989, "label": "ENT", "start_offset": 303, "end_offset": 312}, {"id": 1090990, "label": "ENT", "start_offset": 322, "end_offset": 348}, {"id": 1090991, "label": "ENT", "start_offset": 401, "end_offset": 413}, {"id": 1090992, "label": "ENT", "start_offset": 421, "end_offset": 427}, {"id": 1090993, "label": "ENT", "start_offset": 459, "end_offset": 479}, {"id": 1090994, "label": "ENT", "start_offset": 523, "end_offset": 536}], "relations": [{"id": 174523, "from_id": 1090983, "to_id": 1090982, "type": "USED-FOR"}, {"id": 174524, "from_id": 1090986, "to_id": 1090987, "type": "USED-FOR"}, {"id": 174525, "from_id": 1090990, "to_id": 1090989, "type": "USED-FOR"}, {"id": 174526, "from_id": 1090984, "to_id": 1090983, "type": "PART-OF"}, {"id": 174527, "from_id": 1090986, "to_id": 1090988, "type": "USED-FOR"}, {"id": 174528, "from_id": 1090986, "to_id": 1090984, "type": "HYPONYM-OF"}, {"id": 174529, "from_id": 1090992, "to_id": 1090982, "type": "COREF"}, {"id": 174530, "from_id": 1090984, "to_id": 1090985, "type": "USED-FOR"}, {"id": 174531, "from_id": 1090993, "to_id": 1090992, "type": "EVALUATE-FOR"}, {"id": 174532, "from_id": 1090987, "to_id": 1090988, "type": "CONJUNCTION"}, {"id": 174533, "from_id": 1090989, "to_id": 1090986, "type": "COREF"}, {"id": 174534, "from_id": 1090980, "to_id": 1090981, "type": "USED-FOR"}, {"id": 174535, "from_id": 1090980, "to_id": 1090982, "type": "COREF"}]}
{"id": "CVPR_2004_18_abs", "text": "The automated segmentation of images into semantically meaningful parts requires shape information since low-level feature analysis alone often fails to reach this goal. We introduce a novel method of shape constrained image segmentation which is based on mixtures of feature distributions for color and texture as well as probabilistic shape knowledge. The combined approach is formulated in the framework of Bayesian statistics to account for the robust-ness requirement in image understanding. Experimental evidence shows that semantically meaningful segments are inferred , even when image data alone gives rise to ambiguous segmentations.", "Comments": [], "entities": [{"id": 1090995, "label": "ENT", "start_offset": 4, "end_offset": 26}, {"id": 1090996, "label": "ENT", "start_offset": 30, "end_offset": 36}, {"id": 1090997, "label": "ENT", "start_offset": 81, "end_offset": 98}, {"id": 1090998, "label": "ENT", "start_offset": 105, "end_offset": 131}, {"id": 1090999, "label": "ENT", "start_offset": 191, "end_offset": 197}, {"id": 1091000, "label": "ENT", "start_offset": 201, "end_offset": 237}, {"id": 1091001, "label": "ENT", "start_offset": 256, "end_offset": 289}, {"id": 1091002, "label": "ENT", "start_offset": 294, "end_offset": 299}, {"id": 1091003, "label": "ENT", "start_offset": 304, "end_offset": 311}, {"id": 1091004, "label": "ENT", "start_offset": 323, "end_offset": 352}, {"id": 1091005, "label": "ENT", "start_offset": 367, "end_offset": 375}, {"id": 1091006, "label": "ENT", "start_offset": 410, "end_offset": 429}, {"id": 1091007, "label": "ENT", "start_offset": 449, "end_offset": 495}, {"id": 1091008, "label": "ENT", "start_offset": 588, "end_offset": 598}], "relations": [{"id": 174536, "from_id": 1090996, "to_id": 1090995, "type": "USED-FOR"}, {"id": 174537, "from_id": 1090999, "to_id": 1091000, "type": "USED-FOR"}, {"id": 174538, "from_id": 1091002, "to_id": 1091003, "type": "CONJUNCTION"}, {"id": 174539, "from_id": 1091003, "to_id": 1091004, "type": "CONJUNCTION"}, {"id": 174540, "from_id": 1091001, "to_id": 1090999, "type": "USED-FOR"}, {"id": 174541, "from_id": 1091001, "to_id": 1091002, "type": "USED-FOR"}, {"id": 174542, "from_id": 1091001, "to_id": 1091003, "type": "USED-FOR"}, {"id": 174543, "from_id": 1091001, "to_id": 1091004, "type": "USED-FOR"}, {"id": 174544, "from_id": 1091005, "to_id": 1090999, "type": "COREF"}, {"id": 174545, "from_id": 1091006, "to_id": 1091005, "type": "USED-FOR"}, {"id": 174546, "from_id": 1091005, "to_id": 1091007, "type": "USED-FOR"}]}
{"id": "CVPR_2008_259_abs", "text": "In this paper, we develop a geometric framework for linear or nonlinear discriminant subspace learning and classification. In our framework, the structures of classes are conceptualized as a semi-Riemannian manifold which is considered as a submanifold embedded in an ambient semi-Riemannian space. The class structures of original samples can be characterized and deformed by local metrics of the semi-Riemannian space. Semi-Riemannian metrics are uniquely determined by the smoothing of discrete functions and the nullity of the semi-Riemannian space. Based on the geometrization of class structures, optimizing class structures in the feature space is equivalent to maximizing the quadratic quantities of metric tensors in the semi-Riemannian space. Thus supervised discriminant subspace learning reduces to unsupervised semi-Riemannian mani-fold learning. Based on the proposed framework, a novel algorithm, dubbed as Semi-Riemannian Discriminant Analysis (SRDA), is presented for subspace-based classification. The performance of SRDA is tested on face recognition (singular case) and handwritten capital letter classification (nonsingular case) against existing algorithms. The experimental results show that SRDA works well on recognition and classification, implying that semi-Riemannian geometry is a promising new tool for pattern recognition and machine learning.", "Comments": [], "entities": [{"id": 1091029, "label": "ENT", "start_offset": 28, "end_offset": 47}, {"id": 1091030, "label": "ENT", "start_offset": 52, "end_offset": 121}, {"id": 1091031, "label": "ENT", "start_offset": 130, "end_offset": 139}, {"id": 1091032, "label": "ENT", "start_offset": 145, "end_offset": 166}, {"id": 1091033, "label": "ENT", "start_offset": 191, "end_offset": 215}, {"id": 1091034, "label": "ENT", "start_offset": 241, "end_offset": 252}, {"id": 1091035, "label": "ENT", "start_offset": 268, "end_offset": 297}, {"id": 1091036, "label": "ENT", "start_offset": 303, "end_offset": 319}, {"id": 1091037, "label": "ENT", "start_offset": 377, "end_offset": 419}, {"id": 1091038, "label": "ENT", "start_offset": 421, "end_offset": 444}, {"id": 1091039, "label": "ENT", "start_offset": 476, "end_offset": 507}, {"id": 1091040, "label": "ENT", "start_offset": 516, "end_offset": 552}, {"id": 1091041, "label": "ENT", "start_offset": 567, "end_offset": 601}, {"id": 1091042, "label": "ENT", "start_offset": 614, "end_offset": 630}, {"id": 1091043, "label": "ENT", "start_offset": 638, "end_offset": 651}, {"id": 1091044, "label": "ENT", "start_offset": 684, "end_offset": 722}, {"id": 1091045, "label": "ENT", "start_offset": 730, "end_offset": 751}, {"id": 1091046, "label": "ENT", "start_offset": 758, "end_offset": 799}, {"id": 1091047, "label": "ENT", "start_offset": 811, "end_offset": 858}, {"id": 1091048, "label": "ENT", "start_offset": 882, "end_offset": 891}, {"id": 1091049, "label": "ENT", "start_offset": 901, "end_offset": 910}, {"id": 1091050, "label": "ENT", "start_offset": 922, "end_offset": 966}, {"id": 1091051, "label": "ENT", "start_offset": 985, "end_offset": 1014}, {"id": 1091052, "label": "ENT", "start_offset": 1035, "end_offset": 1039}, {"id": 1091053, "label": "ENT", "start_offset": 1053, "end_offset": 1084}, {"id": 1091054, "label": "ENT", "start_offset": 1090, "end_offset": 1150}, {"id": 1091055, "label": "ENT", "start_offset": 1168, "end_offset": 1178}, {"id": 1091056, "label": "ENT", "start_offset": 1215, "end_offset": 1219}, {"id": 1091057, "label": "ENT", "start_offset": 1234, "end_offset": 1245}, {"id": 1091058, "label": "ENT", "start_offset": 1250, "end_offset": 1264}, {"id": 1091059, "label": "ENT", "start_offset": 1280, "end_offset": 1304}, {"id": 1091060, "label": "ENT", "start_offset": 1333, "end_offset": 1352}, {"id": 1091061, "label": "ENT", "start_offset": 1357, "end_offset": 1373}], "relations": [{"id": 174564, "from_id": 1091031, "to_id": 1091029, "type": "COREF"}, {"id": 174565, "from_id": 1091029, "to_id": 1091030, "type": "USED-FOR"}, {"id": 174566, "from_id": 1091033, "to_id": 1091032, "type": "USED-FOR"}, {"id": 174567, "from_id": 1091034, "to_id": 1091035, "type": "PART-OF"}, {"id": 174568, "from_id": 1091037, "to_id": 1091036, "type": "USED-FOR"}, {"id": 174569, "from_id": 1091038, "to_id": 1091037, "type": "COREF"}, {"id": 174570, "from_id": 1091039, "to_id": 1091038, "type": "USED-FOR"}, {"id": 174571, "from_id": 1091040, "to_id": 1091038, "type": "USED-FOR"}, {"id": 174572, "from_id": 1091043, "to_id": 1091042, "type": "FEATURE-OF"}, {"id": 174573, "from_id": 1091045, "to_id": 1091044, "type": "FEATURE-OF"}, {"id": 174574, "from_id": 1091050, "to_id": 1091049, "type": "COREF"}, {"id": 174575, "from_id": 1091049, "to_id": 1091051, "type": "USED-FOR"}, {"id": 174576, "from_id": 1091029, "to_id": 1091048, "type": "COREF"}, {"id": 174577, "from_id": 1091052, "to_id": 1091050, "type": "COREF"}, {"id": 174578, "from_id": 1091048, "to_id": 1091049, "type": "USED-FOR"}, {"id": 174579, "from_id": 1091053, "to_id": 1091052, "type": "EVALUATE-FOR"}, {"id": 174580, "from_id": 1091054, "to_id": 1091052, "type": "EVALUATE-FOR"}, {"id": 174581, "from_id": 1091053, "to_id": 1091054, "type": "CONJUNCTION"}, {"id": 174582, "from_id": 1091052, "to_id": 1091055, "type": "COMPARE"}, {"id": 174583, "from_id": 1091050, "to_id": 1091056, "type": "COREF"}, {"id": 174584, "from_id": 1091053, "to_id": 1091055, "type": "EVALUATE-FOR"}, {"id": 174585, "from_id": 1091054, "to_id": 1091055, "type": "EVALUATE-FOR"}, {"id": 174586, "from_id": 1091056, "to_id": 1091057, "type": "USED-FOR"}, {"id": 174587, "from_id": 1091056, "to_id": 1091058, "type": "USED-FOR"}, {"id": 174588, "from_id": 1091057, "to_id": 1091058, "type": "CONJUNCTION"}, {"id": 174589, "from_id": 1091060, "to_id": 1091061, "type": "CONJUNCTION"}, {"id": 174590, "from_id": 1091059, "to_id": 1091060, "type": "USED-FOR"}, {"id": 174591, "from_id": 1091059, "to_id": 1091061, "type": "USED-FOR"}, {"id": 174592, "from_id": 1091039, "to_id": 1091040, "type": "CONJUNCTION"}]}
{"id": "A92-1026", "text": "   It is often assumed that when  natural language processing  meets the real world, the ideal of aiming for complete and correct interpretations has to be abandoned. However, our experience with  TACITUS  ; especially in the  MUC-3 evaluation  , has shown that principled techniques for  syntactic and pragmatic analysis  can be bolstered with methods for achieving robustness. We describe three techniques for making  syntactic analysis  more robust---an  agenda-based scheduling parser  , a  recovery technique for failed parses  , and a new technique called  terminal substring parsing  . For  pragmatics processing  , we describe how the method of  abductive inference  is inherently robust, in that an interpretation is always possible, so that in the absence of the required  world knowledge  , performance degrades gracefully. Each of these techniques have been evaluated and the results of the evaluations are presented. ", "Comments": [], "entities": [{"id": 1091110, "label": "ENT", "start_offset": 34, "end_offset": 61}, {"id": 1091111, "label": "ENT", "start_offset": 197, "end_offset": 204}, {"id": 1091112, "label": "ENT", "start_offset": 227, "end_offset": 243}, {"id": 1091113, "label": "ENT", "start_offset": 273, "end_offset": 283}, {"id": 1091114, "label": "ENT", "start_offset": 289, "end_offset": 321}, {"id": 1091115, "label": "ENT", "start_offset": 345, "end_offset": 352}, {"id": 1091116, "label": "ENT", "start_offset": 367, "end_offset": 377}, {"id": 1091117, "label": "ENT", "start_offset": 391, "end_offset": 407}, {"id": 1091118, "label": "ENT", "start_offset": 420, "end_offset": 438}, {"id": 1091119, "label": "ENT", "start_offset": 458, "end_offset": 488}, {"id": 1091120, "label": "ENT", "start_offset": 495, "end_offset": 513}, {"id": 1091121, "label": "ENT", "start_offset": 518, "end_offset": 531}, {"id": 1091122, "label": "ENT", "start_offset": 545, "end_offset": 554}, {"id": 1091123, "label": "ENT", "start_offset": 563, "end_offset": 589}, {"id": 1091124, "label": "ENT", "start_offset": 598, "end_offset": 619}, {"id": 1091125, "label": "ENT", "start_offset": 654, "end_offset": 673}, {"id": 1091126, "label": "ENT", "start_offset": 783, "end_offset": 798}, {"id": 1091127, "label": "ENT", "start_offset": 843, "end_offset": 859}], "relations": [{"id": 174633, "from_id": 1091125, "to_id": 1091124, "type": "USED-FOR"}, {"id": 174634, "from_id": 1091124, "to_id": 1091114, "type": "HYPONYM-OF"}, {"id": 174635, "from_id": 1091118, "to_id": 1091114, "type": "HYPONYM-OF"}, {"id": 174636, "from_id": 1091119, "to_id": 1091117, "type": "HYPONYM-OF"}, {"id": 174637, "from_id": 1091117, "to_id": 1091127, "type": "HYPONYM-OF"}, {"id": 174638, "from_id": 1091125, "to_id": 1091127, "type": "HYPONYM-OF"}, {"id": 174639, "from_id": 1091113, "to_id": 1091114, "type": "USED-FOR"}, {"id": 174640, "from_id": 1091116, "to_id": 1091115, "type": "EVALUATE-FOR"}, {"id": 174641, "from_id": 1091123, "to_id": 1091122, "type": "COREF"}, {"id": 174642, "from_id": 1091122, "to_id": 1091117, "type": "HYPONYM-OF"}, {"id": 174643, "from_id": 1091120, "to_id": 1091121, "type": "USED-FOR"}, {"id": 174644, "from_id": 1091119, "to_id": 1091120, "type": "CONJUNCTION"}, {"id": 174645, "from_id": 1091120, "to_id": 1091122, "type": "CONJUNCTION"}, {"id": 174646, "from_id": 1091120, "to_id": 1091117, "type": "HYPONYM-OF"}, {"id": 174647, "from_id": 1091120, "to_id": 1091118, "type": "USED-FOR"}, {"id": 174648, "from_id": 1091117, "to_id": 1091118, "type": "USED-FOR"}]}
{"id": "P05-1058", "text": " This paper proposes an  alignment adaptation approach  to improve  domain-specific (in-domain) word alignment . The basic idea of  alignment adaptation  is to use  out-of-domain corpus  to improve  in-domain word alignment  results. In this paper, we first train two  statistical word alignment models  with the large-scale  out-of-domain corpus  and the small-scale  in-domain corpus  respectively, and then interpolate these two models to improve the  domain-specific word alignment . Experimental results show that our approach improves  domain-specific word alignment  in terms of both  precision  and  recall , achieving a  relative error rate reduction  of 6.56% as compared with the state-of-the-art technologies. ", "Comments": [], "entities": [{"id": 1091128, "label": "ENT", "start_offset": 25, "end_offset": 54}, {"id": 1091129, "label": "ENT", "start_offset": 68, "end_offset": 110}, {"id": 1091130, "label": "ENT", "start_offset": 132, "end_offset": 152}, {"id": 1091131, "label": "ENT", "start_offset": 165, "end_offset": 185}, {"id": 1091132, "label": "ENT", "start_offset": 199, "end_offset": 223}, {"id": 1091133, "label": "ENT", "start_offset": 269, "end_offset": 302}, {"id": 1091134, "label": "ENT", "start_offset": 313, "end_offset": 346}, {"id": 1091135, "label": "ENT", "start_offset": 356, "end_offset": 385}, {"id": 1091136, "label": "ENT", "start_offset": 432, "end_offset": 438}, {"id": 1091137, "label": "ENT", "start_offset": 455, "end_offset": 485}, {"id": 1091138, "label": "ENT", "start_offset": 523, "end_offset": 531}, {"id": 1091139, "label": "ENT", "start_offset": 542, "end_offset": 572}, {"id": 1091140, "label": "ENT", "start_offset": 592, "end_offset": 601}, {"id": 1091141, "label": "ENT", "start_offset": 608, "end_offset": 614}, {"id": 1091142, "label": "ENT", "start_offset": 630, "end_offset": 659}, {"id": 1091143, "label": "ENT", "start_offset": 691, "end_offset": 720}], "relations": [{"id": 174649, "from_id": 1091128, "to_id": 1091129, "type": "USED-FOR"}, {"id": 174650, "from_id": 1091128, "to_id": 1091130, "type": "COREF"}, {"id": 174651, "from_id": 1091129, "to_id": 1091132, "type": "COREF"}, {"id": 174652, "from_id": 1091131, "to_id": 1091130, "type": "USED-FOR"}, {"id": 174653, "from_id": 1091130, "to_id": 1091132, "type": "USED-FOR"}, {"id": 174654, "from_id": 1091134, "to_id": 1091133, "type": "USED-FOR"}, {"id": 174655, "from_id": 1091134, "to_id": 1091135, "type": "CONJUNCTION"}, {"id": 174656, "from_id": 1091135, "to_id": 1091133, "type": "USED-FOR"}, {"id": 174657, "from_id": 1091133, "to_id": 1091136, "type": "COREF"}, {"id": 174658, "from_id": 1091136, "to_id": 1091137, "type": "USED-FOR"}, {"id": 174659, "from_id": 1091132, "to_id": 1091137, "type": "COREF"}, {"id": 174660, "from_id": 1091138, "to_id": 1091139, "type": "USED-FOR"}, {"id": 174661, "from_id": 1091140, "to_id": 1091141, "type": "CONJUNCTION"}, {"id": 174662, "from_id": 1091140, "to_id": 1091138, "type": "EVALUATE-FOR"}, {"id": 174663, "from_id": 1091141, "to_id": 1091138, "type": "EVALUATE-FOR"}, {"id": 174664, "from_id": 1091138, "to_id": 1091143, "type": "COMPARE"}, {"id": 174665, "from_id": 1091142, "to_id": 1091143, "type": "EVALUATE-FOR"}, {"id": 174666, "from_id": 1091142, "to_id": 1091138, "type": "EVALUATE-FOR"}, {"id": 174667, "from_id": 1091139, "to_id": 1091132, "type": "COREF"}]}
{"id": "CVPR_2014_21_abs", "text": "In this paper we present a novel autonomous pipeline to build a personalized parametric model (pose-driven avatar) using a single depth sensor. Our method first captures a few high-quality scans of the user rotating herself at multiple poses from different views. We fit each incomplete scan using template fitting techniques with a generic human template, and register all scans to every pose using global consistency constraints. After registration, these watertight models with different poses are used to train a parametric model in a fashion similar to the SCAPE method. Once the parametric model is built, it can be used as an anim-itable avatar or more interestingly synthesizing dynamic 3D models from single-view depth videos. Experimental results demonstrate the effectiveness of our system to produce dynamic models.", "Comments": [], "entities": [{"id": 1091144, "label": "ENT", "start_offset": 33, "end_offset": 52}, {"id": 1091145, "label": "ENT", "start_offset": 64, "end_offset": 114}, {"id": 1091146, "label": "ENT", "start_offset": 123, "end_offset": 142}, {"id": 1091147, "label": "ENT", "start_offset": 148, "end_offset": 154}, {"id": 1091148, "label": "ENT", "start_offset": 298, "end_offset": 325}, {"id": 1091149, "label": "ENT", "start_offset": 341, "end_offset": 355}, {"id": 1091150, "label": "ENT", "start_offset": 400, "end_offset": 430}, {"id": 1091151, "label": "ENT", "start_offset": 458, "end_offset": 475}, {"id": 1091152, "label": "ENT", "start_offset": 517, "end_offset": 533}, {"id": 1091153, "label": "ENT", "start_offset": 562, "end_offset": 574}, {"id": 1091154, "label": "ENT", "start_offset": 585, "end_offset": 601}, {"id": 1091155, "label": "ENT", "start_offset": 612, "end_offset": 614}, {"id": 1091156, "label": "ENT", "start_offset": 633, "end_offset": 651}, {"id": 1091157, "label": "ENT", "start_offset": 687, "end_offset": 704}, {"id": 1091158, "label": "ENT", "start_offset": 710, "end_offset": 734}, {"id": 1091159, "label": "ENT", "start_offset": 794, "end_offset": 800}, {"id": 1091160, "label": "ENT", "start_offset": 812, "end_offset": 826}], "relations": [{"id": 174668, "from_id": 1091144, "to_id": 1091145, "type": "USED-FOR"}, {"id": 174669, "from_id": 1091147, "to_id": 1091144, "type": "COREF"}, {"id": 174670, "from_id": 1091146, "to_id": 1091144, "type": "USED-FOR"}, {"id": 174671, "from_id": 1091149, "to_id": 1091148, "type": "USED-FOR"}, {"id": 174672, "from_id": 1091151, "to_id": 1091152, "type": "USED-FOR"}, {"id": 174673, "from_id": 1091153, "to_id": 1091152, "type": "USED-FOR"}, {"id": 174674, "from_id": 1091152, "to_id": 1091154, "type": "COREF"}, {"id": 174675, "from_id": 1091154, "to_id": 1091155, "type": "COREF"}, {"id": 174676, "from_id": 1091158, "to_id": 1091157, "type": "USED-FOR"}, {"id": 174677, "from_id": 1091155, "to_id": 1091157, "type": "USED-FOR"}, {"id": 174678, "from_id": 1091155, "to_id": 1091156, "type": "USED-FOR"}, {"id": 174679, "from_id": 1091159, "to_id": 1091160, "type": "USED-FOR"}, {"id": 174680, "from_id": 1091147, "to_id": 1091159, "type": "COREF"}]}
{"id": "H01-1070", "text": " This paper proposes a practical approach employing  n-gram models  and  error-correction rules  for  Thai key prediction  and  Thai-English language identification  . The paper also proposes  rule-reduction algorithm  applying  mutual information  to reduce the  error-correction rules  . Our algorithm reported more than 99%  accuracy  in both  language identification  and  key prediction  . ", "Comments": [], "entities": [{"id": 1091183, "label": "ENT", "start_offset": 33, "end_offset": 41}, {"id": 1091184, "label": "ENT", "start_offset": 53, "end_offset": 66}, {"id": 1091185, "label": "ENT", "start_offset": 73, "end_offset": 95}, {"id": 1091186, "label": "ENT", "start_offset": 102, "end_offset": 121}, {"id": 1091187, "label": "ENT", "start_offset": 128, "end_offset": 164}, {"id": 1091188, "label": "ENT", "start_offset": 193, "end_offset": 217}, {"id": 1091189, "label": "ENT", "start_offset": 229, "end_offset": 247}, {"id": 1091190, "label": "ENT", "start_offset": 264, "end_offset": 286}, {"id": 1091191, "label": "ENT", "start_offset": 294, "end_offset": 303}, {"id": 1091192, "label": "ENT", "start_offset": 328, "end_offset": 336}, {"id": 1091193, "label": "ENT", "start_offset": 347, "end_offset": 370}, {"id": 1091194, "label": "ENT", "start_offset": 377, "end_offset": 391}], "relations": [{"id": 174698, "from_id": 1091189, "to_id": 1091190, "type": "USED-FOR"}, {"id": 174699, "from_id": 1091184, "to_id": 1091183, "type": "USED-FOR"}, {"id": 174700, "from_id": 1091185, "to_id": 1091183, "type": "USED-FOR"}, {"id": 174701, "from_id": 1091183, "to_id": 1091186, "type": "USED-FOR"}, {"id": 174702, "from_id": 1091183, "to_id": 1091187, "type": "USED-FOR"}, {"id": 174703, "from_id": 1091184, "to_id": 1091185, "type": "CONJUNCTION"}, {"id": 174704, "from_id": 1091186, "to_id": 1091187, "type": "CONJUNCTION"}, {"id": 174705, "from_id": 1091189, "to_id": 1091188, "type": "USED-FOR"}, {"id": 174706, "from_id": 1091191, "to_id": 1091193, "type": "USED-FOR"}, {"id": 174707, "from_id": 1091191, "to_id": 1091194, "type": "USED-FOR"}, {"id": 174708, "from_id": 1091183, "to_id": 1091191, "type": "COREF"}, {"id": 174709, "from_id": 1091192, "to_id": 1091191, "type": "EVALUATE-FOR"}]}
{"id": "C90-1002", "text": "  A deterministic parser  is under development which represents a departure from  traditional deterministic parsers  in that it combines both  symbolic and connectionist components . The connectionist component is trained either from  patterns  derived from the  rules  of a  deterministic grammar . The development and evolution of such a  hybrid architecture  has lead to a  parser  which is superior to any  known deterministic parser . Experiments are described and powerful  training techniques  are demonstrated that permit  decision-making  by the  connectionist component  in the  parsing process . This approach has permitted some simplifications to the  rules  of other  deterministic parsers , including the elimination of  rule packets  and priorities. Furthermore,  parsing  is performed more robustly and with more tolerance for error. Data are presented which show how a  connectionist (neural) network  trained with  linguistic rules  can parse both  expected (grammatical) sentences  as well as some novel (ungrammatical or lexically ambiguous) sentences. ", "Comments": [], "entities": [{"id": 1091234, "label": "ENT", "start_offset": 4, "end_offset": 24}, {"id": 1091235, "label": "ENT", "start_offset": 94, "end_offset": 115}, {"id": 1091236, "label": "ENT", "start_offset": 125, "end_offset": 127}, {"id": 1091237, "label": "ENT", "start_offset": 143, "end_offset": 180}, {"id": 1091238, "label": "ENT", "start_offset": 187, "end_offset": 210}, {"id": 1091239, "label": "ENT", "start_offset": 235, "end_offset": 243}, {"id": 1091240, "label": "ENT", "start_offset": 263, "end_offset": 297}, {"id": 1091241, "label": "ENT", "start_offset": 341, "end_offset": 360}, {"id": 1091242, "label": "ENT", "start_offset": 377, "end_offset": 383}, {"id": 1091243, "label": "ENT", "start_offset": 417, "end_offset": 437}, {"id": 1091244, "label": "ENT", "start_offset": 480, "end_offset": 499}, {"id": 1091245, "label": "ENT", "start_offset": 531, "end_offset": 546}, {"id": 1091246, "label": "ENT", "start_offset": 556, "end_offset": 579}, {"id": 1091247, "label": "ENT", "start_offset": 589, "end_offset": 604}, {"id": 1091248, "label": "ENT", "start_offset": 612, "end_offset": 620}, {"id": 1091249, "label": "ENT", "start_offset": 664, "end_offset": 669}, {"id": 1091250, "label": "ENT", "start_offset": 681, "end_offset": 702}, {"id": 1091251, "label": "ENT", "start_offset": 779, "end_offset": 786}, {"id": 1091252, "label": "ENT", "start_offset": 887, "end_offset": 917}, {"id": 1091253, "label": "ENT", "start_offset": 933, "end_offset": 949}, {"id": 1091254, "label": "ENT", "start_offset": 967, "end_offset": 999}, {"id": 1091255, "label": "ENT", "start_offset": 1023, "end_offset": 1071}], "relations": [{"id": 174731, "from_id": 1091244, "to_id": 1091245, "type": "USED-FOR"}, {"id": 174732, "from_id": 1091246, "to_id": 1091247, "type": "PART-OF"}, {"id": 174733, "from_id": 1091252, "to_id": 1091254, "type": "USED-FOR"}, {"id": 174734, "from_id": 1091236, "to_id": 1091234, "type": "COREF"}, {"id": 174735, "from_id": 1091237, "to_id": 1091236, "type": "PART-OF"}, {"id": 174736, "from_id": 1091238, "to_id": 1091237, "type": "HYPONYM-OF"}, {"id": 174737, "from_id": 1091241, "to_id": 1091237, "type": "COREF"}, {"id": 174738, "from_id": 1091242, "to_id": 1091243, "type": "COMPARE"}, {"id": 174739, "from_id": 1091242, "to_id": 1091234, "type": "COREF"}, {"id": 174740, "from_id": 1091246, "to_id": 1091245, "type": "USED-FOR"}, {"id": 174741, "from_id": 1091248, "to_id": 1091244, "type": "COREF"}, {"id": 174742, "from_id": 1091250, "to_id": 1091243, "type": "COREF"}, {"id": 174743, "from_id": 1091253, "to_id": 1091252, "type": "USED-FOR"}, {"id": 174744, "from_id": 1091234, "to_id": 1091235, "type": "COMPARE"}, {"id": 174745, "from_id": 1091241, "to_id": 1091242, "type": "USED-FOR"}, {"id": 174746, "from_id": 1091252, "to_id": 1091255, "type": "USED-FOR"}, {"id": 174747, "from_id": 1091254, "to_id": 1091255, "type": "CONJUNCTION"}, {"id": 174748, "from_id": 1091239, "to_id": 1091238, "type": "USED-FOR"}, {"id": 174749, "from_id": 1091240, "to_id": 1091239, "type": "USED-FOR"}]}
{"id": "ICASSP_2016_4_abs", "text": "High frequency oscillations (HFOs) are a promising biomarker of epileptic brain tissue and activity. HFOs additionally serve as a prototypical example of challenges in the analysis of discrete events in high-temporal resolution, intracranial EEG data. Two primary challenges are 1) dimensionality reduction, and 2) assessing feasibility of classification. Dimensionality reduction assumes that the data lie on a manifold with dimension less than that of the features space. However, previous HFO analysis have assumed a linear manifold, global across time, space (i.e. recording electrode/channel), and individual patients. Instead, we assess both a) whether linear methods are appropriate and b) the consistency of the manifold across time, space, and patients. We also estimate bounds on the Bayes classification error to quantify the distinction between two classes of HFOs (those occurring during seizures and those occurring due to other processes). This analysis provides the foundation for future clinical use of HFO features and guides the analysis for other discrete events, such as individual action potentials or multi-unit activity.", "Comments": [], "entities": [{"id": 1091287, "label": "ENT", "start_offset": 0, "end_offset": 34}, {"id": 1091288, "label": "ENT", "start_offset": 64, "end_offset": 99}, {"id": 1091289, "label": "ENT", "start_offset": 101, "end_offset": 105}, {"id": 1091290, "label": "ENT", "start_offset": 172, "end_offset": 199}, {"id": 1091291, "label": "ENT", "start_offset": 203, "end_offset": 250}, {"id": 1091292, "label": "ENT", "start_offset": 282, "end_offset": 306}, {"id": 1091293, "label": "ENT", "start_offset": 315, "end_offset": 354}, {"id": 1091294, "label": "ENT", "start_offset": 356, "end_offset": 380}, {"id": 1091295, "label": "ENT", "start_offset": 412, "end_offset": 420}, {"id": 1091296, "label": "ENT", "start_offset": 458, "end_offset": 472}, {"id": 1091297, "label": "ENT", "start_offset": 492, "end_offset": 504}, {"id": 1091298, "label": "ENT", "start_offset": 520, "end_offset": 535}, {"id": 1091299, "label": "ENT", "start_offset": 659, "end_offset": 673}, {"id": 1091300, "label": "ENT", "start_offset": 720, "end_offset": 728}, {"id": 1091301, "label": "ENT", "start_offset": 780, "end_offset": 786}, {"id": 1091302, "label": "ENT", "start_offset": 794, "end_offset": 820}, {"id": 1091303, "label": "ENT", "start_offset": 872, "end_offset": 876}, {"id": 1091304, "label": "ENT", "start_offset": 878, "end_offset": 883}, {"id": 1091305, "label": "ENT", "start_offset": 914, "end_offset": 919}, {"id": 1091306, "label": "ENT", "start_offset": 1004, "end_offset": 1032}, {"id": 1091307, "label": "ENT", "start_offset": 1067, "end_offset": 1082}, {"id": 1091308, "label": "ENT", "start_offset": 1103, "end_offset": 1120}, {"id": 1091309, "label": "ENT", "start_offset": 1124, "end_offset": 1143}], "relations": [{"id": 174774, "from_id": 1091287, "to_id": 1091289, "type": "COREF"}, {"id": 174775, "from_id": 1091291, "to_id": 1091290, "type": "USED-FOR"}, {"id": 174776, "from_id": 1091304, "to_id": 1091303, "type": "HYPONYM-OF"}, {"id": 174777, "from_id": 1091305, "to_id": 1091303, "type": "HYPONYM-OF"}, {"id": 174778, "from_id": 1091304, "to_id": 1091305, "type": "CONJUNCTION"}, {"id": 174779, "from_id": 1091298, "to_id": 1091297, "type": "USED-FOR"}, {"id": 174780, "from_id": 1091292, "to_id": 1091294, "type": "COREF"}, {"id": 174781, "from_id": 1091289, "to_id": 1091303, "type": "COREF"}, {"id": 174782, "from_id": 1091308, "to_id": 1091307, "type": "HYPONYM-OF"}, {"id": 174783, "from_id": 1091309, "to_id": 1091307, "type": "HYPONYM-OF"}, {"id": 174784, "from_id": 1091308, "to_id": 1091309, "type": "CONJUNCTION"}]}
{"id": "C08-3010", "text": " In this paper, we will describe a  search tool  for a huge set of  ngrams . The tool supports  queries  with an arbitrary number of  wildcards . It takes a fraction of a second for a search, and can provide the  fillers  of the  wildcards . The system runs on a single Linux PC with reasonable size  memory  (less than 4GB) and  disk space  (less than 400GB). This system can be a very useful tool for  linguistic knowledge discovery  and other  NLP tasks . ", "Comments": [], "entities": [{"id": 1091310, "label": "ENT", "start_offset": 36, "end_offset": 47}, {"id": 1091311, "label": "ENT", "start_offset": 68, "end_offset": 74}, {"id": 1091312, "label": "ENT", "start_offset": 81, "end_offset": 85}, {"id": 1091313, "label": "ENT", "start_offset": 146, "end_offset": 148}, {"id": 1091314, "label": "ENT", "start_offset": 246, "end_offset": 252}, {"id": 1091315, "label": "ENT", "start_offset": 301, "end_offset": 307}, {"id": 1091316, "label": "ENT", "start_offset": 330, "end_offset": 340}, {"id": 1091317, "label": "ENT", "start_offset": 366, "end_offset": 372}, {"id": 1091318, "label": "ENT", "start_offset": 394, "end_offset": 398}, {"id": 1091319, "label": "ENT", "start_offset": 404, "end_offset": 434}, {"id": 1091320, "label": "ENT", "start_offset": 447, "end_offset": 456}], "relations": [{"id": 174785, "from_id": 1091310, "to_id": 1091311, "type": "USED-FOR"}, {"id": 174786, "from_id": 1091312, "to_id": 1091310, "type": "COREF"}, {"id": 174787, "from_id": 1091317, "to_id": 1091314, "type": "COREF"}, {"id": 174788, "from_id": 1091318, "to_id": 1091317, "type": "COREF"}, {"id": 174789, "from_id": 1091318, "to_id": 1091319, "type": "USED-FOR"}, {"id": 174790, "from_id": 1091318, "to_id": 1091320, "type": "USED-FOR"}, {"id": 174791, "from_id": 1091319, "to_id": 1091320, "type": "CONJUNCTION"}, {"id": 174792, "from_id": 1091313, "to_id": 1091312, "type": "COREF"}, {"id": 174793, "from_id": 1091314, "to_id": 1091313, "type": "COREF"}]}
{"id": "IJCAI_2003_15_abs", "text": "Many description logics (DLs) combine knowledge representation on an abstract, logical level with an interface to \"concrete\" domains such as numbers and strings. We propose to extend such DLs with key constraints that allow the expression of statements like \"US citizens are uniquely identified by their social security number\". Based on this idea, we introduce a number of natural description logics and present (un)decidability results and tight NEx-PTlME complexity bounds.", "Comments": [], "entities": [{"id": 1091321, "label": "ENT", "start_offset": 5, "end_offset": 29}, {"id": 1091322, "label": "ENT", "start_offset": 38, "end_offset": 62}, {"id": 1091323, "label": "ENT", "start_offset": 188, "end_offset": 191}, {"id": 1091324, "label": "ENT", "start_offset": 374, "end_offset": 400}, {"id": 1091325, "label": "ENT", "start_offset": 442, "end_offset": 475}], "relations": [{"id": 174794, "from_id": 1091321, "to_id": 1091323, "type": "COREF"}, {"id": 174795, "from_id": 1091322, "to_id": 1091321, "type": "CONJUNCTION"}]}
{"id": "N03-3010", "text": " In this paper, we propose a novel  Cooperative Model  for  natural language understanding  in a  dialogue system  . We build this based on both  Finite State Model (FSM)  and  Statistical Learning Model (SLM)  .  FSM  provides two strategies for  language understanding  and have a high accuracy but little robustness and flexibility.  Statistical approach  is much more robust but less accurate.  Cooperative Model  incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies. ", "Comments": [], "entities": [{"id": 1091326, "label": "ENT", "start_offset": 36, "end_offset": 53}, {"id": 1091327, "label": "ENT", "start_offset": 60, "end_offset": 90}, {"id": 1091328, "label": "ENT", "start_offset": 98, "end_offset": 113}, {"id": 1091329, "label": "ENT", "start_offset": 126, "end_offset": 130}, {"id": 1091330, "label": "ENT", "start_offset": 146, "end_offset": 170}, {"id": 1091331, "label": "ENT", "start_offset": 177, "end_offset": 209}, {"id": 1091332, "label": "ENT", "start_offset": 214, "end_offset": 217}, {"id": 1091333, "label": "ENT", "start_offset": 248, "end_offset": 270}, {"id": 1091334, "label": "ENT", "start_offset": 337, "end_offset": 357}, {"id": 1091335, "label": "ENT", "start_offset": 399, "end_offset": 416}], "relations": [{"id": 174796, "from_id": 1091329, "to_id": 1091326, "type": "COREF"}, {"id": 174797, "from_id": 1091330, "to_id": 1091329, "type": "USED-FOR"}, {"id": 174798, "from_id": 1091331, "to_id": 1091329, "type": "USED-FOR"}, {"id": 174799, "from_id": 1091330, "to_id": 1091331, "type": "CONJUNCTION"}, {"id": 174800, "from_id": 1091332, "to_id": 1091330, "type": "COREF"}, {"id": 174801, "from_id": 1091332, "to_id": 1091333, "type": "USED-FOR"}, {"id": 174802, "from_id": 1091334, "to_id": 1091331, "type": "COREF"}, {"id": 174803, "from_id": 1091335, "to_id": 1091326, "type": "COREF"}, {"id": 174804, "from_id": 1091327, "to_id": 1091328, "type": "USED-FOR"}, {"id": 174805, "from_id": 1091326, "to_id": 1091327, "type": "USED-FOR"}]}
{"id": "CVPR_2009_18_abs", "text": "We present a new method for detecting interest points using histogram information. Unlike existing interest point detectors, which measure pixel-wise differences in image intensity, our detectors incorporate histogram-based representations , and thus can find image regions that present a distinct distribution in the neighborhood. The proposed detectors are able to capture large-scale structures and distinctive textured patterns, and exhibit strong invariance to rotation, illumination variation, and blur. The experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes, in terms of repeatability and distinctiveness. An extension of our method to space-time interest point detection for action classification is also presented.", "Comments": [], "entities": [{"id": 1091336, "label": "ENT", "start_offset": 17, "end_offset": 23}, {"id": 1091337, "label": "ENT", "start_offset": 28, "end_offset": 53}, {"id": 1091338, "label": "ENT", "start_offset": 60, "end_offset": 81}, {"id": 1091339, "label": "ENT", "start_offset": 99, "end_offset": 123}, {"id": 1091340, "label": "ENT", "start_offset": 139, "end_offset": 180}, {"id": 1091341, "label": "ENT", "start_offset": 186, "end_offset": 195}, {"id": 1091342, "label": "ENT", "start_offset": 208, "end_offset": 239}, {"id": 1091343, "label": "ENT", "start_offset": 345, "end_offset": 354}, {"id": 1091344, "label": "ENT", "start_offset": 375, "end_offset": 397}, {"id": 1091345, "label": "ENT", "start_offset": 402, "end_offset": 431}, {"id": 1091346, "label": "ENT", "start_offset": 466, "end_offset": 474}, {"id": 1091347, "label": "ENT", "start_offset": 476, "end_offset": 498}, {"id": 1091348, "label": "ENT", "start_offset": 504, "end_offset": 508}, {"id": 1091349, "label": "ENT", "start_offset": 558, "end_offset": 598}, {"id": 1091350, "label": "ENT", "start_offset": 642, "end_offset": 666}, {"id": 1091351, "label": "ENT", "start_offset": 673, "end_offset": 702}, {"id": 1091352, "label": "ENT", "start_offset": 716, "end_offset": 729}, {"id": 1091353, "label": "ENT", "start_offset": 734, "end_offset": 749}, {"id": 1091354, "label": "ENT", "start_offset": 771, "end_offset": 777}, {"id": 1091355, "label": "ENT", "start_offset": 781, "end_offset": 816}, {"id": 1091356, "label": "ENT", "start_offset": 821, "end_offset": 842}], "relations": [{"id": 174806, "from_id": 1091338, "to_id": 1091337, "type": "USED-FOR"}, {"id": 174807, "from_id": 1091336, "to_id": 1091337, "type": "USED-FOR"}, {"id": 174808, "from_id": 1091341, "to_id": 1091336, "type": "COREF"}, {"id": 174809, "from_id": 1091342, "to_id": 1091341, "type": "PART-OF"}, {"id": 174810, "from_id": 1091340, "to_id": 1091339, "type": "EVALUATE-FOR"}, {"id": 174811, "from_id": 1091343, "to_id": 1091339, "type": "COREF"}, {"id": 174812, "from_id": 1091343, "to_id": 1091344, "type": "USED-FOR"}, {"id": 174813, "from_id": 1091344, "to_id": 1091345, "type": "CONJUNCTION"}, {"id": 174814, "from_id": 1091343, "to_id": 1091345, "type": "USED-FOR"}, {"id": 174815, "from_id": 1091343, "to_id": 1091346, "type": "USED-FOR"}, {"id": 174816, "from_id": 1091346, "to_id": 1091347, "type": "CONJUNCTION"}, {"id": 174817, "from_id": 1091347, "to_id": 1091348, "type": "CONJUNCTION"}, {"id": 174818, "from_id": 1091343, "to_id": 1091347, "type": "USED-FOR"}, {"id": 174819, "from_id": 1091343, "to_id": 1091348, "type": "USED-FOR"}, {"id": 174820, "from_id": 1091349, "to_id": 1091343, "type": "COREF"}, {"id": 174821, "from_id": 1091349, "to_id": 1091350, "type": "USED-FOR"}, {"id": 174822, "from_id": 1091354, "to_id": 1091349, "type": "COREF"}, {"id": 174823, "from_id": 1091354, "to_id": 1091355, "type": "USED-FOR"}, {"id": 174824, "from_id": 1091355, "to_id": 1091356, "type": "USED-FOR"}, {"id": 174825, "from_id": 1091352, "to_id": 1091349, "type": "EVALUATE-FOR"}, {"id": 174826, "from_id": 1091353, "to_id": 1091349, "type": "EVALUATE-FOR"}, {"id": 174827, "from_id": 1091352, "to_id": 1091353, "type": "CONJUNCTION"}]}
{"id": "P05-2008", "text": "  Sentiment Classification  seeks to identify a piece of  text  according to its author's general feeling toward their  subject , be it positive or negative. Traditional  machine learning techniques  have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the  training and test data  with respect to  topic . This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with  training data  labeled with  emoticons , which has the potential of being independent of  domain ,  topic  and time. ", "Comments": [], "entities": [{"id": 1091421, "label": "ENT", "start_offset": 2, "end_offset": 26}, {"id": 1091422, "label": "ENT", "start_offset": 171, "end_offset": 198}, {"id": 1091423, "label": "ENT", "start_offset": 226, "end_offset": 233}, {"id": 1091424, "label": "ENT", "start_offset": 342, "end_offset": 364}, {"id": 1091425, "label": "ENT", "start_offset": 520, "end_offset": 533}, {"id": 1091426, "label": "ENT", "start_offset": 549, "end_offset": 558}], "relations": [{"id": 174883, "from_id": 1091426, "to_id": 1091425, "type": "FEATURE-OF"}, {"id": 174884, "from_id": 1091421, "to_id": 1091423, "type": "COREF"}, {"id": 174885, "from_id": 1091422, "to_id": 1091423, "type": "USED-FOR"}]}
{"id": "A92-1027", "text": "   We present an efficient algorithm for  chart-based phrase structure parsing  of  natural language  that is tailored to the problem of extracting specific information from  unrestricted texts  where many of the  words  are unknown and much of the  text  is irrelevant to the task. The  parser  gains algorithmic efficiency through a  reduction  of its  search space  . As each new  edge  is added to the  chart  , the algorithm checks only the topmost of the  edges  adjacent to it, rather than all such  edges  as in conventional treatments. The resulting  spanning edges  are insured to be the correct ones by carefully controlling the order in which  edges  are introduced so that every final  constituent  covers the longest possible  span  . This is facilitated through the use of  phrase boundary heuristics  based on the placement of  function words  , and by  heuristic rules  that permit certain kinds of  phrases  to be deduced despite the presence of  unknown words  . A further  reduction in the search space  is achieved by using  semantic  rather than  syntactic categories  on the  terminal and non-terminal edges  , thereby reducing the amount of  ambiguity  and thus the number of  edges  , since only  edges  with a valid  semantic  interpretation are ever introduced. ", "Comments": [], "entities": [{"id": 1091440, "label": "ENT", "start_offset": 27, "end_offset": 36}, {"id": 1091441, "label": "ENT", "start_offset": 42, "end_offset": 78}, {"id": 1091442, "label": "ENT", "start_offset": 84, "end_offset": 100}, {"id": 1091443, "label": "ENT", "start_offset": 288, "end_offset": 294}, {"id": 1091444, "label": "ENT", "start_offset": 355, "end_offset": 367}, {"id": 1091445, "label": "ENT", "start_offset": 384, "end_offset": 388}, {"id": 1091446, "label": "ENT", "start_offset": 407, "end_offset": 412}, {"id": 1091447, "label": "ENT", "start_offset": 462, "end_offset": 467}, {"id": 1091448, "label": "ENT", "start_offset": 507, "end_offset": 512}, {"id": 1091449, "label": "ENT", "start_offset": 560, "end_offset": 574}, {"id": 1091450, "label": "ENT", "start_offset": 656, "end_offset": 661}, {"id": 1091451, "label": "ENT", "start_offset": 789, "end_offset": 815}, {"id": 1091452, "label": "ENT", "start_offset": 844, "end_offset": 858}, {"id": 1091453, "label": "ENT", "start_offset": 870, "end_offset": 885}, {"id": 1091454, "label": "ENT", "start_offset": 965, "end_offset": 978}, {"id": 1091455, "label": "ENT", "start_offset": 993, "end_offset": 1022}, {"id": 1091456, "label": "ENT", "start_offset": 1046, "end_offset": 1054}, {"id": 1091457, "label": "ENT", "start_offset": 1069, "end_offset": 1089}, {"id": 1091458, "label": "ENT", "start_offset": 1099, "end_offset": 1130}, {"id": 1091459, "label": "ENT", "start_offset": 1201, "end_offset": 1206}, {"id": 1091460, "label": "ENT", "start_offset": 1222, "end_offset": 1227}], "relations": [{"id": 174898, "from_id": 1091442, "to_id": 1091441, "type": "USED-FOR"}, {"id": 174899, "from_id": 1091452, "to_id": 1091451, "type": "USED-FOR"}, {"id": 174900, "from_id": 1091456, "to_id": 1091457, "type": "COMPARE"}, {"id": 174901, "from_id": 1091456, "to_id": 1091455, "type": "USED-FOR"}, {"id": 174902, "from_id": 1091440, "to_id": 1091441, "type": "USED-FOR"}, {"id": 174903, "from_id": 1091443, "to_id": 1091440, "type": "COREF"}, {"id": 174904, "from_id": 1091457, "to_id": 1091458, "type": "FEATURE-OF"}, {"id": 174905, "from_id": 1091456, "to_id": 1091458, "type": "FEATURE-OF"}]}
{"id": "CVPR_2016_15_abs", "text": "This paper proposes a method for learning joint embed-dings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large-margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase lo-calization on the Flickr30K Entities dataset.", "Comments": [], "entities": [{"id": 1091506, "label": "ENT", "start_offset": 22, "end_offset": 28}, {"id": 1091507, "label": "ENT", "start_offset": 42, "end_offset": 78}, {"id": 1091508, "label": "ENT", "start_offset": 87, "end_offset": 112}, {"id": 1091509, "label": "ENT", "start_offset": 118, "end_offset": 155}, {"id": 1091510, "label": "ENT", "start_offset": 168, "end_offset": 182}, {"id": 1091511, "label": "ENT", "start_offset": 188, "end_offset": 195}, {"id": 1091512, "label": "ENT", "start_offset": 215, "end_offset": 237}, {"id": 1091513, "label": "ENT", "start_offset": 252, "end_offset": 282}, {"id": 1091514, "label": "ENT", "start_offset": 288, "end_offset": 347}, {"id": 1091515, "label": "ENT", "start_offset": 360, "end_offset": 386}, {"id": 1091516, "label": "ENT", "start_offset": 424, "end_offset": 432}, {"id": 1091517, "label": "ENT", "start_offset": 467, "end_offset": 475}, {"id": 1091518, "label": "ENT", "start_offset": 480, "end_offset": 521}, {"id": 1091519, "label": "ENT", "start_offset": 527, "end_offset": 533}, {"id": 1091520, "label": "ENT", "start_offset": 579, "end_offset": 623}, {"id": 1091521, "label": "ENT", "start_offset": 661, "end_offset": 681}, {"id": 1091522, "label": "ENT", "start_offset": 689, "end_offset": 715}], "relations": [{"id": 174938, "from_id": 1091506, "to_id": 1091507, "type": "USED-FOR"}, {"id": 174939, "from_id": 1091508, "to_id": 1091506, "type": "USED-FOR"}, {"id": 174940, "from_id": 1091509, "to_id": 1091508, "type": "PART-OF"}, {"id": 174941, "from_id": 1091510, "to_id": 1091508, "type": "PART-OF"}, {"id": 174942, "from_id": 1091509, "to_id": 1091510, "type": "CONJUNCTION"}, {"id": 174943, "from_id": 1091511, "to_id": 1091508, "type": "COREF"}, {"id": 174944, "from_id": 1091512, "to_id": 1091511, "type": "USED-FOR"}, {"id": 174945, "from_id": 1091513, "to_id": 1091514, "type": "CONJUNCTION"}, {"id": 174946, "from_id": 1091513, "to_id": 1091512, "type": "FEATURE-OF"}, {"id": 174947, "from_id": 1091514, "to_id": 1091512, "type": "FEATURE-OF"}, {"id": 174948, "from_id": 1091506, "to_id": 1091516, "type": "COREF"}, {"id": 174949, "from_id": 1091517, "to_id": 1091516, "type": "EVALUATE-FOR"}, {"id": 174950, "from_id": 1091518, "to_id": 1091516, "type": "EVALUATE-FOR"}, {"id": 174951, "from_id": 1091519, "to_id": 1091516, "type": "COREF"}, {"id": 174952, "from_id": 1091520, "to_id": 1091519, "type": "EVALUATE-FOR"}, {"id": 174953, "from_id": 1091522, "to_id": 1091521, "type": "USED-FOR"}, {"id": 174954, "from_id": 1091521, "to_id": 1091519, "type": "EVALUATE-FOR"}]}
{"id": "E83-1029", "text": " In this paper a system which understands and conceptualizes  scenes descriptions in natural language  is presented. Specifically, the following components of the system are described: the  syntactic analyzer , based on a  Procedural Systemic Grammar , the  semantic analyzer  relying on the  Conceptual Dependency Theory , and the  dictionary . ", "Comments": [], "entities": [{"id": 1091523, "label": "ENT", "start_offset": 17, "end_offset": 23}, {"id": 1091524, "label": "ENT", "start_offset": 62, "end_offset": 101}, {"id": 1091525, "label": "ENT", "start_offset": 145, "end_offset": 155}, {"id": 1091526, "label": "ENT", "start_offset": 163, "end_offset": 169}, {"id": 1091527, "label": "ENT", "start_offset": 190, "end_offset": 208}, {"id": 1091528, "label": "ENT", "start_offset": 223, "end_offset": 250}, {"id": 1091529, "label": "ENT", "start_offset": 258, "end_offset": 275}, {"id": 1091530, "label": "ENT", "start_offset": 293, "end_offset": 321}, {"id": 1091531, "label": "ENT", "start_offset": 333, "end_offset": 343}], "relations": [{"id": 174955, "from_id": 1091528, "to_id": 1091527, "type": "USED-FOR"}, {"id": 174956, "from_id": 1091530, "to_id": 1091529, "type": "USED-FOR"}, {"id": 174957, "from_id": 1091523, "to_id": 1091524, "type": "USED-FOR"}, {"id": 174958, "from_id": 1091526, "to_id": 1091523, "type": "COREF"}, {"id": 174959, "from_id": 1091525, "to_id": 1091526, "type": "PART-OF"}, {"id": 174960, "from_id": 1091527, "to_id": 1091525, "type": "PART-OF"}, {"id": 174961, "from_id": 1091529, "to_id": 1091525, "type": "PART-OF"}, {"id": 174962, "from_id": 1091531, "to_id": 1091525, "type": "PART-OF"}, {"id": 174963, "from_id": 1091527, "to_id": 1091529, "type": "CONJUNCTION"}, {"id": 174964, "from_id": 1091529, "to_id": 1091531, "type": "CONJUNCTION"}]}
{"id": "CVPR_2001_111_abs", "text": "In this paper, we propose a novel method, called local non-negative matrix factorization (LNMF), for learning spatially localized, parts-based subspace representation of visual patterns. An objective function is defined to impose lo-calization constraint, in addition to the non-negativity constraint in the standard NMF [1]. This gives a set of bases which not only allows a non-subtractive (part-based) representation of images but also manifests localized features. An algorithm is presented for the learning of such basis components. Experimental results are presented to compare LNMF with the NMF and PCA methods for face representation and recognition, which demonstrates advantages of LNMF.", "Comments": [], "entities": [{"id": 1091554, "label": "ENT", "start_offset": 34, "end_offset": 40}, {"id": 1091555, "label": "ENT", "start_offset": 49, "end_offset": 95}, {"id": 1091556, "label": "ENT", "start_offset": 110, "end_offset": 185}, {"id": 1091557, "label": "ENT", "start_offset": 190, "end_offset": 208}, {"id": 1091558, "label": "ENT", "start_offset": 230, "end_offset": 254}, {"id": 1091559, "label": "ENT", "start_offset": 275, "end_offset": 300}, {"id": 1091560, "label": "ENT", "start_offset": 317, "end_offset": 320}, {"id": 1091561, "label": "ENT", "start_offset": 376, "end_offset": 429}, {"id": 1091562, "label": "ENT", "start_offset": 449, "end_offset": 467}, {"id": 1091563, "label": "ENT", "start_offset": 472, "end_offset": 481}, {"id": 1091564, "label": "ENT", "start_offset": 503, "end_offset": 511}, {"id": 1091565, "label": "ENT", "start_offset": 584, "end_offset": 588}, {"id": 1091566, "label": "ENT", "start_offset": 598, "end_offset": 617}, {"id": 1091567, "label": "ENT", "start_offset": 622, "end_offset": 657}, {"id": 1091568, "label": "ENT", "start_offset": 692, "end_offset": 696}], "relations": [{"id": 174980, "from_id": 1091554, "to_id": 1091555, "type": "COREF"}, {"id": 174981, "from_id": 1091554, "to_id": 1091556, "type": "USED-FOR"}, {"id": 174982, "from_id": 1091565, "to_id": 1091566, "type": "COMPARE"}, {"id": 174983, "from_id": 1091565, "to_id": 1091567, "type": "USED-FOR"}, {"id": 174984, "from_id": 1091566, "to_id": 1091567, "type": "USED-FOR"}, {"id": 174985, "from_id": 1091568, "to_id": 1091565, "type": "COREF"}, {"id": 174986, "from_id": 1091567, "to_id": 1091568, "type": "EVALUATE-FOR"}, {"id": 174987, "from_id": 1091559, "to_id": 1091560, "type": "PART-OF"}, {"id": 174988, "from_id": 1091557, "to_id": 1091558, "type": "USED-FOR"}, {"id": 174989, "from_id": 1091563, "to_id": 1091564, "type": "USED-FOR"}, {"id": 174990, "from_id": 1091554, "to_id": 1091565, "type": "COREF"}]}
{"id": "CVPR_2010_18_abs", "text": "Graph-cuts optimization is prevalent in vision and graphics problems. It is thus of great practical importance to parallelize the graph-cuts optimization using to-day's ubiquitous multi-core machines. However, the current best serial algorithm by Boykov and Kolmogorov [4] (called the BK algorithm) still has the superior empirical performance. It is non-trivial to parallelize as expensive synchronization overhead easily offsets the advantage of parallelism. In this paper, we propose a novel adaptive bottom-up approach to parallelize the BK algorithm. We first uniformly partition the graph into a number of regularly-shaped dis-joint subgraphs and process them in parallel, then we incre-mentally merge the subgraphs in an adaptive way to obtain the global optimum. The new algorithm has three benefits: 1) it is more cache-friendly within smaller subgraphs; 2) it keeps balanced workloads among computing cores; 3) it causes little overhead and is adaptable to the number of available cores. Extensive experiments in common applications such as 2D/3D image segmentations and 3D surface fitting demonstrate the effectiveness of our approach.", "Comments": [], "entities": [{"id": 1091621, "label": "ENT", "start_offset": 0, "end_offset": 23}, {"id": 1091622, "label": "ENT", "start_offset": 40, "end_offset": 68}, {"id": 1091623, "label": "ENT", "start_offset": 130, "end_offset": 153}, {"id": 1091624, "label": "ENT", "start_offset": 180, "end_offset": 199}, {"id": 1091625, "label": "ENT", "start_offset": 227, "end_offset": 243}, {"id": 1091626, "label": "ENT", "start_offset": 285, "end_offset": 297}, {"id": 1091627, "label": "ENT", "start_offset": 391, "end_offset": 415}, {"id": 1091628, "label": "ENT", "start_offset": 448, "end_offset": 459}, {"id": 1091629, "label": "ENT", "start_offset": 495, "end_offset": 522}, {"id": 1091630, "label": "ENT", "start_offset": 542, "end_offset": 554}, {"id": 1091631, "label": "ENT", "start_offset": 589, "end_offset": 594}, {"id": 1091632, "label": "ENT", "start_offset": 612, "end_offset": 648}, {"id": 1091633, "label": "ENT", "start_offset": 661, "end_offset": 665}, {"id": 1091634, "label": "ENT", "start_offset": 712, "end_offset": 721}, {"id": 1091635, "label": "ENT", "start_offset": 755, "end_offset": 769}, {"id": 1091636, "label": "ENT", "start_offset": 779, "end_offset": 788}, {"id": 1091637, "label": "ENT", "start_offset": 853, "end_offset": 862}, {"id": 1091638, "label": "ENT", "start_offset": 876, "end_offset": 894}, {"id": 1091639, "label": "ENT", "start_offset": 938, "end_offset": 946}, {"id": 1091640, "label": "ENT", "start_offset": 1030, "end_offset": 1042}, {"id": 1091641, "label": "ENT", "start_offset": 1051, "end_offset": 1076}, {"id": 1091642, "label": "ENT", "start_offset": 1081, "end_offset": 1099}, {"id": 1091643, "label": "ENT", "start_offset": 1137, "end_offset": 1145}], "relations": [{"id": 175036, "from_id": 1091621, "to_id": 1091622, "type": "USED-FOR"}, {"id": 175037, "from_id": 1091623, "to_id": 1091621, "type": "COREF"}, {"id": 175038, "from_id": 1091626, "to_id": 1091625, "type": "HYPONYM-OF"}, {"id": 175039, "from_id": 1091624, "to_id": 1091623, "type": "USED-FOR"}, {"id": 175040, "from_id": 1091630, "to_id": 1091626, "type": "COREF"}, {"id": 175041, "from_id": 1091629, "to_id": 1091630, "type": "USED-FOR"}, {"id": 175042, "from_id": 1091633, "to_id": 1091632, "type": "COREF"}, {"id": 175043, "from_id": 1091634, "to_id": 1091633, "type": "COREF"}, {"id": 175044, "from_id": 1091636, "to_id": 1091629, "type": "COREF"}, {"id": 175045, "from_id": 1091643, "to_id": 1091636, "type": "COREF"}, {"id": 175046, "from_id": 1091641, "to_id": 1091640, "type": "HYPONYM-OF"}, {"id": 175047, "from_id": 1091642, "to_id": 1091640, "type": "HYPONYM-OF"}, {"id": 175048, "from_id": 1091641, "to_id": 1091642, "type": "CONJUNCTION"}, {"id": 175049, "from_id": 1091640, "to_id": 1091643, "type": "EVALUATE-FOR"}]}
{"id": "ICML_2006_112_abs", "text": "We describe a general framework for online multiclass learning based on the notion of hypothesis sharing. In our framework sets of classes are associated with hypotheses. Thus, all classes within a given set share the same hypothesis. This framework includes as special cases commonly used constructions for multiclass categorization such as allocating a unique hypothesis for each class and allocating a single common hypothesis for all classes. We generalize the multiclass Perceptron to our framework and derive a unifying mistake bound analysis. Our construction naturally extends to settings where the number of classes is not known in advance but, rather, is revealed along the online learning process. We demonstrate the merits of our approach by comparing it to previous methods on both synthetic and natural datasets.", "Comments": [], "entities": [{"id": 1091656, "label": "ENT", "start_offset": 22, "end_offset": 31}, {"id": 1091657, "label": "ENT", "start_offset": 36, "end_offset": 62}, {"id": 1091658, "label": "ENT", "start_offset": 76, "end_offset": 104}, {"id": 1091659, "label": "ENT", "start_offset": 113, "end_offset": 122}, {"id": 1091660, "label": "ENT", "start_offset": 240, "end_offset": 249}, {"id": 1091661, "label": "ENT", "start_offset": 308, "end_offset": 333}, {"id": 1091662, "label": "ENT", "start_offset": 465, "end_offset": 486}, {"id": 1091663, "label": "ENT", "start_offset": 494, "end_offset": 503}, {"id": 1091664, "label": "ENT", "start_offset": 517, "end_offset": 548}, {"id": 1091665, "label": "ENT", "start_offset": 684, "end_offset": 707}, {"id": 1091666, "label": "ENT", "start_offset": 742, "end_offset": 750}, {"id": 1091667, "label": "ENT", "start_offset": 764, "end_offset": 766}, {"id": 1091668, "label": "ENT", "start_offset": 779, "end_offset": 786}, {"id": 1091669, "label": "ENT", "start_offset": 795, "end_offset": 825}], "relations": [{"id": 175060, "from_id": 1091656, "to_id": 1091657, "type": "USED-FOR"}, {"id": 175061, "from_id": 1091658, "to_id": 1091656, "type": "USED-FOR"}, {"id": 175062, "from_id": 1091656, "to_id": 1091659, "type": "COREF"}, {"id": 175063, "from_id": 1091659, "to_id": 1091660, "type": "COREF"}, {"id": 175064, "from_id": 1091660, "to_id": 1091663, "type": "COREF"}, {"id": 175065, "from_id": 1091662, "to_id": 1091663, "type": "USED-FOR"}, {"id": 175066, "from_id": 1091663, "to_id": 1091666, "type": "COREF"}, {"id": 175067, "from_id": 1091666, "to_id": 1091667, "type": "COREF"}, {"id": 175068, "from_id": 1091667, "to_id": 1091668, "type": "COMPARE"}, {"id": 175069, "from_id": 1091669, "to_id": 1091668, "type": "EVALUATE-FOR"}, {"id": 175070, "from_id": 1091669, "to_id": 1091667, "type": "EVALUATE-FOR"}]}
{"id": "ECCV_2012_40_abs", "text": "In this paper, we introduce KAZE features, a novel multiscale 2D feature detection and description algorithm in nonlinear scale spaces. Previous approaches detect and describe features at different scale levels by building or approximating the Gaussian scale space of an image. However, Gaussian blurring does not respect the natural boundaries of objects and smoothes to the same degree both details and noise, reducing localization accuracy and distinctiveness. In contrast, we detect and describe 2D features in a nonlinear scale space by means of nonlinear diffusion filtering. In this way, we can make blurring locally adaptive to the image data, reducing noise but retaining object boundaries, obtaining superior localization accuracy and distinctiviness. The nonlinear scale space is built using efficient Additive Operator Splitting (AOS) techniques and variable con-ductance diffusion. We present an extensive evaluation on benchmark datasets and a practical matching application on deformable surfaces. Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space, but comparable to SIFT, our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods.", "Comments": [], "entities": [{"id": 1091696, "label": "ENT", "start_offset": 28, "end_offset": 41}, {"id": 1091697, "label": "ENT", "start_offset": 51, "end_offset": 108}, {"id": 1091698, "label": "ENT", "start_offset": 112, "end_offset": 134}, {"id": 1091699, "label": "ENT", "start_offset": 244, "end_offset": 264}, {"id": 1091700, "label": "ENT", "start_offset": 287, "end_offset": 304}, {"id": 1091701, "label": "ENT", "start_offset": 334, "end_offset": 355}, {"id": 1091702, "label": "ENT", "start_offset": 421, "end_offset": 462}, {"id": 1091703, "label": "ENT", "start_offset": 500, "end_offset": 511}, {"id": 1091704, "label": "ENT", "start_offset": 517, "end_offset": 538}, {"id": 1091705, "label": "ENT", "start_offset": 551, "end_offset": 580}, {"id": 1091706, "label": "ENT", "start_offset": 640, "end_offset": 650}, {"id": 1091707, "label": "ENT", "start_offset": 681, "end_offset": 698}, {"id": 1091708, "label": "ENT", "start_offset": 719, "end_offset": 760}, {"id": 1091709, "label": "ENT", "start_offset": 766, "end_offset": 787}, {"id": 1091710, "label": "ENT", "start_offset": 813, "end_offset": 857}, {"id": 1091711, "label": "ENT", "start_offset": 862, "end_offset": 893}, {"id": 1091712, "label": "ENT", "start_offset": 933, "end_offset": 951}, {"id": 1091713, "label": "ENT", "start_offset": 968, "end_offset": 1011}, {"id": 1091714, "label": "ENT", "start_offset": 1029, "end_offset": 1037}, {"id": 1091715, "label": "ENT", "start_offset": 1082, "end_offset": 1086}, {"id": 1091716, "label": "ENT", "start_offset": 1118, "end_offset": 1139}, {"id": 1091717, "label": "ENT", "start_offset": 1159, "end_offset": 1163}, {"id": 1091718, "label": "ENT", "start_offset": 1169, "end_offset": 1176}, {"id": 1091719, "label": "ENT", "start_offset": 1222, "end_offset": 1231}, {"id": 1091720, "label": "ENT", "start_offset": 1236, "end_offset": 1247}, {"id": 1091721, "label": "ENT", "start_offset": 1265, "end_offset": 1289}], "relations": [{"id": 175095, "from_id": 1091698, "to_id": 1091697, "type": "FEATURE-OF"}, {"id": 175096, "from_id": 1091696, "to_id": 1091697, "type": "HYPONYM-OF"}, {"id": 175097, "from_id": 1091704, "to_id": 1091703, "type": "FEATURE-OF"}, {"id": 175098, "from_id": 1091705, "to_id": 1091703, "type": "USED-FOR"}, {"id": 175099, "from_id": 1091702, "to_id": 1091708, "type": "COREF"}, {"id": 175100, "from_id": 1091703, "to_id": 1091714, "type": "COREF"}, {"id": 175101, "from_id": 1091696, "to_id": 1091703, "type": "COREF"}, {"id": 175102, "from_id": 1091714, "to_id": 1091715, "type": "COMPARE"}, {"id": 175103, "from_id": 1091714, "to_id": 1091717, "type": "COMPARE"}, {"id": 175104, "from_id": 1091714, "to_id": 1091718, "type": "COREF"}, {"id": 175105, "from_id": 1091719, "to_id": 1091718, "type": "EVALUATE-FOR"}, {"id": 175106, "from_id": 1091720, "to_id": 1091718, "type": "EVALUATE-FOR"}, {"id": 175107, "from_id": 1091719, "to_id": 1091720, "type": "CONJUNCTION"}, {"id": 175108, "from_id": 1091718, "to_id": 1091721, "type": "COMPARE"}, {"id": 175109, "from_id": 1091719, "to_id": 1091721, "type": "EVALUATE-FOR"}, {"id": 175110, "from_id": 1091720, "to_id": 1091721, "type": "EVALUATE-FOR"}, {"id": 175111, "from_id": 1091710, "to_id": 1091711, "type": "CONJUNCTION"}, {"id": 175112, "from_id": 1091710, "to_id": 1091709, "type": "USED-FOR"}, {"id": 175113, "from_id": 1091711, "to_id": 1091709, "type": "USED-FOR"}, {"id": 175114, "from_id": 1091709, "to_id": 1091716, "type": "COREF"}, {"id": 175115, "from_id": 1091704, "to_id": 1091709, "type": "COREF"}]}
{"id": "C00-1054", "text": "  Multimodal interfaces require effective  parsing  and understanding of  utterances  whose content is distributed across multiple input modes. Johnston 1998 presents an approach in which strategies for  multimodal integration  are stated declaratively using a  unification-based grammar  that is used by a  multidimensional chart parser  to compose inputs. This approach is highly expressive and supports a broad class of  interfaces , but offers only limited potential for mutual compensation among the input modes, is subject to significant concerns in terms of computational complexity, and complicates selection among alternative multimodal interpretations of the input. In this paper, we present an alternative approach in which  multimodal parsing and understanding  are achieved using a  weighted finite-state device  which takes  speech and gesture streams  as inputs and outputs their joint interpretation. This approach is significantly more efficient, enables tight-coupling of multimodal understanding with  speech recognition , and provides a general probabilistic framework for  multimodal ambiguity resolution . ", "Comments": [], "entities": [{"id": 1091722, "label": "ENT", "start_offset": 2, "end_offset": 23}, {"id": 1091723, "label": "ENT", "start_offset": 43, "end_offset": 50}, {"id": 1091724, "label": "ENT", "start_offset": 170, "end_offset": 178}, {"id": 1091725, "label": "ENT", "start_offset": 204, "end_offset": 226}, {"id": 1091726, "label": "ENT", "start_offset": 262, "end_offset": 287}, {"id": 1091727, "label": "ENT", "start_offset": 308, "end_offset": 337}, {"id": 1091728, "label": "ENT", "start_offset": 363, "end_offset": 371}, {"id": 1091729, "label": "ENT", "start_offset": 424, "end_offset": 434}, {"id": 1091730, "label": "ENT", "start_offset": 565, "end_offset": 589}, {"id": 1091731, "label": "ENT", "start_offset": 717, "end_offset": 725}, {"id": 1091732, "label": "ENT", "start_offset": 736, "end_offset": 772}, {"id": 1091733, "label": "ENT", "start_offset": 796, "end_offset": 824}, {"id": 1091734, "label": "ENT", "start_offset": 839, "end_offset": 865}, {"id": 1091735, "label": "ENT", "start_offset": 922, "end_offset": 930}, {"id": 1091736, "label": "ENT", "start_offset": 990, "end_offset": 1014}, {"id": 1091737, "label": "ENT", "start_offset": 1021, "end_offset": 1039}, {"id": 1091738, "label": "ENT", "start_offset": 1094, "end_offset": 1125}], "relations": [{"id": 175116, "from_id": 1091726, "to_id": 1091727, "type": "USED-FOR"}, {"id": 175117, "from_id": 1091733, "to_id": 1091732, "type": "USED-FOR"}, {"id": 175118, "from_id": 1091728, "to_id": 1091724, "type": "COREF"}, {"id": 175119, "from_id": 1091735, "to_id": 1091731, "type": "COREF"}, {"id": 175120, "from_id": 1091731, "to_id": 1091732, "type": "USED-FOR"}, {"id": 175121, "from_id": 1091726, "to_id": 1091725, "type": "USED-FOR"}, {"id": 175122, "from_id": 1091724, "to_id": 1091725, "type": "USED-FOR"}, {"id": 175123, "from_id": 1091723, "to_id": 1091722, "type": "USED-FOR"}, {"id": 175124, "from_id": 1091737, "to_id": 1091736, "type": "CONJUNCTION"}, {"id": 175125, "from_id": 1091735, "to_id": 1091738, "type": "USED-FOR"}, {"id": 175126, "from_id": 1091734, "to_id": 1091733, "type": "USED-FOR"}]}
{"id": "P84-1078", "text": "   This report describes  Paul  , a  computer text generation system  designed to create  cohesive text  through the use of  lexical substitutions  . Specifically, this system is designed to deterministically choose between  pronominalization  ,  superordinate substitution  , and definite  noun phrase reiteration  . The system identifies a strength of  antecedence recovery  for each of the  lexical substitutions . ", "Comments": [], "entities": [{"id": 1091755, "label": "ENT", "start_offset": 26, "end_offset": 30}, {"id": 1091756, "label": "ENT", "start_offset": 37, "end_offset": 68}, {"id": 1091757, "label": "ENT", "start_offset": 90, "end_offset": 103}, {"id": 1091758, "label": "ENT", "start_offset": 125, "end_offset": 146}, {"id": 1091759, "label": "ENT", "start_offset": 169, "end_offset": 175}, {"id": 1091760, "label": "ENT", "start_offset": 225, "end_offset": 242}, {"id": 1091761, "label": "ENT", "start_offset": 247, "end_offset": 273}, {"id": 1091762, "label": "ENT", "start_offset": 281, "end_offset": 314}, {"id": 1091763, "label": "ENT", "start_offset": 322, "end_offset": 328}, {"id": 1091764, "label": "ENT", "start_offset": 355, "end_offset": 375}, {"id": 1091765, "label": "ENT", "start_offset": 394, "end_offset": 415}], "relations": [{"id": 175139, "from_id": 1091760, "to_id": 1091761, "type": "COMPARE"}, {"id": 175140, "from_id": 1091764, "to_id": 1091765, "type": "USED-FOR"}, {"id": 175141, "from_id": 1091755, "to_id": 1091756, "type": "HYPONYM-OF"}, {"id": 175142, "from_id": 1091756, "to_id": 1091757, "type": "USED-FOR"}, {"id": 175143, "from_id": 1091755, "to_id": 1091759, "type": "COREF"}, {"id": 175144, "from_id": 1091759, "to_id": 1091763, "type": "COREF"}, {"id": 175145, "from_id": 1091761, "to_id": 1091762, "type": "COMPARE"}, {"id": 175146, "from_id": 1091763, "to_id": 1091764, "type": "USED-FOR"}, {"id": 175147, "from_id": 1091758, "to_id": 1091755, "type": "USED-FOR"}]}
{"id": "H05-2007", "text": " We describe a method for identifying systematic  patterns  in  translation data  using  part-of-speech tag sequences  . We incorporate this analysis into a  diagnostic tool  intended for  developers  of  machine translation systems  , and demonstrate how our application can be used by  developers  to explore  patterns  in  machine translation output  . ", "Comments": [], "entities": [{"id": 1091780, "label": "ENT", "start_offset": 15, "end_offset": 21}, {"id": 1091781, "label": "ENT", "start_offset": 38, "end_offset": 80}, {"id": 1091782, "label": "ENT", "start_offset": 89, "end_offset": 117}, {"id": 1091783, "label": "ENT", "start_offset": 141, "end_offset": 149}, {"id": 1091784, "label": "ENT", "start_offset": 158, "end_offset": 173}, {"id": 1091785, "label": "ENT", "start_offset": 205, "end_offset": 232}, {"id": 1091786, "label": "ENT", "start_offset": 260, "end_offset": 271}, {"id": 1091787, "label": "ENT", "start_offset": 312, "end_offset": 352}], "relations": [{"id": 175158, "from_id": 1091782, "to_id": 1091780, "type": "USED-FOR"}, {"id": 175159, "from_id": 1091780, "to_id": 1091783, "type": "COREF"}, {"id": 175160, "from_id": 1091783, "to_id": 1091784, "type": "PART-OF"}, {"id": 175161, "from_id": 1091784, "to_id": 1091785, "type": "USED-FOR"}, {"id": 175162, "from_id": 1091786, "to_id": 1091784, "type": "COREF"}, {"id": 175163, "from_id": 1091780, "to_id": 1091781, "type": "USED-FOR"}, {"id": 175164, "from_id": 1091786, "to_id": 1091787, "type": "USED-FOR"}]}
{"id": "P05-1048", "text": " We directly investigate a subject of much recent debate: do  word sense disambigation models  help  statistical machine translation   quality  ? We present empirical results casting doubt on this common, but unproved, assumption. Using a state-of-the-art  Chinese word sense disambiguation model  to choose  translation candidates  for a typical  IBM statistical MT system  , we find that  word sense disambiguation  does not yield significantly better  translation quality  than the  statistical machine translation system  alone.  Error analysis  suggests several key factors behind this surprising finding, including inherent limitations of current  statistical MT architectures  . ", "Comments": [], "entities": [{"id": 1091788, "label": "ENT", "start_offset": 62, "end_offset": 93}, {"id": 1091789, "label": "ENT", "start_offset": 101, "end_offset": 142}, {"id": 1091790, "label": "ENT", "start_offset": 257, "end_offset": 296}, {"id": 1091791, "label": "ENT", "start_offset": 309, "end_offset": 331}, {"id": 1091792, "label": "ENT", "start_offset": 348, "end_offset": 373}, {"id": 1091793, "label": "ENT", "start_offset": 391, "end_offset": 416}, {"id": 1091794, "label": "ENT", "start_offset": 455, "end_offset": 474}, {"id": 1091795, "label": "ENT", "start_offset": 486, "end_offset": 524}, {"id": 1091796, "label": "ENT", "start_offset": 534, "end_offset": 548}, {"id": 1091797, "label": "ENT", "start_offset": 654, "end_offset": 682}], "relations": [{"id": 175165, "from_id": 1091790, "to_id": 1091791, "type": "USED-FOR"}, {"id": 175166, "from_id": 1091791, "to_id": 1091792, "type": "USED-FOR"}, {"id": 175167, "from_id": 1091788, "to_id": 1091793, "type": "COREF"}, {"id": 175168, "from_id": 1091793, "to_id": 1091795, "type": "COMPARE"}, {"id": 175169, "from_id": 1091794, "to_id": 1091795, "type": "EVALUATE-FOR"}, {"id": 175170, "from_id": 1091794, "to_id": 1091793, "type": "EVALUATE-FOR"}, {"id": 175171, "from_id": 1091788, "to_id": 1091789, "type": "USED-FOR"}]}
{"id": "P05-1074", "text": " Previous work has used  monolingual parallel corpora  to extract and generate  paraphrases  . We show that this task can be done using  bilingual parallel corpora  , a much more commonly available  resource  . Using  alignment techniques  from  phrase-based statistical machine translation  , we show how  paraphrases  in one  language  can be identified using a  phrase  in another language as a pivot. We define a  paraphrase probability  that allows  paraphrases  extracted from a  bilingual parallel corpus  to be ranked using  translation probabilities  , and show how it can be refined to take  contextual information  into account. We evaluate our  paraphrase extraction and ranking methods  using a set of  manual word alignments  , and contrast the  quality  with  paraphrases  extracted from  automatic alignments  . ", "Comments": [], "entities": [{"id": 1091798, "label": "ENT", "start_offset": 25, "end_offset": 53}, {"id": 1091799, "label": "ENT", "start_offset": 80, "end_offset": 91}, {"id": 1091800, "label": "ENT", "start_offset": 113, "end_offset": 117}, {"id": 1091801, "label": "ENT", "start_offset": 137, "end_offset": 163}, {"id": 1091802, "label": "ENT", "start_offset": 218, "end_offset": 238}, {"id": 1091803, "label": "ENT", "start_offset": 246, "end_offset": 290}, {"id": 1091804, "label": "ENT", "start_offset": 307, "end_offset": 318}, {"id": 1091805, "label": "ENT", "start_offset": 418, "end_offset": 440}, {"id": 1091806, "label": "ENT", "start_offset": 455, "end_offset": 466}, {"id": 1091807, "label": "ENT", "start_offset": 486, "end_offset": 511}, {"id": 1091808, "label": "ENT", "start_offset": 533, "end_offset": 558}, {"id": 1091809, "label": "ENT", "start_offset": 575, "end_offset": 577}, {"id": 1091810, "label": "ENT", "start_offset": 602, "end_offset": 624}, {"id": 1091811, "label": "ENT", "start_offset": 657, "end_offset": 698}, {"id": 1091812, "label": "ENT", "start_offset": 716, "end_offset": 738}, {"id": 1091813, "label": "ENT", "start_offset": 760, "end_offset": 767}, {"id": 1091814, "label": "ENT", "start_offset": 775, "end_offset": 786}, {"id": 1091815, "label": "ENT", "start_offset": 804, "end_offset": 824}], "relations": [{"id": 175172, "from_id": 1091798, "to_id": 1091799, "type": "USED-FOR"}, {"id": 175173, "from_id": 1091806, "to_id": 1091807, "type": "PART-OF"}, {"id": 175174, "from_id": 1091814, "to_id": 1091815, "type": "PART-OF"}, {"id": 175175, "from_id": 1091801, "to_id": 1091798, "type": "COREF"}, {"id": 175176, "from_id": 1091801, "to_id": 1091800, "type": "USED-FOR"}, {"id": 175177, "from_id": 1091802, "to_id": 1091803, "type": "USED-FOR"}, {"id": 175178, "from_id": 1091808, "to_id": 1091806, "type": "USED-FOR"}, {"id": 175179, "from_id": 1091805, "to_id": 1091809, "type": "COREF"}, {"id": 175180, "from_id": 1091810, "to_id": 1091809, "type": "USED-FOR"}, {"id": 175181, "from_id": 1091812, "to_id": 1091811, "type": "EVALUATE-FOR"}, {"id": 175182, "from_id": 1091813, "to_id": 1091814, "type": "EVALUATE-FOR"}]}
{"id": "AAAI_2015_11_abs", "text": "We derive a convex optimization problem for the task of segmenting sequential data, which explicitly treats presence of outliers. We describe two algorithms for solving this problem, one exact and one a top-down novel approach , and we derive a consistency results for the case of two segments and no outliers. Robustness to outliers is evaluated on two real-world tasks related to speech segmentation. Our algorithms outperform baseline seg-mentation algorithms.", "Comments": [], "entities": [{"id": 1091816, "label": "ENT", "start_offset": 12, "end_offset": 39}, {"id": 1091817, "label": "ENT", "start_offset": 56, "end_offset": 82}, {"id": 1091818, "label": "ENT", "start_offset": 120, "end_offset": 128}, {"id": 1091819, "label": "ENT", "start_offset": 146, "end_offset": 156}, {"id": 1091820, "label": "ENT", "start_offset": 174, "end_offset": 181}, {"id": 1091821, "label": "ENT", "start_offset": 311, "end_offset": 321}, {"id": 1091822, "label": "ENT", "start_offset": 325, "end_offset": 333}, {"id": 1091823, "label": "ENT", "start_offset": 354, "end_offset": 370}, {"id": 1091824, "label": "ENT", "start_offset": 382, "end_offset": 401}, {"id": 1091825, "label": "ENT", "start_offset": 407, "end_offset": 417}, {"id": 1091826, "label": "ENT", "start_offset": 429, "end_offset": 462}], "relations": [{"id": 175183, "from_id": 1091816, "to_id": 1091817, "type": "USED-FOR"}, {"id": 175184, "from_id": 1091820, "to_id": 1091816, "type": "COREF"}, {"id": 175185, "from_id": 1091819, "to_id": 1091820, "type": "USED-FOR"}, {"id": 175186, "from_id": 1091825, "to_id": 1091826, "type": "COMPARE"}, {"id": 175187, "from_id": 1091819, "to_id": 1091825, "type": "COREF"}, {"id": 175188, "from_id": 1091817, "to_id": 1091818, "type": "USED-FOR"}, {"id": 175189, "from_id": 1091822, "to_id": 1091821, "type": "FEATURE-OF"}, {"id": 175190, "from_id": 1091823, "to_id": 1091821, "type": "EVALUATE-FOR"}, {"id": 175191, "from_id": 1091824, "to_id": 1091821, "type": "EVALUATE-FOR"}, {"id": 175192, "from_id": 1091824, "to_id": 1091823, "type": "FEATURE-OF"}]}
{"id": "A97-1028", "text": " In this paper we present a  statistical profile  of the  Named Entity task , a specific  information extraction task  for which  corpora  in several  languages  are available. Using the  results  of the  statistical analysis , we propose an  algorithm  for  lower bound estimation  for  Named Entity corpora  and discuss the significance of the  cross-lingual comparisons  provided by the  analysis . ", "Comments": [], "entities": [{"id": 1091850, "label": "ENT", "start_offset": 29, "end_offset": 48}, {"id": 1091851, "label": "ENT", "start_offset": 58, "end_offset": 75}, {"id": 1091852, "label": "ENT", "start_offset": 90, "end_offset": 117}, {"id": 1091853, "label": "ENT", "start_offset": 205, "end_offset": 225}, {"id": 1091854, "label": "ENT", "start_offset": 243, "end_offset": 252}, {"id": 1091855, "label": "ENT", "start_offset": 259, "end_offset": 281}, {"id": 1091856, "label": "ENT", "start_offset": 288, "end_offset": 308}, {"id": 1091857, "label": "ENT", "start_offset": 391, "end_offset": 399}], "relations": [{"id": 175213, "from_id": 1091850, "to_id": 1091851, "type": "USED-FOR"}, {"id": 175214, "from_id": 1091854, "to_id": 1091855, "type": "USED-FOR"}, {"id": 175215, "from_id": 1091851, "to_id": 1091852, "type": "HYPONYM-OF"}, {"id": 175216, "from_id": 1091857, "to_id": 1091853, "type": "COREF"}, {"id": 175217, "from_id": 1091853, "to_id": 1091850, "type": "COREF"}, {"id": 175218, "from_id": 1091853, "to_id": 1091854, "type": "USED-FOR"}, {"id": 175219, "from_id": 1091855, "to_id": 1091856, "type": "USED-FOR"}]}
{"id": "C04-1116", "text": " We present a  text mining method  for finding  synonymous expressions  based on the  distributional hypothesis  in a set of coherent  corpora . This paper proposes a new methodology to improve the  accuracy  of a  term aggregation system  using each author's  text  as a coherent  corpus . Our approach is based on the idea that one person tends to use one  expression  for one  meaning . According to our assumption, most of the  words  with  similar context features  in each author's  corpus  tend not to be  synonymous expressions . Our proposed method improves the  accuracy  of our  term aggregation system , showing that our approach is successful. ", "Comments": [], "entities": [{"id": 1091887, "label": "ENT", "start_offset": 15, "end_offset": 33}, {"id": 1091888, "label": "ENT", "start_offset": 48, "end_offset": 70}, {"id": 1091889, "label": "ENT", "start_offset": 86, "end_offset": 111}, {"id": 1091890, "label": "ENT", "start_offset": 171, "end_offset": 182}, {"id": 1091891, "label": "ENT", "start_offset": 199, "end_offset": 207}, {"id": 1091892, "label": "ENT", "start_offset": 215, "end_offset": 238}, {"id": 1091893, "label": "ENT", "start_offset": 295, "end_offset": 303}, {"id": 1091894, "label": "ENT", "start_offset": 445, "end_offset": 469}, {"id": 1091895, "label": "ENT", "start_offset": 513, "end_offset": 535}, {"id": 1091896, "label": "ENT", "start_offset": 551, "end_offset": 557}, {"id": 1091897, "label": "ENT", "start_offset": 572, "end_offset": 580}, {"id": 1091898, "label": "ENT", "start_offset": 590, "end_offset": 613}, {"id": 1091899, "label": "ENT", "start_offset": 633, "end_offset": 641}], "relations": [{"id": 175246, "from_id": 1091889, "to_id": 1091887, "type": "USED-FOR"}, {"id": 175247, "from_id": 1091887, "to_id": 1091888, "type": "USED-FOR"}, {"id": 175248, "from_id": 1091890, "to_id": 1091887, "type": "COREF"}, {"id": 175249, "from_id": 1091893, "to_id": 1091890, "type": "COREF"}, {"id": 175250, "from_id": 1091896, "to_id": 1091893, "type": "COREF"}, {"id": 175251, "from_id": 1091898, "to_id": 1091892, "type": "COREF"}, {"id": 175252, "from_id": 1091892, "to_id": 1091890, "type": "EVALUATE-FOR"}, {"id": 175253, "from_id": 1091898, "to_id": 1091896, "type": "EVALUATE-FOR"}, {"id": 175254, "from_id": 1091899, "to_id": 1091896, "type": "COREF"}, {"id": 175255, "from_id": 1091891, "to_id": 1091892, "type": "EVALUATE-FOR"}, {"id": 175256, "from_id": 1091897, "to_id": 1091898, "type": "EVALUATE-FOR"}]}
{"id": "C04-1102", "text": " We propose a  detection method  for orthographic variants caused by  transliteration  in a large  corpus . The method employs two  similarities . One is  string similarity  based on  edit distance . The other is  contextual similarity  by a  vector space model . Experimental results show that the method performed a 0.889  F-measure  in an open test. ", "Comments": [], "entities": [{"id": 1091900, "label": "ENT", "start_offset": 15, "end_offset": 31}, {"id": 1091901, "label": "ENT", "start_offset": 37, "end_offset": 58}, {"id": 1091902, "label": "ENT", "start_offset": 70, "end_offset": 85}, {"id": 1091903, "label": "ENT", "start_offset": 112, "end_offset": 118}, {"id": 1091904, "label": "ENT", "start_offset": 132, "end_offset": 144}, {"id": 1091905, "label": "ENT", "start_offset": 155, "end_offset": 172}, {"id": 1091906, "label": "ENT", "start_offset": 184, "end_offset": 197}, {"id": 1091907, "label": "ENT", "start_offset": 214, "end_offset": 235}, {"id": 1091908, "label": "ENT", "start_offset": 243, "end_offset": 261}, {"id": 1091909, "label": "ENT", "start_offset": 299, "end_offset": 305}, {"id": 1091910, "label": "ENT", "start_offset": 325, "end_offset": 334}], "relations": [{"id": 175257, "from_id": 1091906, "to_id": 1091905, "type": "USED-FOR"}, {"id": 175258, "from_id": 1091908, "to_id": 1091907, "type": "USED-FOR"}, {"id": 175259, "from_id": 1091900, "to_id": 1091901, "type": "USED-FOR"}, {"id": 175260, "from_id": 1091903, "to_id": 1091900, "type": "COREF"}, {"id": 175261, "from_id": 1091904, "to_id": 1091903, "type": "USED-FOR"}, {"id": 175262, "from_id": 1091905, "to_id": 1091904, "type": "HYPONYM-OF"}, {"id": 175263, "from_id": 1091907, "to_id": 1091904, "type": "HYPONYM-OF"}, {"id": 175264, "from_id": 1091909, "to_id": 1091903, "type": "COREF"}, {"id": 175265, "from_id": 1091910, "to_id": 1091909, "type": "EVALUATE-FOR"}]}
{"id": "C90-3014", "text": "   This paper describes the framework of a  Korean phonological knowledge base system  using the  unification-based grammar formalism  :  Korean Phonology Structure Grammar (KPSG)  . The approach of  KPSG  provides an explicit development model for constructing a computational  phonological system  :  speech recognition  and  synthesis system  . We show that the proposed approach is more describable than other approaches such as those employing a traditional  generative phonological approach  . ", "Comments": [], "entities": [{"id": 1091938, "label": "ENT", "start_offset": 44, "end_offset": 85}, {"id": 1091939, "label": "ENT", "start_offset": 98, "end_offset": 133}, {"id": 1091940, "label": "ENT", "start_offset": 138, "end_offset": 179}, {"id": 1091941, "label": "ENT", "start_offset": 187, "end_offset": 195}, {"id": 1091942, "label": "ENT", "start_offset": 200, "end_offset": 204}, {"id": 1091943, "label": "ENT", "start_offset": 279, "end_offset": 298}, {"id": 1091944, "label": "ENT", "start_offset": 303, "end_offset": 344}, {"id": 1091945, "label": "ENT", "start_offset": 374, "end_offset": 382}, {"id": 1091946, "label": "ENT", "start_offset": 414, "end_offset": 424}, {"id": 1091947, "label": "ENT", "start_offset": 433, "end_offset": 438}, {"id": 1091948, "label": "ENT", "start_offset": 464, "end_offset": 496}], "relations": [{"id": 175285, "from_id": 1091939, "to_id": 1091938, "type": "USED-FOR"}, {"id": 175286, "from_id": 1091942, "to_id": 1091943, "type": "USED-FOR"}, {"id": 175287, "from_id": 1091942, "to_id": 1091940, "type": "COREF"}, {"id": 175288, "from_id": 1091945, "to_id": 1091946, "type": "COMPARE"}, {"id": 175289, "from_id": 1091945, "to_id": 1091941, "type": "COREF"}, {"id": 175290, "from_id": 1091947, "to_id": 1091946, "type": "COREF"}, {"id": 175291, "from_id": 1091948, "to_id": 1091947, "type": "USED-FOR"}, {"id": 175292, "from_id": 1091940, "to_id": 1091939, "type": "HYPONYM-OF"}, {"id": 175293, "from_id": 1091944, "to_id": 1091943, "type": "COREF"}, {"id": 175294, "from_id": 1091941, "to_id": 1091942, "type": "USED-FOR"}]}
{"id": "N06-1018", "text": " Recent years have seen increasing research on extracting and using temporal information in  natural language applications . However most of the works found in the literature have focused on identifying and understanding  temporal expressions  in  newswire texts . In this paper we report our work on anchoring  temporal expressions  in a novel  genre , emails. The highly under-specified nature of these  expressions  fits well with our  constraint-based representation  of time,  Time Calculus for Natural Language (TCNL) . We have developed and evaluated a  Temporal Expression Anchoror (TEA) , and the result shows that it performs significantly better than the  baseline , and compares favorably with some of the closely related work.", "Comments": [], "entities": [{"id": 1091949, "label": "ENT", "start_offset": 68, "end_offset": 88}, {"id": 1091950, "label": "ENT", "start_offset": 93, "end_offset": 122}, {"id": 1091951, "label": "ENT", "start_offset": 222, "end_offset": 242}, {"id": 1091952, "label": "ENT", "start_offset": 248, "end_offset": 262}, {"id": 1091953, "label": "ENT", "start_offset": 312, "end_offset": 332}, {"id": 1091954, "label": "ENT", "start_offset": 354, "end_offset": 360}, {"id": 1091955, "label": "ENT", "start_offset": 406, "end_offset": 417}, {"id": 1091956, "label": "ENT", "start_offset": 439, "end_offset": 479}, {"id": 1091957, "label": "ENT", "start_offset": 482, "end_offset": 523}, {"id": 1091958, "label": "ENT", "start_offset": 561, "end_offset": 595}, {"id": 1091959, "label": "ENT", "start_offset": 624, "end_offset": 626}, {"id": 1091960, "label": "ENT", "start_offset": 667, "end_offset": 675}], "relations": [{"id": 175295, "from_id": 1091955, "to_id": 1091953, "type": "COREF"}, {"id": 175296, "from_id": 1091957, "to_id": 1091956, "type": "HYPONYM-OF"}, {"id": 175297, "from_id": 1091958, "to_id": 1091959, "type": "COREF"}, {"id": 175298, "from_id": 1091959, "to_id": 1091960, "type": "COMPARE"}, {"id": 175299, "from_id": 1091954, "to_id": 1091953, "type": "FEATURE-OF"}, {"id": 175300, "from_id": 1091952, "to_id": 1091951, "type": "FEATURE-OF"}]}
{"id": "N03-1012", "text": " In this paper we present  ONTOSCORE  , a system for scoring sets of  concepts  on the basis of an  ontology  . We apply our system to the task of  scoring  alternative  speech recognition hypotheses (SRH)  in terms of their  semantic coherence  . We conducted an  annotation experiment  and showed that  human annotators  can reliably differentiate between semantically coherent and incoherent  speech recognition hypotheses  . An evaluation of our system against the  annotated data  shows that, it successfully classifies 73.2% in a  German corpus  of 2.284  SRHs  as either coherent or incoherent (given a  baseline  of 54.55%). ", "Comments": [], "entities": [{"id": 1091976, "label": "ENT", "start_offset": 27, "end_offset": 36}, {"id": 1091977, "label": "ENT", "start_offset": 42, "end_offset": 48}, {"id": 1091978, "label": "ENT", "start_offset": 100, "end_offset": 108}, {"id": 1091979, "label": "ENT", "start_offset": 125, "end_offset": 131}, {"id": 1091980, "label": "ENT", "start_offset": 170, "end_offset": 205}, {"id": 1091981, "label": "ENT", "start_offset": 226, "end_offset": 244}, {"id": 1091982, "label": "ENT", "start_offset": 396, "end_offset": 425}, {"id": 1091983, "label": "ENT", "start_offset": 450, "end_offset": 456}, {"id": 1091984, "label": "ENT", "start_offset": 498, "end_offset": 500}, {"id": 1091985, "label": "ENT", "start_offset": 537, "end_offset": 550}, {"id": 1091986, "label": "ENT", "start_offset": 562, "end_offset": 566}], "relations": [{"id": 175311, "from_id": 1091976, "to_id": 1091977, "type": "COREF"}, {"id": 175312, "from_id": 1091978, "to_id": 1091977, "type": "USED-FOR"}, {"id": 175313, "from_id": 1091979, "to_id": 1091977, "type": "COREF"}, {"id": 175314, "from_id": 1091979, "to_id": 1091980, "type": "USED-FOR"}, {"id": 175315, "from_id": 1091983, "to_id": 1091979, "type": "COREF"}, {"id": 175316, "from_id": 1091983, "to_id": 1091984, "type": "COREF"}, {"id": 175317, "from_id": 1091984, "to_id": 1091985, "type": "COREF"}]}
{"id": "CVPR_2011_292_abs", "text": "Video provides not only rich visual cues such as motion and appearance, but also much less explored long-range temporal interactions among objects. We aim to capture such interactions and to construct a powerful intermediate-level video representation for subsequent recognition. Motivated by this goal, we seek to obtain spatio-temporal over-segmentation of a video into regions that respect object boundaries and, at the same time, associate object pix-els over many video frames. The contributions of this paper are twofold. First, we develop an efficient spatio-temporal video segmentation algorithm, which naturally incorporates long-range motion cues from the past and future frames in the form of clusters of point tracks with coherent motion. Second, we devise a new track clustering cost function that includes occlusion reasoning, in the form of depth ordering constraints, as well as motion similarity along the tracks. We evaluate the proposed approach on a challenging set of video sequences of office scenes from feature length movies.", "Comments": [], "entities": [{"id": 1091987, "label": "ENT", "start_offset": 0, "end_offset": 5}, {"id": 1091988, "label": "ENT", "start_offset": 29, "end_offset": 40}, {"id": 1091989, "label": "ENT", "start_offset": 49, "end_offset": 55}, {"id": 1091990, "label": "ENT", "start_offset": 60, "end_offset": 70}, {"id": 1091991, "label": "ENT", "start_offset": 100, "end_offset": 132}, {"id": 1091992, "label": "ENT", "start_offset": 171, "end_offset": 183}, {"id": 1091993, "label": "ENT", "start_offset": 212, "end_offset": 251}, {"id": 1091994, "label": "ENT", "start_offset": 267, "end_offset": 278}, {"id": 1091995, "label": "ENT", "start_offset": 322, "end_offset": 355}, {"id": 1091996, "label": "ENT", "start_offset": 393, "end_offset": 410}, {"id": 1091997, "label": "ENT", "start_offset": 559, "end_offset": 603}, {"id": 1091998, "label": "ENT", "start_offset": 634, "end_offset": 656}, {"id": 1091999, "label": "ENT", "start_offset": 704, "end_offset": 728}, {"id": 1092000, "label": "ENT", "start_offset": 775, "end_offset": 805}, {"id": 1092001, "label": "ENT", "start_offset": 820, "end_offset": 839}, {"id": 1092002, "label": "ENT", "start_offset": 856, "end_offset": 882}, {"id": 1092003, "label": "ENT", "start_offset": 895, "end_offset": 912}, {"id": 1092004, "label": "ENT", "start_offset": 956, "end_offset": 964}, {"id": 1092005, "label": "ENT", "start_offset": 989, "end_offset": 1021}, {"id": 1092006, "label": "ENT", "start_offset": 1042, "end_offset": 1048}], "relations": [{"id": 175318, "from_id": 1091989, "to_id": 1091990, "type": "CONJUNCTION"}, {"id": 175319, "from_id": 1091993, "to_id": 1091994, "type": "USED-FOR"}, {"id": 175320, "from_id": 1091988, "to_id": 1091987, "type": "FEATURE-OF"}, {"id": 175321, "from_id": 1091989, "to_id": 1091988, "type": "HYPONYM-OF"}, {"id": 175322, "from_id": 1091990, "to_id": 1091988, "type": "HYPONYM-OF"}, {"id": 175323, "from_id": 1091992, "to_id": 1091991, "type": "COREF"}, {"id": 175324, "from_id": 1092001, "to_id": 1092000, "type": "PART-OF"}, {"id": 175325, "from_id": 1092003, "to_id": 1092000, "type": "PART-OF"}, {"id": 175326, "from_id": 1091999, "to_id": 1091998, "type": "USED-FOR"}, {"id": 175327, "from_id": 1091998, "to_id": 1091997, "type": "USED-FOR"}, {"id": 175328, "from_id": 1092005, "to_id": 1092004, "type": "EVALUATE-FOR"}, {"id": 175329, "from_id": 1092002, "to_id": 1092001, "type": "FEATURE-OF"}]}
{"id": "I05-4008", "text": "  Taiwan Child Language Corpus  contains  scripts  transcribed from about 330 hours of  recordings  of fourteen young children from  Southern Min Chinese  speaking families in Taiwan. The format of the  corpus  adopts the  Child Language Data Exchange System (CHILDES) . The size of the  corpus  is about 1.6 million  words . In this paper, we describe  data collection ,  transcription ,  word segmentation , and  part-of-speech annotation  of this  corpus . Applications of the  corpus  are also discussed. ", "Comments": [], "entities": [{"id": 1092051, "label": "ENT", "start_offset": 2, "end_offset": 30}, {"id": 1092052, "label": "ENT", "start_offset": 203, "end_offset": 209}, {"id": 1092053, "label": "ENT", "start_offset": 223, "end_offset": 268}, {"id": 1092054, "label": "ENT", "start_offset": 288, "end_offset": 294}, {"id": 1092055, "label": "ENT", "start_offset": 354, "end_offset": 369}, {"id": 1092056, "label": "ENT", "start_offset": 373, "end_offset": 386}, {"id": 1092057, "label": "ENT", "start_offset": 390, "end_offset": 407}, {"id": 1092058, "label": "ENT", "start_offset": 415, "end_offset": 440}, {"id": 1092059, "label": "ENT", "start_offset": 451, "end_offset": 457}, {"id": 1092060, "label": "ENT", "start_offset": 481, "end_offset": 487}], "relations": [{"id": 175362, "from_id": 1092053, "to_id": 1092052, "type": "FEATURE-OF"}, {"id": 175363, "from_id": 1092058, "to_id": 1092059, "type": "USED-FOR"}, {"id": 175364, "from_id": 1092052, "to_id": 1092051, "type": "COREF"}, {"id": 175365, "from_id": 1092054, "to_id": 1092052, "type": "COREF"}, {"id": 175366, "from_id": 1092055, "to_id": 1092059, "type": "USED-FOR"}, {"id": 175367, "from_id": 1092056, "to_id": 1092059, "type": "USED-FOR"}, {"id": 175368, "from_id": 1092057, "to_id": 1092059, "type": "USED-FOR"}, {"id": 175369, "from_id": 1092057, "to_id": 1092058, "type": "CONJUNCTION"}, {"id": 175370, "from_id": 1092056, "to_id": 1092057, "type": "CONJUNCTION"}, {"id": 175371, "from_id": 1092055, "to_id": 1092056, "type": "CONJUNCTION"}, {"id": 175372, "from_id": 1092059, "to_id": 1092054, "type": "COREF"}, {"id": 175373, "from_id": 1092060, "to_id": 1092059, "type": "COREF"}]}
{"id": "NIPS_2016_80_abs", "text": "Fast algorithms for nearest neighbor (NN) search have in large part focused on 2 distance. Here we develop an approach for 1 distance that begins with an explicit and exactly distance-preserving embedding of the points into 2 2. We show how this can efficiently be combined with random-projection based methods for 2 NN search, such as locality-sensitive hashing (LSH) or random projection trees. We rigorously establish the correctness of the methodology and show by experimentation using LSH that it is competitive in practice with available alternatives.", "Comments": [], "entities": [{"id": 1092061, "label": "ENT", "start_offset": 0, "end_offset": 15}, {"id": 1092062, "label": "ENT", "start_offset": 20, "end_offset": 48}, {"id": 1092063, "label": "ENT", "start_offset": 81, "end_offset": 89}, {"id": 1092064, "label": "ENT", "start_offset": 110, "end_offset": 118}, {"id": 1092065, "label": "ENT", "start_offset": 123, "end_offset": 133}, {"id": 1092066, "label": "ENT", "start_offset": 175, "end_offset": 204}, {"id": 1092067, "label": "ENT", "start_offset": 241, "end_offset": 245}, {"id": 1092068, "label": "ENT", "start_offset": 279, "end_offset": 310}, {"id": 1092069, "label": "ENT", "start_offset": 317, "end_offset": 326}, {"id": 1092070, "label": "ENT", "start_offset": 336, "end_offset": 368}, {"id": 1092071, "label": "ENT", "start_offset": 372, "end_offset": 395}, {"id": 1092072, "label": "ENT", "start_offset": 490, "end_offset": 493}, {"id": 1092073, "label": "ENT", "start_offset": 499, "end_offset": 501}, {"id": 1092074, "label": "ENT", "start_offset": 544, "end_offset": 556}], "relations": [{"id": 175374, "from_id": 1092061, "to_id": 1092062, "type": "USED-FOR"}, {"id": 175375, "from_id": 1092064, "to_id": 1092065, "type": "USED-FOR"}, {"id": 175376, "from_id": 1092062, "to_id": 1092069, "type": "COREF"}, {"id": 175377, "from_id": 1092064, "to_id": 1092067, "type": "COREF"}, {"id": 175378, "from_id": 1092068, "to_id": 1092069, "type": "USED-FOR"}, {"id": 175379, "from_id": 1092067, "to_id": 1092068, "type": "CONJUNCTION"}, {"id": 175380, "from_id": 1092070, "to_id": 1092068, "type": "HYPONYM-OF"}, {"id": 175381, "from_id": 1092071, "to_id": 1092068, "type": "HYPONYM-OF"}, {"id": 175382, "from_id": 1092070, "to_id": 1092071, "type": "CONJUNCTION"}, {"id": 175383, "from_id": 1092070, "to_id": 1092072, "type": "COREF"}, {"id": 175384, "from_id": 1092072, "to_id": 1092073, "type": "COREF"}, {"id": 175385, "from_id": 1092073, "to_id": 1092074, "type": "COMPARE"}]}
{"id": "H93-1076", "text": " Two themes have evolved in  speech and text image processing  work at  Xerox PARC  that expand and redefine the role of  recognition technology  in  document-oriented applications . One is the development of systems that provide functionality similar to that of  text processors  but operate directly on  audio and scanned image data . A second, related theme is the use of  speech and text-image recognition  to retrieve arbitrary, user-specified information from  documents with signal content . This paper discusses three research initiatives at  PARC  that exemplify these themes: a  text-image editor [1], a  wordspotter  for  voice editing and indexing [12], and a  decoding framework  for  scanned-document content retrieval [4]. The discussion focuses on key concepts embodied in the research that enable novel  signal-based document processing functionality . ", "Comments": [], "entities": [{"id": 1092075, "label": "ENT", "start_offset": 5, "end_offset": 11}, {"id": 1092076, "label": "ENT", "start_offset": 29, "end_offset": 61}, {"id": 1092077, "label": "ENT", "start_offset": 122, "end_offset": 144}, {"id": 1092078, "label": "ENT", "start_offset": 150, "end_offset": 180}, {"id": 1092079, "label": "ENT", "start_offset": 183, "end_offset": 186}, {"id": 1092080, "label": "ENT", "start_offset": 209, "end_offset": 216}, {"id": 1092081, "label": "ENT", "start_offset": 264, "end_offset": 279}, {"id": 1092082, "label": "ENT", "start_offset": 306, "end_offset": 334}, {"id": 1092083, "label": "ENT", "start_offset": 355, "end_offset": 360}, {"id": 1092084, "label": "ENT", "start_offset": 376, "end_offset": 409}, {"id": 1092085, "label": "ENT", "start_offset": 467, "end_offset": 496}, {"id": 1092086, "label": "ENT", "start_offset": 526, "end_offset": 534}, {"id": 1092087, "label": "ENT", "start_offset": 578, "end_offset": 584}, {"id": 1092088, "label": "ENT", "start_offset": 589, "end_offset": 606}, {"id": 1092089, "label": "ENT", "start_offset": 615, "end_offset": 626}, {"id": 1092090, "label": "ENT", "start_offset": 633, "end_offset": 659}, {"id": 1092091, "label": "ENT", "start_offset": 673, "end_offset": 691}, {"id": 1092092, "label": "ENT", "start_offset": 698, "end_offset": 732}, {"id": 1092093, "label": "ENT", "start_offset": 821, "end_offset": 867}], "relations": [{"id": 175386, "from_id": 1092077, "to_id": 1092078, "type": "USED-FOR"}, {"id": 175387, "from_id": 1092085, "to_id": 1092084, "type": "USED-FOR"}, {"id": 175388, "from_id": 1092079, "to_id": 1092075, "type": "HYPONYM-OF"}, {"id": 175389, "from_id": 1092080, "to_id": 1092079, "type": "COREF"}, {"id": 175390, "from_id": 1092084, "to_id": 1092083, "type": "USED-FOR"}, {"id": 175391, "from_id": 1092083, "to_id": 1092075, "type": "HYPONYM-OF"}, {"id": 175392, "from_id": 1092091, "to_id": 1092092, "type": "USED-FOR"}, {"id": 175393, "from_id": 1092088, "to_id": 1092089, "type": "CONJUNCTION"}, {"id": 175394, "from_id": 1092089, "to_id": 1092090, "type": "CONJUNCTION"}, {"id": 175395, "from_id": 1092090, "to_id": 1092091, "type": "CONJUNCTION"}, {"id": 175396, "from_id": 1092082, "to_id": 1092080, "type": "USED-FOR"}, {"id": 175397, "from_id": 1092075, "to_id": 1092076, "type": "PART-OF"}, {"id": 175398, "from_id": 1092087, "to_id": 1092075, "type": "COREF"}, {"id": 175399, "from_id": 1092080, "to_id": 1092081, "type": "CONJUNCTION"}, {"id": 175400, "from_id": 1092088, "to_id": 1092086, "type": "HYPONYM-OF"}, {"id": 175401, "from_id": 1092089, "to_id": 1092086, "type": "HYPONYM-OF"}, {"id": 175402, "from_id": 1092091, "to_id": 1092086, "type": "HYPONYM-OF"}]}
{"id": "INTERSPEECH_2014_31_abs", "text": "During late-2013 through early-2014 NIST coordinated a special i-vector challenge based on data used in previous NIST Speaker Recognition Evaluations (SREs). Unlike evaluations in the SRE series, the i-vector challenge was run entirely online and used fixed-length feature vectors projected into a low-dimensional space (i-vectors) rather than audio recordings. These changes made the challenge more readily accessible, especially to participants from outside the audio processing field. Compared to the 2012 SRE, the i-vector challenge saw an increase in the number of participants by nearly a factor of two, and a two orders of magnitude increase in the number of systems submitted for evaluation. Initial results indicate the leading system achieved an approximate 37% improvement relative to the baseline system.", "Comments": [], "entities": [{"id": 1092127, "label": "ENT", "start_offset": 63, "end_offset": 81}, {"id": 1092128, "label": "ENT", "start_offset": 113, "end_offset": 156}, {"id": 1092129, "label": "ENT", "start_offset": 184, "end_offset": 194}, {"id": 1092130, "label": "ENT", "start_offset": 200, "end_offset": 218}, {"id": 1092131, "label": "ENT", "start_offset": 252, "end_offset": 280}, {"id": 1092132, "label": "ENT", "start_offset": 298, "end_offset": 331}, {"id": 1092133, "label": "ENT", "start_offset": 344, "end_offset": 360}, {"id": 1092134, "label": "ENT", "start_offset": 464, "end_offset": 486}, {"id": 1092135, "label": "ENT", "start_offset": 509, "end_offset": 512}, {"id": 1092136, "label": "ENT", "start_offset": 518, "end_offset": 536}, {"id": 1092137, "label": "ENT", "start_offset": 729, "end_offset": 743}, {"id": 1092138, "label": "ENT", "start_offset": 800, "end_offset": 815}], "relations": [{"id": 175432, "from_id": 1092127, "to_id": 1092130, "type": "COREF"}, {"id": 175433, "from_id": 1092128, "to_id": 1092135, "type": "COREF"}, {"id": 175434, "from_id": 1092137, "to_id": 1092138, "type": "COMPARE"}, {"id": 175435, "from_id": 1092135, "to_id": 1092136, "type": "COMPARE"}, {"id": 175436, "from_id": 1092128, "to_id": 1092127, "type": "USED-FOR"}, {"id": 175437, "from_id": 1092128, "to_id": 1092129, "type": "COREF"}, {"id": 175438, "from_id": 1092132, "to_id": 1092130, "type": "USED-FOR"}, {"id": 175439, "from_id": 1092131, "to_id": 1092132, "type": "USED-FOR"}, {"id": 175440, "from_id": 1092133, "to_id": 1092132, "type": "COMPARE"}]}
{"id": "INTERSPEECH_2000_483_abs", "text": "Is it possible to use out-of-domain acoustic training data to improve a speech recognizer's performance on a speciic, independent application? In our experiments, we use Wallstreet Journal (WSJ) data to train a recognizer, which is adapted and evaluated in the Phonebook domain. Apart from their common language (US English), the two corpora diier in many important respects: microphone vs. telephone channel, continuous speech vs. isolated words, mismatch i n s p e a k i n g r a t e. This paper deals with two questions. First, starting from the WSJ-trained recognizer, how much adaptation data (taken from the Phonebook training corpus) is necessary to achieve a reasonable recognition performance in spite of the high degree of mismatch? Second, is it possible to improve the recognition performance of a Phonebook-trained baseline acoustic model by using additional out-of-domain training data? The paper describes the adaptation and normalization techniques used to bridge the mismatch b e-tween the two corpora.", "Comments": [], "entities": [{"id": 1092155, "label": "ENT", "start_offset": 22, "end_offset": 58}, {"id": 1092156, "label": "ENT", "start_offset": 72, "end_offset": 89}, {"id": 1092157, "label": "ENT", "start_offset": 170, "end_offset": 199}, {"id": 1092158, "label": "ENT", "start_offset": 211, "end_offset": 221}, {"id": 1092159, "label": "ENT", "start_offset": 261, "end_offset": 277}, {"id": 1092160, "label": "ENT", "start_offset": 296, "end_offset": 324}, {"id": 1092161, "label": "ENT", "start_offset": 376, "end_offset": 408}, {"id": 1092162, "label": "ENT", "start_offset": 410, "end_offset": 427}, {"id": 1092163, "label": "ENT", "start_offset": 432, "end_offset": 446}, {"id": 1092164, "label": "ENT", "start_offset": 548, "end_offset": 570}, {"id": 1092165, "label": "ENT", "start_offset": 581, "end_offset": 596}, {"id": 1092166, "label": "ENT", "start_offset": 613, "end_offset": 638}, {"id": 1092167, "label": "ENT", "start_offset": 677, "end_offset": 688}, {"id": 1092168, "label": "ENT", "start_offset": 732, "end_offset": 740}, {"id": 1092169, "label": "ENT", "start_offset": 780, "end_offset": 791}, {"id": 1092170, "label": "ENT", "start_offset": 809, "end_offset": 850}, {"id": 1092171, "label": "ENT", "start_offset": 871, "end_offset": 898}, {"id": 1092172, "label": "ENT", "start_offset": 924, "end_offset": 963}], "relations": [{"id": 175456, "from_id": 1092155, "to_id": 1092156, "type": "USED-FOR"}, {"id": 175457, "from_id": 1092157, "to_id": 1092158, "type": "USED-FOR"}, {"id": 175458, "from_id": 1092159, "to_id": 1092158, "type": "EVALUATE-FOR"}, {"id": 175459, "from_id": 1092165, "to_id": 1092166, "type": "PART-OF"}, {"id": 175460, "from_id": 1092165, "to_id": 1092164, "type": "USED-FOR"}, {"id": 175461, "from_id": 1092164, "to_id": 1092167, "type": "USED-FOR"}, {"id": 175462, "from_id": 1092170, "to_id": 1092169, "type": "USED-FOR"}, {"id": 175463, "from_id": 1092171, "to_id": 1092170, "type": "USED-FOR"}, {"id": 175464, "from_id": 1092167, "to_id": 1092169, "type": "COREF"}, {"id": 175465, "from_id": 1092156, "to_id": 1092158, "type": "COREF"}]}
{"id": "C88-1007", "text": "   This paper discusses the application of  Unification Categorial Grammar (UCG)  to the framework of  Isomorphic Grammars  for  Machine Translation  pioneered by Landsbergen. The  Isomorphic Grammars approach to MT  involves developing the  grammars  of the  Source and Target languages  in parallel, in order to ensure that  SL  and  TL  expressions which stand in the  translation relation  have  isomorphic derivations  . The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited, obviating the need for answers to  semantic questions  that we do not yet have.  Semantic  and other information may still be incorporated, but as constraints on the  translation relation  , not as levels of  textual representation  . After introducing this approach to  MT system  design, and the basics of  monolingual UCG  , we will show how the two can be integrated, and present an example from an implemented  bi-directional English-Spanish fragment  . Finally we will present some outstanding problems with the approach. ", "Comments": [], "entities": [{"id": 1092216, "label": "ENT", "start_offset": 44, "end_offset": 80}, {"id": 1092217, "label": "ENT", "start_offset": 103, "end_offset": 122}, {"id": 1092218, "label": "ENT", "start_offset": 129, "end_offset": 148}, {"id": 1092219, "label": "ENT", "start_offset": 181, "end_offset": 209}, {"id": 1092220, "label": "ENT", "start_offset": 213, "end_offset": 215}, {"id": 1092221, "label": "ENT", "start_offset": 372, "end_offset": 392}, {"id": 1092222, "label": "ENT", "start_offset": 400, "end_offset": 422}, {"id": 1092223, "label": "ENT", "start_offset": 729, "end_offset": 749}, {"id": 1092224, "label": "ENT", "start_offset": 771, "end_offset": 793}, {"id": 1092225, "label": "ENT", "start_offset": 820, "end_offset": 828}, {"id": 1092226, "label": "ENT", "start_offset": 833, "end_offset": 850}, {"id": 1092227, "label": "ENT", "start_offset": 871, "end_offset": 886}, {"id": 1092228, "label": "ENT", "start_offset": 911, "end_offset": 914}, {"id": 1092229, "label": "ENT", "start_offset": 978, "end_offset": 1017}, {"id": 1092230, "label": "ENT", "start_offset": 1080, "end_offset": 1088}], "relations": [{"id": 175504, "from_id": 1092217, "to_id": 1092218, "type": "USED-FOR"}, {"id": 175505, "from_id": 1092225, "to_id": 1092227, "type": "USED-FOR"}, {"id": 175506, "from_id": 1092230, "to_id": 1092225, "type": "COREF"}, {"id": 175507, "from_id": 1092227, "to_id": 1092228, "type": "HYPONYM-OF"}, {"id": 175508, "from_id": 1092216, "to_id": 1092217, "type": "USED-FOR"}, {"id": 175509, "from_id": 1092217, "to_id": 1092219, "type": "COREF"}, {"id": 175510, "from_id": 1092219, "to_id": 1092220, "type": "USED-FOR"}, {"id": 175511, "from_id": 1092220, "to_id": 1092218, "type": "COREF"}, {"id": 175512, "from_id": 1092219, "to_id": 1092225, "type": "COREF"}, {"id": 175513, "from_id": 1092220, "to_id": 1092226, "type": "COREF"}, {"id": 175514, "from_id": 1092225, "to_id": 1092226, "type": "USED-FOR"}, {"id": 175515, "from_id": 1092226, "to_id": 1092227, "type": "CONJUNCTION"}, {"id": 175516, "from_id": 1092226, "to_id": 1092228, "type": "HYPONYM-OF"}]}
{"id": "E95-1021", "text": " In this paper we compare two competing approaches to  part-of-speech tagging ,  statistical and constraint-based disambiguation , using  French  as our  test language . We imposed a time limit on our experiment: the amount of time spent on the design of our  constraint system  was about the same as the time we used to train and test the easy-to-implement  statistical model . We describe the two systems and compare the results. The  accuracy  of the  statistical method  is reasonably good, comparable to  taggers  for  English . But the  constraint-based tagger  seems to be superior even with the limited time we allowed ourselves for  rule development . ", "Comments": [], "entities": [{"id": 1092273, "label": "ENT", "start_offset": 40, "end_offset": 50}, {"id": 1092274, "label": "ENT", "start_offset": 55, "end_offset": 77}, {"id": 1092275, "label": "ENT", "start_offset": 81, "end_offset": 128}, {"id": 1092276, "label": "ENT", "start_offset": 138, "end_offset": 144}, {"id": 1092277, "label": "ENT", "start_offset": 260, "end_offset": 277}, {"id": 1092278, "label": "ENT", "start_offset": 359, "end_offset": 376}, {"id": 1092279, "label": "ENT", "start_offset": 399, "end_offset": 406}, {"id": 1092280, "label": "ENT", "start_offset": 437, "end_offset": 445}, {"id": 1092281, "label": "ENT", "start_offset": 455, "end_offset": 473}, {"id": 1092282, "label": "ENT", "start_offset": 510, "end_offset": 517}, {"id": 1092283, "label": "ENT", "start_offset": 524, "end_offset": 531}, {"id": 1092284, "label": "ENT", "start_offset": 543, "end_offset": 566}], "relations": [{"id": 175551, "from_id": 1092277, "to_id": 1092278, "type": "COMPARE"}, {"id": 175552, "from_id": 1092281, "to_id": 1092282, "type": "COMPARE"}, {"id": 175553, "from_id": 1092273, "to_id": 1092274, "type": "USED-FOR"}, {"id": 175554, "from_id": 1092275, "to_id": 1092273, "type": "COREF"}, {"id": 175555, "from_id": 1092276, "to_id": 1092273, "type": "USED-FOR"}, {"id": 175556, "from_id": 1092281, "to_id": 1092278, "type": "COREF"}, {"id": 175557, "from_id": 1092282, "to_id": 1092283, "type": "USED-FOR"}, {"id": 175558, "from_id": 1092284, "to_id": 1092277, "type": "COREF"}, {"id": 175559, "from_id": 1092279, "to_id": 1092273, "type": "COREF"}, {"id": 175560, "from_id": 1092277, "to_id": 1092273, "type": "HYPONYM-OF"}, {"id": 175561, "from_id": 1092278, "to_id": 1092273, "type": "HYPONYM-OF"}, {"id": 175562, "from_id": 1092280, "to_id": 1092281, "type": "EVALUATE-FOR"}, {"id": 175563, "from_id": 1092280, "to_id": 1092282, "type": "EVALUATE-FOR"}]}
{"id": "E06-1035", "text": " In this paper, we investigate the problem of automatically predicting  segment boundaries  in  spoken multiparty dialogue  . We extend prior work in two ways. We first apply approaches that have been proposed for  predicting top-level topic shifts  to the problem of  identifying subtopic boundaries  . We then explore the impact on  performance  of using  ASR output  as opposed to  human transcription  . Examination of the effect of  features  shows that  predicting top-level and predicting subtopic boundaries  are two distinct tasks: (1) for predicting  subtopic boundaries  , the  lexical cohesion-based approach  alone can achieve competitive results, (2) for  predicting top-level boundaries  , the  machine learning approach  that combines  lexical-cohesion and conversational features  performs best, and (3)  conversational cues  , such as  cue phrases  and  overlapping speech  , are better indicators for the top-level prediction task. We also find that the  transcription errors  inevitable in  ASR output  have a negative impact on models that combine  lexical-cohesion and conversational features  , but do not change the general preference of approach for the two tasks. ", "Comments": [], "entities": [{"id": 1092319, "label": "ENT", "start_offset": 60, "end_offset": 90}, {"id": 1092320, "label": "ENT", "start_offset": 96, "end_offset": 122}, {"id": 1092321, "label": "ENT", "start_offset": 175, "end_offset": 185}, {"id": 1092322, "label": "ENT", "start_offset": 215, "end_offset": 248}, {"id": 1092323, "label": "ENT", "start_offset": 269, "end_offset": 300}, {"id": 1092324, "label": "ENT", "start_offset": 358, "end_offset": 368}, {"id": 1092325, "label": "ENT", "start_offset": 385, "end_offset": 404}, {"id": 1092326, "label": "ENT", "start_offset": 438, "end_offset": 446}, {"id": 1092327, "label": "ENT", "start_offset": 460, "end_offset": 515}, {"id": 1092328, "label": "ENT", "start_offset": 534, "end_offset": 539}, {"id": 1092329, "label": "ENT", "start_offset": 549, "end_offset": 580}, {"id": 1092330, "label": "ENT", "start_offset": 589, "end_offset": 620}, {"id": 1092331, "label": "ENT", "start_offset": 670, "end_offset": 701}, {"id": 1092332, "label": "ENT", "start_offset": 710, "end_offset": 735}, {"id": 1092333, "label": "ENT", "start_offset": 752, "end_offset": 796}, {"id": 1092334, "label": "ENT", "start_offset": 822, "end_offset": 841}, {"id": 1092335, "label": "ENT", "start_offset": 854, "end_offset": 865}, {"id": 1092336, "label": "ENT", "start_offset": 872, "end_offset": 890}, {"id": 1092337, "label": "ENT", "start_offset": 905, "end_offset": 915}, {"id": 1092338, "label": "ENT", "start_offset": 924, "end_offset": 949}, {"id": 1092339, "label": "ENT", "start_offset": 974, "end_offset": 994}, {"id": 1092340, "label": "ENT", "start_offset": 1011, "end_offset": 1021}, {"id": 1092341, "label": "ENT", "start_offset": 1049, "end_offset": 1055}, {"id": 1092342, "label": "ENT", "start_offset": 1070, "end_offset": 1114}, {"id": 1092343, "label": "ENT", "start_offset": 1183, "end_offset": 1188}], "relations": [{"id": 175594, "from_id": 1092324, "to_id": 1092325, "type": "COMPARE"}, {"id": 175595, "from_id": 1092332, "to_id": 1092331, "type": "USED-FOR"}, {"id": 175596, "from_id": 1092339, "to_id": 1092340, "type": "FEATURE-OF"}, {"id": 175597, "from_id": 1092322, "to_id": 1092323, "type": "FEATURE-OF"}, {"id": 175598, "from_id": 1092321, "to_id": 1092323, "type": "USED-FOR"}, {"id": 175599, "from_id": 1092323, "to_id": 1092327, "type": "HYPONYM-OF"}, {"id": 175600, "from_id": 1092331, "to_id": 1092327, "type": "PART-OF"}, {"id": 175601, "from_id": 1092333, "to_id": 1092332, "type": "CONJUNCTION"}, {"id": 175602, "from_id": 1092335, "to_id": 1092334, "type": "HYPONYM-OF"}, {"id": 175603, "from_id": 1092336, "to_id": 1092334, "type": "HYPONYM-OF"}, {"id": 175604, "from_id": 1092336, "to_id": 1092335, "type": "CONJUNCTION"}, {"id": 175605, "from_id": 1092337, "to_id": 1092334, "type": "COREF"}, {"id": 175606, "from_id": 1092337, "to_id": 1092338, "type": "USED-FOR"}, {"id": 175607, "from_id": 1092338, "to_id": 1092331, "type": "COREF"}, {"id": 175608, "from_id": 1092341, "to_id": 1092342, "type": "CONJUNCTION"}, {"id": 175609, "from_id": 1092328, "to_id": 1092327, "type": "COREF"}, {"id": 175610, "from_id": 1092343, "to_id": 1092328, "type": "COREF"}, {"id": 175611, "from_id": 1092341, "to_id": 1092332, "type": "COREF"}, {"id": 175612, "from_id": 1092321, "to_id": 1092322, "type": "USED-FOR"}, {"id": 175613, "from_id": 1092329, "to_id": 1092327, "type": "PART-OF"}, {"id": 175614, "from_id": 1092329, "to_id": 1092323, "type": "COREF"}, {"id": 175615, "from_id": 1092330, "to_id": 1092329, "type": "USED-FOR"}, {"id": 175616, "from_id": 1092320, "to_id": 1092319, "type": "USED-FOR"}]}
{"id": "CVPR_2016_405_abs", "text": "With the recent popularity of animated GIFs on social media, there is need for ways to index them with rich meta-data. To advance research on animated GIF understanding, we collected a new dataset, Tumblr GIF (TGIF), with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing. The motivation for this work is to develop a testbed for image sequence description systems, where the task is to generate natural language descriptions for animated GIFs or video clips. To ensure a high quality dataset, we developed a series of novel quality controls to validate free-form text input from crowd-workers. We show that there is unambiguous association between visual content and natural language descriptions in our dataset, making it an ideal benchmark for the visual content captioning task. We perform extensive statistical analyses to compare our dataset to existing image and video description datasets. Next, we provide baseline results on the animated GIF description task, using three representative techniques: nearest neighbor, statistical machine translation , and recurrent neural networks. Finally, we show that models fine-tuned from our animated GIF description dataset can be helpful for automatic movie description.", "Comments": [], "entities": [{"id": 1092361, "label": "ENT", "start_offset": 30, "end_offset": 43}, {"id": 1092362, "label": "ENT", "start_offset": 47, "end_offset": 59}, {"id": 1092363, "label": "ENT", "start_offset": 103, "end_offset": 117}, {"id": 1092364, "label": "ENT", "start_offset": 142, "end_offset": 168}, {"id": 1092365, "label": "ENT", "start_offset": 189, "end_offset": 196}, {"id": 1092366, "label": "ENT", "start_offset": 198, "end_offset": 215}, {"id": 1092367, "label": "ENT", "start_offset": 227, "end_offset": 240}, {"id": 1092368, "label": "ENT", "start_offset": 262, "end_offset": 291}, {"id": 1092369, "label": "ENT", "start_offset": 305, "end_offset": 318}, {"id": 1092370, "label": "ENT", "start_offset": 377, "end_offset": 411}, {"id": 1092371, "label": "ENT", "start_offset": 443, "end_offset": 472}, {"id": 1092372, "label": "ENT", "start_offset": 477, "end_offset": 490}, {"id": 1092373, "label": "ENT", "start_offset": 494, "end_offset": 505}, {"id": 1092374, "label": "ENT", "start_offset": 572, "end_offset": 588}, {"id": 1092375, "label": "ENT", "start_offset": 601, "end_offset": 621}, {"id": 1092376, "label": "ENT", "start_offset": 696, "end_offset": 710}, {"id": 1092377, "label": "ENT", "start_offset": 715, "end_offset": 744}, {"id": 1092378, "label": "ENT", "start_offset": 752, "end_offset": 759}, {"id": 1092379, "label": "ENT", "start_offset": 768, "end_offset": 770}, {"id": 1092380, "label": "ENT", "start_offset": 798, "end_offset": 828}, {"id": 1092381, "label": "ENT", "start_offset": 851, "end_offset": 871}, {"id": 1092382, "label": "ENT", "start_offset": 887, "end_offset": 894}, {"id": 1092383, "label": "ENT", "start_offset": 907, "end_offset": 943}, {"id": 1092384, "label": "ENT", "start_offset": 986, "end_offset": 1015}, {"id": 1092385, "label": "ENT", "start_offset": 1029, "end_offset": 1054}, {"id": 1092386, "label": "ENT", "start_offset": 1056, "end_offset": 1072}, {"id": 1092387, "label": "ENT", "start_offset": 1074, "end_offset": 1105}, {"id": 1092388, "label": "ENT", "start_offset": 1112, "end_offset": 1137}, {"id": 1092389, "label": "ENT", "start_offset": 1188, "end_offset": 1220}, {"id": 1092390, "label": "ENT", "start_offset": 1240, "end_offset": 1267}], "relations": [{"id": 175632, "from_id": 1092366, "to_id": 1092365, "type": "COREF"}, {"id": 175633, "from_id": 1092371, "to_id": 1092372, "type": "USED-FOR"}, {"id": 175634, "from_id": 1092371, "to_id": 1092373, "type": "USED-FOR"}, {"id": 175635, "from_id": 1092377, "to_id": 1092378, "type": "PART-OF"}, {"id": 175636, "from_id": 1092376, "to_id": 1092378, "type": "PART-OF"}, {"id": 175637, "from_id": 1092378, "to_id": 1092365, "type": "COREF"}, {"id": 175638, "from_id": 1092382, "to_id": 1092378, "type": "COREF"}, {"id": 175639, "from_id": 1092389, "to_id": 1092382, "type": "COREF"}, {"id": 175640, "from_id": 1092362, "to_id": 1092361, "type": "FEATURE-OF"}, {"id": 175641, "from_id": 1092369, "to_id": 1092368, "type": "USED-FOR"}, {"id": 175642, "from_id": 1092368, "to_id": 1092367, "type": "CONJUNCTION"}, {"id": 175643, "from_id": 1092365, "to_id": 1092364, "type": "USED-FOR"}, {"id": 175644, "from_id": 1092372, "to_id": 1092373, "type": "CONJUNCTION"}, {"id": 175645, "from_id": 1092376, "to_id": 1092377, "type": "CONJUNCTION"}, {"id": 175646, "from_id": 1092378, "to_id": 1092379, "type": "COREF"}, {"id": 175647, "from_id": 1092379, "to_id": 1092380, "type": "EVALUATE-FOR"}, {"id": 175648, "from_id": 1092382, "to_id": 1092383, "type": "COMPARE"}, {"id": 175649, "from_id": 1092386, "to_id": 1092387, "type": "CONJUNCTION"}, {"id": 175650, "from_id": 1092387, "to_id": 1092388, "type": "CONJUNCTION"}, {"id": 175651, "from_id": 1092386, "to_id": 1092385, "type": "HYPONYM-OF"}, {"id": 175652, "from_id": 1092387, "to_id": 1092385, "type": "HYPONYM-OF"}, {"id": 175653, "from_id": 1092388, "to_id": 1092385, "type": "HYPONYM-OF"}, {"id": 175654, "from_id": 1092385, "to_id": 1092384, "type": "USED-FOR"}, {"id": 175655, "from_id": 1092389, "to_id": 1092390, "type": "USED-FOR"}, {"id": 175656, "from_id": 1092374, "to_id": 1092375, "type": "USED-FOR"}]}
{"id": "ICML_2016_18_abs", "text": "In the Object Recognition task, there exists a di-chotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep archi-tectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose estimation using these approaches has received relatively less attention. In this work, we study how Convolutional Neural Networks (CNN) architectures can be adapted to the task of simultaneous object recognition and pose estimation. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations. We extensively experiment on two recent large and challenging multi-view datasets and we achieve better than the state-of-the-art.", "Comments": [], "entities": [{"id": 1092391, "label": "ENT", "start_offset": 7, "end_offset": 30}, {"id": 1092392, "label": "ENT", "start_offset": 70, "end_offset": 95}, {"id": 1092393, "label": "ENT", "start_offset": 100, "end_offset": 122}, {"id": 1092394, "label": "ENT", "start_offset": 134, "end_offset": 140}, {"id": 1092395, "label": "ENT", "start_offset": 156, "end_offset": 185}, {"id": 1092396, "label": "ENT", "start_offset": 197, "end_offset": 203}, {"id": 1092397, "label": "ENT", "start_offset": 215, "end_offset": 229}, {"id": 1092398, "label": "ENT", "start_offset": 251, "end_offset": 267}, {"id": 1092399, "label": "ENT", "start_offset": 323, "end_offset": 342}, {"id": 1092400, "label": "ENT", "start_offset": 372, "end_offset": 399}, {"id": 1092401, "label": "ENT", "start_offset": 401, "end_offset": 422}, {"id": 1092402, "label": "ENT", "start_offset": 458, "end_offset": 462}, {"id": 1092403, "label": "ENT", "start_offset": 477, "end_offset": 499}, {"id": 1092404, "label": "ENT", "start_offset": 512, "end_offset": 522}, {"id": 1092405, "label": "ENT", "start_offset": 590, "end_offset": 639}, {"id": 1092406, "label": "ENT", "start_offset": 683, "end_offset": 701}, {"id": 1092407, "label": "ENT", "start_offset": 706, "end_offset": 721}, {"id": 1092408, "label": "ENT", "start_offset": 754, "end_offset": 760}, {"id": 1092409, "label": "ENT", "start_offset": 772, "end_offset": 782}, {"id": 1092410, "label": "ENT", "start_offset": 815, "end_offset": 819}, {"id": 1092411, "label": "ENT", "start_offset": 857, "end_offset": 894}, {"id": 1092412, "label": "ENT", "start_offset": 902, "end_offset": 906}, {"id": 1092413, "label": "ENT", "start_offset": 917, "end_offset": 940}, {"id": 1092414, "label": "ENT", "start_offset": 949, "end_offset": 953}, {"id": 1092415, "label": "ENT", "start_offset": 971, "end_offset": 1002}, {"id": 1092416, "label": "ENT", "start_offset": 1066, "end_offset": 1085}], "relations": [{"id": 175657, "from_id": 1092394, "to_id": 1092392, "type": "COREF"}, {"id": 175658, "from_id": 1092396, "to_id": 1092393, "type": "COREF"}, {"id": 175659, "from_id": 1092392, "to_id": 1092393, "type": "CONJUNCTION"}, {"id": 175660, "from_id": 1092397, "to_id": 1092398, "type": "USED-FOR"}, {"id": 175661, "from_id": 1092397, "to_id": 1092396, "type": "USED-FOR"}, {"id": 175662, "from_id": 1092395, "to_id": 1092394, "type": "USED-FOR"}, {"id": 175663, "from_id": 1092392, "to_id": 1092391, "type": "PART-OF"}, {"id": 175664, "from_id": 1092393, "to_id": 1092391, "type": "PART-OF"}, {"id": 175665, "from_id": 1092399, "to_id": 1092400, "type": "USED-FOR"}, {"id": 175666, "from_id": 1092400, "to_id": 1092392, "type": "COREF"}, {"id": 175667, "from_id": 1092402, "to_id": 1092400, "type": "COREF"}, {"id": 175668, "from_id": 1092403, "to_id": 1092393, "type": "COREF"}, {"id": 175669, "from_id": 1092404, "to_id": 1092401, "type": "COREF"}, {"id": 175670, "from_id": 1092407, "to_id": 1092403, "type": "COREF"}, {"id": 175671, "from_id": 1092409, "to_id": 1092405, "type": "COREF"}, {"id": 175672, "from_id": 1092411, "to_id": 1092412, "type": "PART-OF"}, {"id": 175673, "from_id": 1092411, "to_id": 1092413, "type": "USED-FOR"}, {"id": 175674, "from_id": 1092414, "to_id": 1092411, "type": "COREF"}, {"id": 175675, "from_id": 1092414, "to_id": 1092415, "type": "COMPARE"}, {"id": 175676, "from_id": 1092412, "to_id": 1092409, "type": "COREF"}, {"id": 175677, "from_id": 1092408, "to_id": 1092409, "type": "PART-OF"}, {"id": 175678, "from_id": 1092410, "to_id": 1092408, "type": "COREF"}, {"id": 175679, "from_id": 1092404, "to_id": 1092403, "type": "USED-FOR"}, {"id": 175680, "from_id": 1092406, "to_id": 1092407, "type": "CONJUNCTION"}, {"id": 175681, "from_id": 1092405, "to_id": 1092406, "type": "USED-FOR"}, {"id": 175682, "from_id": 1092405, "to_id": 1092407, "type": "USED-FOR"}, {"id": 175683, "from_id": 1092406, "to_id": 1092402, "type": "COREF"}]}
{"id": "H92-1045", "text": "   It is well-known that there are  polysemous words  like  sentence  whose  meaning  or  sense  depends on the context of use. We have recently reported on two new  word-sense disambiguation systems  , one trained on  bilingual material  (the  Canadian Hansards  ) and the other trained on  monolingual material  (  Roget's Thesaurus  and  Grolier's Encyclopedia  ). As this work was nearing completion, we observed a very strong  discourse  effect. That is, if a  polysemous word  such as  sentence  appears two or more times in a  well-written discourse  , it is extremely likely that they will all share the same  sense  . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share  sense  in the same  discourse  is extremely strong (98%). This result can be used as an additional source of  constraint  for improving the performance of the  word-sense disambiguation algorithm  . In addition, it could also be used to help evaluate  disambiguation algorithms  that did not make use of the  discourse constraint  . ", "Comments": [], "entities": [{"id": 1092472, "label": "ENT", "start_offset": 36, "end_offset": 52}, {"id": 1092473, "label": "ENT", "start_offset": 166, "end_offset": 199}, {"id": 1092474, "label": "ENT", "start_offset": 203, "end_offset": 206}, {"id": 1092475, "label": "ENT", "start_offset": 219, "end_offset": 237}, {"id": 1092476, "label": "ENT", "start_offset": 245, "end_offset": 262}, {"id": 1092477, "label": "ENT", "start_offset": 274, "end_offset": 279}, {"id": 1092478, "label": "ENT", "start_offset": 292, "end_offset": 312}, {"id": 1092479, "label": "ENT", "start_offset": 317, "end_offset": 334}, {"id": 1092480, "label": "ENT", "start_offset": 341, "end_offset": 363}, {"id": 1092481, "label": "ENT", "start_offset": 432, "end_offset": 441}, {"id": 1092482, "label": "ENT", "start_offset": 466, "end_offset": 481}, {"id": 1092483, "label": "ENT", "start_offset": 534, "end_offset": 556}, {"id": 1092484, "label": "ENT", "start_offset": 752, "end_offset": 761}, {"id": 1092485, "label": "ENT", "start_offset": 842, "end_offset": 852}, {"id": 1092486, "label": "ENT", "start_offset": 892, "end_offset": 927}, {"id": 1092487, "label": "ENT", "start_offset": 944, "end_offset": 946}, {"id": 1092488, "label": "ENT", "start_offset": 984, "end_offset": 1009}, {"id": 1092489, "label": "ENT", "start_offset": 1041, "end_offset": 1061}], "relations": [{"id": 175727, "from_id": 1092474, "to_id": 1092473, "type": "HYPONYM-OF"}, {"id": 175728, "from_id": 1092477, "to_id": 1092473, "type": "HYPONYM-OF"}, {"id": 175729, "from_id": 1092475, "to_id": 1092474, "type": "EVALUATE-FOR"}, {"id": 175730, "from_id": 1092476, "to_id": 1092475, "type": "COREF"}, {"id": 175731, "from_id": 1092479, "to_id": 1092478, "type": "HYPONYM-OF"}, {"id": 175732, "from_id": 1092480, "to_id": 1092478, "type": "HYPONYM-OF"}, {"id": 175733, "from_id": 1092479, "to_id": 1092480, "type": "CONJUNCTION"}, {"id": 175734, "from_id": 1092474, "to_id": 1092477, "type": "CONJUNCTION"}, {"id": 175735, "from_id": 1092478, "to_id": 1092477, "type": "USED-FOR"}, {"id": 175736, "from_id": 1092487, "to_id": 1092488, "type": "EVALUATE-FOR"}, {"id": 175737, "from_id": 1092486, "to_id": 1092473, "type": "COREF"}]}
{"id": "P83-1004", "text": "  Metagrammatical formalisms  that combine  context-free phrase structure rules  and  metarules (MPS grammars)  allow concise statement of generalizations about the  syntax  of  natural languages .  Unconstrained MPS grammars , unfortunately, are not computationally safe. We evaluate several proposals for constraining them, basing our assessment on  computational tractability and explanatory adequacy . We show that none of them satisfies both criteria, and suggest new directions for research on alternative  metagrammatical formalisms . ", "Comments": [], "entities": [{"id": 1092507, "label": "ENT", "start_offset": 2, "end_offset": 28}, {"id": 1092508, "label": "ENT", "start_offset": 44, "end_offset": 79}, {"id": 1092509, "label": "ENT", "start_offset": 86, "end_offset": 110}, {"id": 1092510, "label": "ENT", "start_offset": 166, "end_offset": 195}, {"id": 1092511, "label": "ENT", "start_offset": 199, "end_offset": 225}, {"id": 1092512, "label": "ENT", "start_offset": 320, "end_offset": 324}, {"id": 1092513, "label": "ENT", "start_offset": 352, "end_offset": 403}, {"id": 1092514, "label": "ENT", "start_offset": 427, "end_offset": 431}, {"id": 1092515, "label": "ENT", "start_offset": 447, "end_offset": 455}, {"id": 1092516, "label": "ENT", "start_offset": 513, "end_offset": 539}], "relations": [{"id": 175748, "from_id": 1092508, "to_id": 1092507, "type": "PART-OF"}, {"id": 175749, "from_id": 1092508, "to_id": 1092509, "type": "CONJUNCTION"}, {"id": 175750, "from_id": 1092509, "to_id": 1092507, "type": "PART-OF"}, {"id": 175751, "from_id": 1092511, "to_id": 1092512, "type": "COREF"}, {"id": 175752, "from_id": 1092512, "to_id": 1092514, "type": "COREF"}, {"id": 175753, "from_id": 1092513, "to_id": 1092515, "type": "COREF"}, {"id": 175754, "from_id": 1092513, "to_id": 1092512, "type": "EVALUATE-FOR"}]}
{"id": "H90-1016", "text": "   We describe the methods and hardware that we are using to produce a real-time demonstration of an  integrated Spoken Language System  . We describe algorithms that greatly reduce the computation needed to compute the  N-Best sentence hypotheses  . To avoid  grammar coverage problems  we use a  fully-connected first-order statistical class grammar  . The  speech-search algorithm  is implemented on a  board  with a single  Intel i860 chip  , which provides a factor of 5 speed-up over a  SUN 4  for  straight C code  . The  board  plugs directly into the  VME bus  of the  SUN4  , which controls the system and contains the  natural language system  and  application back end  . ", "Comments": [], "entities": [{"id": 1092517, "label": "ENT", "start_offset": 19, "end_offset": 26}, {"id": 1092518, "label": "ENT", "start_offset": 31, "end_offset": 39}, {"id": 1092519, "label": "ENT", "start_offset": 102, "end_offset": 135}, {"id": 1092520, "label": "ENT", "start_offset": 151, "end_offset": 161}, {"id": 1092521, "label": "ENT", "start_offset": 221, "end_offset": 247}, {"id": 1092522, "label": "ENT", "start_offset": 261, "end_offset": 286}, {"id": 1092523, "label": "ENT", "start_offset": 298, "end_offset": 351}, {"id": 1092524, "label": "ENT", "start_offset": 360, "end_offset": 383}, {"id": 1092525, "label": "ENT", "start_offset": 406, "end_offset": 411}, {"id": 1092526, "label": "ENT", "start_offset": 428, "end_offset": 443}, {"id": 1092527, "label": "ENT", "start_offset": 493, "end_offset": 498}, {"id": 1092528, "label": "ENT", "start_offset": 505, "end_offset": 520}, {"id": 1092529, "label": "ENT", "start_offset": 529, "end_offset": 534}, {"id": 1092530, "label": "ENT", "start_offset": 561, "end_offset": 568}, {"id": 1092531, "label": "ENT", "start_offset": 578, "end_offset": 582}, {"id": 1092532, "label": "ENT", "start_offset": 605, "end_offset": 611}, {"id": 1092533, "label": "ENT", "start_offset": 630, "end_offset": 653}, {"id": 1092534, "label": "ENT", "start_offset": 660, "end_offset": 680}], "relations": [{"id": 175755, "from_id": 1092523, "to_id": 1092522, "type": "USED-FOR"}, {"id": 175756, "from_id": 1092530, "to_id": 1092531, "type": "PART-OF"}, {"id": 175757, "from_id": 1092517, "to_id": 1092518, "type": "CONJUNCTION"}, {"id": 175758, "from_id": 1092517, "to_id": 1092519, "type": "USED-FOR"}, {"id": 175759, "from_id": 1092518, "to_id": 1092519, "type": "USED-FOR"}, {"id": 175760, "from_id": 1092520, "to_id": 1092521, "type": "USED-FOR"}, {"id": 175761, "from_id": 1092526, "to_id": 1092527, "type": "COMPARE"}, {"id": 175762, "from_id": 1092527, "to_id": 1092528, "type": "USED-FOR"}, {"id": 175763, "from_id": 1092526, "to_id": 1092528, "type": "USED-FOR"}, {"id": 175764, "from_id": 1092533, "to_id": 1092534, "type": "CONJUNCTION"}, {"id": 175765, "from_id": 1092532, "to_id": 1092519, "type": "COREF"}, {"id": 175766, "from_id": 1092524, "to_id": 1092520, "type": "COREF"}, {"id": 175767, "from_id": 1092520, "to_id": 1092517, "type": "COREF"}, {"id": 175768, "from_id": 1092526, "to_id": 1092525, "type": "PART-OF"}, {"id": 175769, "from_id": 1092525, "to_id": 1092529, "type": "COREF"}, {"id": 175770, "from_id": 1092529, "to_id": 1092532, "type": "USED-FOR"}, {"id": 175771, "from_id": 1092533, "to_id": 1092529, "type": "PART-OF"}, {"id": 175772, "from_id": 1092534, "to_id": 1092529, "type": "PART-OF"}, {"id": 175773, "from_id": 1092525, "to_id": 1092524, "type": "USED-FOR"}]}
{"id": "ECCV_2012_37_abs", "text": "The goal of this paper is to discover a set of discriminative patches which can serve as a fully unsupervised mid-level visual representation. The desired patches need to satisfy two requirements: 1) to be representative, they need to occur frequently enough in the visual world; 2) to be discriminative, they need to be different enough from the rest of the visual world. The patches could correspond to parts, objects, \" visual phrases \" , etc. but are not restricted to be any one of them. We pose this as an unsupervised discriminative clustering problem on a huge dataset of image patches. We use an iterative procedure which alternates between clustering and training discriminative classifiers, while applying careful cross-validation at each step to prevent overfitting. The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation, suggesting that it could be used in place of visual words for many tasks. Furthermore, discrim-inative patches can also be used in a supervised regime, such as scene classification, where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset.", "Comments": [], "entities": [{"id": 1092535, "label": "ENT", "start_offset": 47, "end_offset": 69}, {"id": 1092536, "label": "ENT", "start_offset": 97, "end_offset": 141}, {"id": 1092537, "label": "ENT", "start_offset": 377, "end_offset": 384}, {"id": 1092538, "label": "ENT", "start_offset": 501, "end_offset": 505}, {"id": 1092539, "label": "ENT", "start_offset": 512, "end_offset": 558}, {"id": 1092540, "label": "ENT", "start_offset": 580, "end_offset": 593}, {"id": 1092541, "label": "ENT", "start_offset": 605, "end_offset": 624}, {"id": 1092542, "label": "ENT", "start_offset": 650, "end_offset": 660}, {"id": 1092543, "label": "ENT", "start_offset": 665, "end_offset": 700}, {"id": 1092544, "label": "ENT", "start_offset": 725, "end_offset": 741}, {"id": 1092545, "label": "ENT", "start_offset": 766, "end_offset": 777}, {"id": 1092546, "label": "ENT", "start_offset": 838, "end_offset": 860}, {"id": 1092547, "label": "ENT", "start_offset": 867, "end_offset": 911}, {"id": 1092548, "label": "ENT", "start_offset": 929, "end_offset": 931}, {"id": 1092549, "label": "ENT", "start_offset": 1000, "end_offset": 1023}, {"id": 1092550, "label": "ENT", "start_offset": 1046, "end_offset": 1063}, {"id": 1092551, "label": "ENT", "start_offset": 1073, "end_offset": 1093}, {"id": 1092552, "label": "ENT", "start_offset": 1101, "end_offset": 1105}, {"id": 1092553, "label": "ENT", "start_offset": 1154, "end_offset": 1175}], "relations": [{"id": 175774, "from_id": 1092535, "to_id": 1092536, "type": "USED-FOR"}, {"id": 175775, "from_id": 1092540, "to_id": 1092539, "type": "USED-FOR"}, {"id": 175776, "from_id": 1092538, "to_id": 1092535, "type": "COREF"}, {"id": 175777, "from_id": 1092537, "to_id": 1092535, "type": "COREF"}, {"id": 175778, "from_id": 1092544, "to_id": 1092545, "type": "USED-FOR"}, {"id": 175779, "from_id": 1092546, "to_id": 1092547, "type": "USED-FOR"}, {"id": 175780, "from_id": 1092548, "to_id": 1092546, "type": "USED-FOR"}, {"id": 175781, "from_id": 1092546, "to_id": 1092535, "type": "COREF"}, {"id": 175782, "from_id": 1092547, "to_id": 1092536, "type": "COREF"}, {"id": 175783, "from_id": 1092549, "to_id": 1092546, "type": "COREF"}, {"id": 175784, "from_id": 1092549, "to_id": 1092550, "type": "USED-FOR"}, {"id": 175785, "from_id": 1092551, "to_id": 1092550, "type": "HYPONYM-OF"}, {"id": 175786, "from_id": 1092552, "to_id": 1092549, "type": "COREF"}, {"id": 175787, "from_id": 1092553, "to_id": 1092552, "type": "EVALUATE-FOR"}]}
{"id": "IJCAI_2013_5_abs", "text": "Constraint propagation is one of the key techniques in constraint programming, and a large body of work has built up around it. Special-purpose constraint propagation algorithms frequently make implicit use of short supports \u2014 by examining a subset of the variables, they can infer support (a justification that a variable-value pair still forms part of a solution to the constraint) for all other variables and values and save substantial work. Recently short supports have been used in general purpose prop-agators, and (when the constraint is amenable to short supports) speed ups of more than three orders of magnitude have been demonstrated. In this paper we present SHORTSTR2, a development of the Simple Tabular Reduction algorithm STR2+. We show that SHORTSTR2 is complementary to the existing algorithms SHORTGAC and HAGGISGAC that exploit short supports, while being much simpler. When a constraint is amenable to short supports, the short support set can be exponentially smaller than the full-length support set. Therefore SHORTSTR2 can efficiently propagate many constraints that STR2+ cannot even load into memory. We also show that SHORTSTR2 can be combined with a simple algorithm to identify short supports from full-length supports, to provide a superior drop-in replacement for STR2+.", "Comments": [], "entities": [{"id": 1092589, "label": "ENT", "start_offset": 0, "end_offset": 22}, {"id": 1092590, "label": "ENT", "start_offset": 55, "end_offset": 77}, {"id": 1092591, "label": "ENT", "start_offset": 128, "end_offset": 177}, {"id": 1092592, "label": "ENT", "start_offset": 488, "end_offset": 516}, {"id": 1092593, "label": "ENT", "start_offset": 532, "end_offset": 542}, {"id": 1092594, "label": "ENT", "start_offset": 672, "end_offset": 681}, {"id": 1092595, "label": "ENT", "start_offset": 704, "end_offset": 744}, {"id": 1092596, "label": "ENT", "start_offset": 759, "end_offset": 768}, {"id": 1092597, "label": "ENT", "start_offset": 813, "end_offset": 821}, {"id": 1092598, "label": "ENT", "start_offset": 826, "end_offset": 835}, {"id": 1092599, "label": "ENT", "start_offset": 898, "end_offset": 908}, {"id": 1092600, "label": "ENT", "start_offset": 924, "end_offset": 938}, {"id": 1092601, "label": "ENT", "start_offset": 944, "end_offset": 961}, {"id": 1092602, "label": "ENT", "start_offset": 1000, "end_offset": 1023}, {"id": 1092603, "label": "ENT", "start_offset": 1035, "end_offset": 1044}, {"id": 1092604, "label": "ENT", "start_offset": 1076, "end_offset": 1087}, {"id": 1092605, "label": "ENT", "start_offset": 1093, "end_offset": 1098}, {"id": 1092606, "label": "ENT", "start_offset": 1147, "end_offset": 1156}, {"id": 1092607, "label": "ENT", "start_offset": 1187, "end_offset": 1196}, {"id": 1092608, "label": "ENT", "start_offset": 1209, "end_offset": 1223}, {"id": 1092609, "label": "ENT", "start_offset": 1229, "end_offset": 1249}, {"id": 1092610, "label": "ENT", "start_offset": 1273, "end_offset": 1292}, {"id": 1092611, "label": "ENT", "start_offset": 1297, "end_offset": 1302}], "relations": [{"id": 175817, "from_id": 1092596, "to_id": 1092597, "type": "COMPARE"}, {"id": 175818, "from_id": 1092596, "to_id": 1092598, "type": "COMPARE"}, {"id": 175819, "from_id": 1092597, "to_id": 1092598, "type": "CONJUNCTION"}, {"id": 175820, "from_id": 1092594, "to_id": 1092596, "type": "COREF"}, {"id": 175821, "from_id": 1092596, "to_id": 1092603, "type": "COREF"}, {"id": 175822, "from_id": 1092603, "to_id": 1092606, "type": "COREF"}, {"id": 175823, "from_id": 1092601, "to_id": 1092602, "type": "COMPARE"}, {"id": 175824, "from_id": 1092607, "to_id": 1092606, "type": "CONJUNCTION"}, {"id": 175825, "from_id": 1092607, "to_id": 1092608, "type": "USED-FOR"}, {"id": 175826, "from_id": 1092606, "to_id": 1092608, "type": "USED-FOR"}, {"id": 175827, "from_id": 1092609, "to_id": 1092606, "type": "USED-FOR"}, {"id": 175828, "from_id": 1092609, "to_id": 1092606, "type": "USED-FOR"}, {"id": 175829, "from_id": 1092609, "to_id": 1092607, "type": "USED-FOR"}, {"id": 175830, "from_id": 1092609, "to_id": 1092607, "type": "USED-FOR"}, {"id": 175831, "from_id": 1092610, "to_id": 1092611, "type": "USED-FOR"}, {"id": 175832, "from_id": 1092606, "to_id": 1092610, "type": "USED-FOR"}, {"id": 175833, "from_id": 1092611, "to_id": 1092595, "type": "COREF"}, {"id": 175834, "from_id": 1092605, "to_id": 1092611, "type": "COREF"}, {"id": 175835, "from_id": 1092595, "to_id": 1092594, "type": "USED-FOR"}, {"id": 175836, "from_id": 1092589, "to_id": 1092590, "type": "PART-OF"}]}
{"id": "N03-4010", "text": " The  JAVELIN system  integrates a flexible,  planning-based architecture  with a variety of  language processing modules  to provide an  open-domain question answering capability  on  free text  . The demonstration will focus on how  JAVELIN  processes  questions  and retrieves the most likely  answer candidates  from the given  text corpus  . The operation of the system will be explained in depth through browsing the  repository  of  data objects  created by the system during each  question answering session  . ", "Comments": [], "entities": [{"id": 1092612, "label": "ENT", "start_offset": 6, "end_offset": 20}, {"id": 1092613, "label": "ENT", "start_offset": 46, "end_offset": 73}, {"id": 1092614, "label": "ENT", "start_offset": 94, "end_offset": 121}, {"id": 1092615, "label": "ENT", "start_offset": 138, "end_offset": 179}, {"id": 1092616, "label": "ENT", "start_offset": 235, "end_offset": 242}, {"id": 1092617, "label": "ENT", "start_offset": 368, "end_offset": 374}, {"id": 1092618, "label": "ENT", "start_offset": 469, "end_offset": 475}, {"id": 1092619, "label": "ENT", "start_offset": 489, "end_offset": 507}], "relations": [{"id": 175837, "from_id": 1092613, "to_id": 1092612, "type": "PART-OF"}, {"id": 175838, "from_id": 1092614, "to_id": 1092612, "type": "PART-OF"}, {"id": 175839, "from_id": 1092614, "to_id": 1092613, "type": "CONJUNCTION"}, {"id": 175840, "from_id": 1092612, "to_id": 1092615, "type": "USED-FOR"}, {"id": 175841, "from_id": 1092616, "to_id": 1092612, "type": "COREF"}, {"id": 175842, "from_id": 1092617, "to_id": 1092616, "type": "COREF"}, {"id": 175843, "from_id": 1092617, "to_id": 1092618, "type": "COREF"}]}
{"id": "J97-1004", "text": " \"To explain complex phenomena, an  explanation system  must be able to select information from a formal representation of  domain knowledge , organize the selected information into  multisentential discourse plans , and realize the  discourse plans  in text. Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for  explanation , empirical results have been limited. This paper reports on a seven-year effort to empirically study  explanation generation  from  semantically rich, large-scale knowledge bases . In particular, it describes a  robust explanation system  that constructs  multisentential and multi-paragraph explanations  from the a  large-scale knowledge base  in the domain of botanical anatomy, physiology, and development. We introduce the evaluation methodology and describe how performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system. In this evaluation, scored within \"\"half a grade\"\" of domain experts, and its performance exceeded that of one of the domain experts.\" ", "Comments": [], "entities": [{"id": 1092624, "label": "ENT", "start_offset": 36, "end_offset": 54}, {"id": 1092625, "label": "ENT", "start_offset": 183, "end_offset": 214}, {"id": 1092626, "label": "ENT", "start_offset": 234, "end_offset": 249}, {"id": 1092627, "label": "ENT", "start_offset": 499, "end_offset": 521}, {"id": 1092628, "label": "ENT", "start_offset": 529, "end_offset": 575}, {"id": 1092629, "label": "ENT", "start_offset": 609, "end_offset": 634}, {"id": 1092630, "label": "ENT", "start_offset": 653, "end_offset": 701}, {"id": 1092631, "label": "ENT", "start_offset": 715, "end_offset": 741}, {"id": 1092632, "label": "ENT", "start_offset": 760, "end_offset": 777}, {"id": 1092633, "label": "ENT", "start_offset": 779, "end_offset": 789}, {"id": 1092634, "label": "ENT", "start_offset": 795, "end_offset": 806}, {"id": 1092635, "label": "ENT", "start_offset": 825, "end_offset": 847}, {"id": 1092636, "label": "ENT", "start_offset": 900, "end_offset": 911}, {"id": 1092637, "label": "ENT", "start_offset": 971, "end_offset": 989}, {"id": 1092638, "label": "ENT", "start_offset": 999, "end_offset": 1009}], "relations": [{"id": 175847, "from_id": 1092628, "to_id": 1092627, "type": "USED-FOR"}, {"id": 175848, "from_id": 1092631, "to_id": 1092629, "type": "USED-FOR"}, {"id": 175849, "from_id": 1092624, "to_id": 1092626, "type": "USED-FOR"}, {"id": 175850, "from_id": 1092629, "to_id": 1092624, "type": "COREF"}, {"id": 175851, "from_id": 1092629, "to_id": 1092630, "type": "USED-FOR"}, {"id": 175852, "from_id": 1092632, "to_id": 1092633, "type": "CONJUNCTION"}, {"id": 175853, "from_id": 1092633, "to_id": 1092634, "type": "CONJUNCTION"}, {"id": 175854, "from_id": 1092636, "to_id": 1092635, "type": "COREF"}, {"id": 175855, "from_id": 1092637, "to_id": 1092629, "type": "COREF"}, {"id": 175856, "from_id": 1092636, "to_id": 1092637, "type": "EVALUATE-FOR"}, {"id": 175857, "from_id": 1092638, "to_id": 1092636, "type": "COREF"}, {"id": 175858, "from_id": 1092629, "to_id": 1092624, "type": "COREF"}, {"id": 175859, "from_id": 1092632, "to_id": 1092631, "type": "FEATURE-OF"}, {"id": 175860, "from_id": 1092633, "to_id": 1092631, "type": "FEATURE-OF"}, {"id": 175861, "from_id": 1092634, "to_id": 1092631, "type": "FEATURE-OF"}]}
{"id": "E95-1036", "text": " This paper presents an analysis of  temporal anaphora  in  sentences  which contain  quantification over events , within the framework of  Discourse Representation Theory . The analysis in (Partee, 1984) of  quantified sentences , introduced by a  temporal connective , gives the wrong  truth-conditions  when the  temporal connective  in the  subordinate clause  is before or after. This  problem  has been previously analyzed in (de Swart, 1991) as an instance of the  proportion problem  and given a solution from a  Generalized Quantifier approach . By using a careful distinction between the different notions of  reference time  based on (Kamp and Reyle, 1993), we propose a solution to this  problem , within the framework of  DRT . We show some applications of this  solution  to additional  temporal anaphora phenomena  in  quantified sentences . ", "Comments": [], "entities": [{"id": 1092639, "label": "ENT", "start_offset": 37, "end_offset": 54}, {"id": 1092640, "label": "ENT", "start_offset": 86, "end_offset": 112}, {"id": 1092641, "label": "ENT", "start_offset": 140, "end_offset": 171}, {"id": 1092642, "label": "ENT", "start_offset": 209, "end_offset": 229}, {"id": 1092643, "label": "ENT", "start_offset": 249, "end_offset": 268}, {"id": 1092644, "label": "ENT", "start_offset": 316, "end_offset": 335}, {"id": 1092645, "label": "ENT", "start_offset": 345, "end_offset": 363}, {"id": 1092646, "label": "ENT", "start_offset": 391, "end_offset": 398}, {"id": 1092647, "label": "ENT", "start_offset": 472, "end_offset": 490}, {"id": 1092648, "label": "ENT", "start_offset": 504, "end_offset": 512}, {"id": 1092649, "label": "ENT", "start_offset": 521, "end_offset": 552}, {"id": 1092650, "label": "ENT", "start_offset": 620, "end_offset": 634}, {"id": 1092651, "label": "ENT", "start_offset": 682, "end_offset": 690}, {"id": 1092652, "label": "ENT", "start_offset": 700, "end_offset": 707}, {"id": 1092653, "label": "ENT", "start_offset": 735, "end_offset": 738}, {"id": 1092654, "label": "ENT", "start_offset": 776, "end_offset": 784}, {"id": 1092655, "label": "ENT", "start_offset": 801, "end_offset": 828}, {"id": 1092656, "label": "ENT", "start_offset": 834, "end_offset": 854}], "relations": [{"id": 175862, "from_id": 1092653, "to_id": 1092652, "type": "USED-FOR"}, {"id": 175863, "from_id": 1092654, "to_id": 1092655, "type": "USED-FOR"}, {"id": 175864, "from_id": 1092641, "to_id": 1092639, "type": "USED-FOR"}, {"id": 175865, "from_id": 1092640, "to_id": 1092639, "type": "PART-OF"}, {"id": 175866, "from_id": 1092649, "to_id": 1092646, "type": "USED-FOR"}, {"id": 175867, "from_id": 1092646, "to_id": 1092639, "type": "COREF"}, {"id": 175868, "from_id": 1092652, "to_id": 1092646, "type": "COREF"}, {"id": 175869, "from_id": 1092649, "to_id": 1092648, "type": "COREF"}, {"id": 175870, "from_id": 1092654, "to_id": 1092651, "type": "COREF"}, {"id": 175871, "from_id": 1092656, "to_id": 1092654, "type": "USED-FOR"}, {"id": 175872, "from_id": 1092651, "to_id": 1092652, "type": "USED-FOR"}, {"id": 175873, "from_id": 1092641, "to_id": 1092653, "type": "COREF"}, {"id": 175874, "from_id": 1092644, "to_id": 1092645, "type": "PART-OF"}]}
{"id": "CVPR_2006_21_abs", "text": "The introduction of prior knowledge has greatly enhanced numerous purely low-level driven image processing algorithms. In this work, we focus on the problem of non-rigid image registration. A number of powerful registration criteria have been developed in the last decade, most prominently the criterion of maximum mutual information. Although this criterion provides for good registration results in many applications, it remains a purely low-level criterion. As a consequence, registration results will deteriorate once this low-level information is corrupted, due to noise, partial occlusions or missing image structure. In this paper , we will develop a Bayesian framework that allows to impose statistically learned prior knowledge about the joint intensity distribution into image registration methods. The prior is given by a kernel density estimate on the space of joint intensity distributions computed from a representative set of pre-registered image pairs. This nonparametric prior accurately models previously learned intensity relations between various image modalities and slice locations. Experimental results demonstrate that the resulting registration process is more robust to missing low-level information as it favors intensity correspondences statistically consistent with the learned intensity distributions.", "Comments": [], "entities": [{"id": 1092703, "label": "ENT", "start_offset": 20, "end_offset": 35}, {"id": 1092704, "label": "ENT", "start_offset": 73, "end_offset": 117}, {"id": 1092705, "label": "ENT", "start_offset": 160, "end_offset": 188}, {"id": 1092706, "label": "ENT", "start_offset": 211, "end_offset": 232}, {"id": 1092707, "label": "ENT", "start_offset": 307, "end_offset": 333}, {"id": 1092708, "label": "ENT", "start_offset": 349, "end_offset": 358}, {"id": 1092709, "label": "ENT", "start_offset": 420, "end_offset": 422}, {"id": 1092710, "label": "ENT", "start_offset": 440, "end_offset": 459}, {"id": 1092711, "label": "ENT", "start_offset": 479, "end_offset": 491}, {"id": 1092712, "label": "ENT", "start_offset": 527, "end_offset": 548}, {"id": 1092713, "label": "ENT", "start_offset": 570, "end_offset": 575}, {"id": 1092714, "label": "ENT", "start_offset": 577, "end_offset": 595}, {"id": 1092715, "label": "ENT", "start_offset": 599, "end_offset": 622}, {"id": 1092716, "label": "ENT", "start_offset": 658, "end_offset": 676}, {"id": 1092717, "label": "ENT", "start_offset": 699, "end_offset": 736}, {"id": 1092718, "label": "ENT", "start_offset": 747, "end_offset": 775}, {"id": 1092719, "label": "ENT", "start_offset": 781, "end_offset": 807}, {"id": 1092720, "label": "ENT", "start_offset": 813, "end_offset": 818}, {"id": 1092721, "label": "ENT", "start_offset": 833, "end_offset": 856}, {"id": 1092722, "label": "ENT", "start_offset": 873, "end_offset": 902}, {"id": 1092723, "label": "ENT", "start_offset": 941, "end_offset": 967}, {"id": 1092724, "label": "ENT", "start_offset": 1031, "end_offset": 1050}, {"id": 1092725, "label": "ENT", "start_offset": 1067, "end_offset": 1083}, {"id": 1092726, "label": "ENT", "start_offset": 1088, "end_offset": 1103}, {"id": 1092727, "label": "ENT", "start_offset": 1157, "end_offset": 1177}, {"id": 1092728, "label": "ENT", "start_offset": 1196, "end_offset": 1225}, {"id": 1092729, "label": "ENT", "start_offset": 1229, "end_offset": 1231}, {"id": 1092730, "label": "ENT", "start_offset": 1239, "end_offset": 1264}, {"id": 1092731, "label": "ENT", "start_offset": 1307, "end_offset": 1330}], "relations": [{"id": 175921, "from_id": 1092707, "to_id": 1092706, "type": "HYPONYM-OF"}, {"id": 175922, "from_id": 1092708, "to_id": 1092707, "type": "COREF"}, {"id": 175923, "from_id": 1092708, "to_id": 1092709, "type": "COREF"}, {"id": 175924, "from_id": 1092710, "to_id": 1092709, "type": "FEATURE-OF"}, {"id": 175925, "from_id": 1092705, "to_id": 1092711, "type": "COREF"}, {"id": 175926, "from_id": 1092718, "to_id": 1092717, "type": "FEATURE-OF"}, {"id": 175927, "from_id": 1092716, "to_id": 1092719, "type": "USED-FOR"}, {"id": 175928, "from_id": 1092717, "to_id": 1092719, "type": "USED-FOR"}, {"id": 175929, "from_id": 1092720, "to_id": 1092717, "type": "COREF"}, {"id": 175930, "from_id": 1092721, "to_id": 1092720, "type": "USED-FOR"}, {"id": 175931, "from_id": 1092721, "to_id": 1092722, "type": "USED-FOR"}, {"id": 175932, "from_id": 1092723, "to_id": 1092722, "type": "USED-FOR"}, {"id": 175933, "from_id": 1092719, "to_id": 1092727, "type": "COREF"}, {"id": 175934, "from_id": 1092727, "to_id": 1092728, "type": "USED-FOR"}, {"id": 175935, "from_id": 1092727, "to_id": 1092729, "type": "COREF"}, {"id": 175936, "from_id": 1092729, "to_id": 1092730, "type": "USED-FOR"}]}
{"id": "E93-1043", "text": " In this paper a  morphological component  with a limited capability to automatically interpret (and generate)  derived words  is presented. The system combines an extended  two-level morphology  [Trost, 1991a; Trost, 1991b] with a  feature-based word grammar  building on a  hierarchical lexicon .  Polymorphemic stems  not explicitly stored in the  lexicon  are given a  compositional interpretation . That way the system allows to minimize redundancy in the  lexicon  because  derived words  that are transparent need not to be stored explicitly. Also,  words formed ad-hoc  can be recognized correctly. The system is implemented in CommonLisp and has been tested on examples from  German derivation . ", "Comments": [], "entities": [{"id": 1092732, "label": "ENT", "start_offset": 18, "end_offset": 41}, {"id": 1092733, "label": "ENT", "start_offset": 112, "end_offset": 125}, {"id": 1092734, "label": "ENT", "start_offset": 145, "end_offset": 151}, {"id": 1092735, "label": "ENT", "start_offset": 174, "end_offset": 194}, {"id": 1092736, "label": "ENT", "start_offset": 233, "end_offset": 259}, {"id": 1092737, "label": "ENT", "start_offset": 276, "end_offset": 296}, {"id": 1092738, "label": "ENT", "start_offset": 300, "end_offset": 319}, {"id": 1092739, "label": "ENT", "start_offset": 373, "end_offset": 401}, {"id": 1092740, "label": "ENT", "start_offset": 417, "end_offset": 423}, {"id": 1092741, "label": "ENT", "start_offset": 480, "end_offset": 493}, {"id": 1092742, "label": "ENT", "start_offset": 557, "end_offset": 576}, {"id": 1092743, "label": "ENT", "start_offset": 611, "end_offset": 617}, {"id": 1092744, "label": "ENT", "start_offset": 636, "end_offset": 646}, {"id": 1092745, "label": "ENT", "start_offset": 685, "end_offset": 702}], "relations": [{"id": 175937, "from_id": 1092732, "to_id": 1092733, "type": "USED-FOR"}, {"id": 175938, "from_id": 1092737, "to_id": 1092736, "type": "USED-FOR"}, {"id": 175939, "from_id": 1092739, "to_id": 1092738, "type": "FEATURE-OF"}, {"id": 175940, "from_id": 1092735, "to_id": 1092736, "type": "CONJUNCTION"}, {"id": 175941, "from_id": 1092735, "to_id": 1092734, "type": "USED-FOR"}, {"id": 175942, "from_id": 1092734, "to_id": 1092732, "type": "COREF"}, {"id": 175943, "from_id": 1092741, "to_id": 1092733, "type": "COREF"}, {"id": 175944, "from_id": 1092740, "to_id": 1092734, "type": "COREF"}, {"id": 175945, "from_id": 1092743, "to_id": 1092740, "type": "COREF"}, {"id": 175946, "from_id": 1092745, "to_id": 1092743, "type": "EVALUATE-FOR"}, {"id": 175947, "from_id": 1092744, "to_id": 1092743, "type": "USED-FOR"}]}
{"id": "C86-1021", "text": " The  interlingual approach to MT  has been repeatedly advocated by researchers originally interested in  natural language understanding  who take  machine translation  to be one possible application. However, not only the  ambiguity  but also the vagueness which every  natural language  inevitably has leads this approach into essential difficulties. In contrast, our project, the  Mu-project , adopts the  transfer approach  as the basic framework of  MT . This paper describes the detailed construction of the  transfer phase  of our system from  Japanese  to  English , and gives some examples of problems which seem difficult to treat in the  interlingual approach . The basic design principles of the  transfer phase  of our system have already been mentioned in (1) (2). Some of the principles which are relevant to the topic of this paper are: (a)  Multiple Layer of Grammars  (b)  Multiple Layer Presentation  (c)  Lexicon Driven Processing  (d)  Form-Oriented Dictionary Description . This paper also shows how these principles are realized in the current system. ", "Comments": [], "entities": [{"id": 1092746, "label": "ENT", "start_offset": 6, "end_offset": 27}, {"id": 1092747, "label": "ENT", "start_offset": 31, "end_offset": 33}, {"id": 1092748, "label": "ENT", "start_offset": 106, "end_offset": 136}, {"id": 1092749, "label": "ENT", "start_offset": 148, "end_offset": 167}, {"id": 1092750, "label": "ENT", "start_offset": 384, "end_offset": 394}, {"id": 1092751, "label": "ENT", "start_offset": 409, "end_offset": 426}, {"id": 1092752, "label": "ENT", "start_offset": 455, "end_offset": 457}, {"id": 1092753, "label": "ENT", "start_offset": 515, "end_offset": 529}, {"id": 1092754, "label": "ENT", "start_offset": 538, "end_offset": 544}, {"id": 1092755, "label": "ENT", "start_offset": 551, "end_offset": 559}, {"id": 1092756, "label": "ENT", "start_offset": 565, "end_offset": 572}, {"id": 1092757, "label": "ENT", "start_offset": 649, "end_offset": 670}, {"id": 1092758, "label": "ENT", "start_offset": 709, "end_offset": 723}, {"id": 1092759, "label": "ENT", "start_offset": 732, "end_offset": 738}, {"id": 1092760, "label": "ENT", "start_offset": 791, "end_offset": 801}, {"id": 1092761, "label": "ENT", "start_offset": 858, "end_offset": 884}, {"id": 1092762, "label": "ENT", "start_offset": 891, "end_offset": 918}, {"id": 1092763, "label": "ENT", "start_offset": 925, "end_offset": 950}, {"id": 1092764, "label": "ENT", "start_offset": 957, "end_offset": 993}, {"id": 1092765, "label": "ENT", "start_offset": 1028, "end_offset": 1038}, {"id": 1092766, "label": "ENT", "start_offset": 1067, "end_offset": 1073}], "relations": [{"id": 175948, "from_id": 1092746, "to_id": 1092747, "type": "USED-FOR"}, {"id": 175949, "from_id": 1092749, "to_id": 1092747, "type": "COREF"}, {"id": 175950, "from_id": 1092750, "to_id": 1092752, "type": "USED-FOR"}, {"id": 175951, "from_id": 1092753, "to_id": 1092754, "type": "PART-OF"}, {"id": 175952, "from_id": 1092754, "to_id": 1092750, "type": "COREF"}, {"id": 175953, "from_id": 1092751, "to_id": 1092750, "type": "USED-FOR"}, {"id": 175954, "from_id": 1092758, "to_id": 1092753, "type": "COREF"}, {"id": 175955, "from_id": 1092759, "to_id": 1092754, "type": "COREF"}, {"id": 175956, "from_id": 1092758, "to_id": 1092759, "type": "PART-OF"}, {"id": 175957, "from_id": 1092761, "to_id": 1092760, "type": "PART-OF"}, {"id": 175958, "from_id": 1092762, "to_id": 1092760, "type": "PART-OF"}, {"id": 175959, "from_id": 1092763, "to_id": 1092760, "type": "PART-OF"}, {"id": 175960, "from_id": 1092764, "to_id": 1092760, "type": "PART-OF"}, {"id": 175961, "from_id": 1092766, "to_id": 1092759, "type": "COREF"}, {"id": 175962, "from_id": 1092765, "to_id": 1092760, "type": "COREF"}, {"id": 175963, "from_id": 1092765, "to_id": 1092766, "type": "PART-OF"}, {"id": 175964, "from_id": 1092753, "to_id": 1092751, "type": "COREF"}, {"id": 175965, "from_id": 1092757, "to_id": 1092746, "type": "COREF"}, {"id": 175966, "from_id": 1092761, "to_id": 1092762, "type": "CONJUNCTION"}, {"id": 175967, "from_id": 1092762, "to_id": 1092763, "type": "CONJUNCTION"}, {"id": 175968, "from_id": 1092763, "to_id": 1092764, "type": "CONJUNCTION"}]}
{"id": "P04-2005", "text": " We present a novel approach for automatically acquiring  English topic signatures . Given a particular  concept , or  word sense , a  topic signature  is a set of  words  that tend to co-occur with it.  Topic signatures  can be useful in a number of  Natural Language Processing (NLP) applications , such as  Word Sense Disambiguation (WSD)  and  Text Summarisation . Our method takes advantage of the different way in which  word senses  are lexicalised in  English  and  Chinese , and also exploits the large amount of  Chinese text  available in  corpora  and on the Web. We evaluated the  topic signatures  on a  WSD task , where we trained a  second-order vector cooccurrence algorithm  on  standard WSD datasets , with promising results. ", "Comments": [], "entities": [{"id": 1092791, "label": "ENT", "start_offset": 20, "end_offset": 28}, {"id": 1092792, "label": "ENT", "start_offset": 33, "end_offset": 82}, {"id": 1092793, "label": "ENT", "start_offset": 105, "end_offset": 112}, {"id": 1092794, "label": "ENT", "start_offset": 119, "end_offset": 129}, {"id": 1092795, "label": "ENT", "start_offset": 135, "end_offset": 150}, {"id": 1092796, "label": "ENT", "start_offset": 204, "end_offset": 220}, {"id": 1092797, "label": "ENT", "start_offset": 252, "end_offset": 298}, {"id": 1092798, "label": "ENT", "start_offset": 310, "end_offset": 341}, {"id": 1092799, "label": "ENT", "start_offset": 348, "end_offset": 366}, {"id": 1092800, "label": "ENT", "start_offset": 373, "end_offset": 379}, {"id": 1092801, "label": "ENT", "start_offset": 427, "end_offset": 438}, {"id": 1092802, "label": "ENT", "start_offset": 460, "end_offset": 467}, {"id": 1092803, "label": "ENT", "start_offset": 474, "end_offset": 481}, {"id": 1092804, "label": "ENT", "start_offset": 523, "end_offset": 535}, {"id": 1092805, "label": "ENT", "start_offset": 551, "end_offset": 558}, {"id": 1092806, "label": "ENT", "start_offset": 571, "end_offset": 574}, {"id": 1092807, "label": "ENT", "start_offset": 594, "end_offset": 610}, {"id": 1092808, "label": "ENT", "start_offset": 618, "end_offset": 626}, {"id": 1092809, "label": "ENT", "start_offset": 649, "end_offset": 691}, {"id": 1092810, "label": "ENT", "start_offset": 706, "end_offset": 718}], "relations": [{"id": 175993, "from_id": 1092796, "to_id": 1092797, "type": "USED-FOR"}, {"id": 175994, "from_id": 1092804, "to_id": 1092805, "type": "PART-OF"}, {"id": 175995, "from_id": 1092808, "to_id": 1092807, "type": "EVALUATE-FOR"}, {"id": 175996, "from_id": 1092791, "to_id": 1092792, "type": "USED-FOR"}, {"id": 175997, "from_id": 1092796, "to_id": 1092798, "type": "USED-FOR"}, {"id": 175998, "from_id": 1092796, "to_id": 1092799, "type": "USED-FOR"}, {"id": 175999, "from_id": 1092798, "to_id": 1092799, "type": "CONJUNCTION"}, {"id": 176000, "from_id": 1092791, "to_id": 1092800, "type": "COREF"}, {"id": 176001, "from_id": 1092804, "to_id": 1092806, "type": "PART-OF"}, {"id": 176002, "from_id": 1092805, "to_id": 1092806, "type": "CONJUNCTION"}, {"id": 176003, "from_id": 1092810, "to_id": 1092809, "type": "USED-FOR"}, {"id": 176004, "from_id": 1092798, "to_id": 1092808, "type": "COREF"}, {"id": 176005, "from_id": 1092798, "to_id": 1092797, "type": "HYPONYM-OF"}, {"id": 176006, "from_id": 1092799, "to_id": 1092797, "type": "HYPONYM-OF"}]}
{"id": "P03-1031", "text": " This paper concerns the  discourse understanding process  in  spoken dialogue systems  . This process enables the system to understand  user utterances  based on the  context  of a  dialogue  . Since multiple  candidates  for the  understanding  result can be obtained for a  user utterance  due to the  ambiguity  of  speech understanding  , it is not appropriate to decide on a single  understanding  result after each  user utterance  . By holding multiple  candidates  for  understanding  results and resolving the  ambiguity  as the  dialogue  progresses, the  discourse understanding accuracy  can be improved. This paper proposes a method for resolving this  ambiguity  based on  statistical information  obtained from  dialogue corpora  . Unlike conventional methods that use  hand-crafted rules  , the proposed method enables easy design of the  discourse understanding process  . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple  candidates  for  understanding  results is effective. ", "Comments": [], "entities": [{"id": 1092811, "label": "ENT", "start_offset": 26, "end_offset": 57}, {"id": 1092812, "label": "ENT", "start_offset": 63, "end_offset": 86}, {"id": 1092813, "label": "ENT", "start_offset": 115, "end_offset": 121}, {"id": 1092814, "label": "ENT", "start_offset": 137, "end_offset": 152}, {"id": 1092815, "label": "ENT", "start_offset": 277, "end_offset": 291}, {"id": 1092816, "label": "ENT", "start_offset": 305, "end_offset": 340}, {"id": 1092817, "label": "ENT", "start_offset": 423, "end_offset": 437}, {"id": 1092818, "label": "ENT", "start_offset": 521, "end_offset": 530}, {"id": 1092819, "label": "ENT", "start_offset": 567, "end_offset": 599}, {"id": 1092820, "label": "ENT", "start_offset": 640, "end_offset": 646}, {"id": 1092821, "label": "ENT", "start_offset": 667, "end_offset": 676}, {"id": 1092822, "label": "ENT", "start_offset": 688, "end_offset": 711}, {"id": 1092823, "label": "ENT", "start_offset": 728, "end_offset": 744}, {"id": 1092824, "label": "ENT", "start_offset": 768, "end_offset": 775}, {"id": 1092825, "label": "ENT", "start_offset": 786, "end_offset": 804}, {"id": 1092826, "label": "ENT", "start_offset": 821, "end_offset": 827}, {"id": 1092827, "label": "ENT", "start_offset": 856, "end_offset": 887}, {"id": 1092828, "label": "ENT", "start_offset": 928, "end_offset": 934}, {"id": 1092829, "label": "ENT", "start_offset": 962, "end_offset": 968}], "relations": [{"id": 176007, "from_id": 1092811, "to_id": 1092812, "type": "USED-FOR"}, {"id": 176008, "from_id": 1092823, "to_id": 1092822, "type": "USED-FOR"}, {"id": 176009, "from_id": 1092812, "to_id": 1092813, "type": "COREF"}, {"id": 176010, "from_id": 1092813, "to_id": 1092814, "type": "USED-FOR"}, {"id": 176011, "from_id": 1092816, "to_id": 1092818, "type": "COREF"}, {"id": 176012, "from_id": 1092818, "to_id": 1092821, "type": "COREF"}, {"id": 176013, "from_id": 1092820, "to_id": 1092821, "type": "USED-FOR"}, {"id": 176014, "from_id": 1092822, "to_id": 1092820, "type": "USED-FOR"}, {"id": 176015, "from_id": 1092825, "to_id": 1092824, "type": "USED-FOR"}, {"id": 176016, "from_id": 1092820, "to_id": 1092826, "type": "COREF"}, {"id": 176017, "from_id": 1092811, "to_id": 1092827, "type": "COREF"}, {"id": 176018, "from_id": 1092826, "to_id": 1092829, "type": "COREF"}, {"id": 176019, "from_id": 1092828, "to_id": 1092829, "type": "USED-FOR"}]}
{"id": "P06-1013", "text": "  Combination methods  are an effective way of improving  system performance  . This paper examines the benefits of  system combination  for  unsupervised WSD  . We investigate several  voting- and arbiter-based combination strategies  over a diverse pool of  unsupervised WSD systems  . Our  combination methods  rely on  predominant senses  which are derived automatically from  raw text  . Experiments using the  SemCor  and  Senseval-3 data sets  demonstrate that our ensembles yield significantly better results when compared with state-of-the-art. ", "Comments": [], "entities": [{"id": 1092847, "label": "ENT", "start_offset": 2, "end_offset": 21}, {"id": 1092848, "label": "ENT", "start_offset": 117, "end_offset": 135}, {"id": 1092849, "label": "ENT", "start_offset": 142, "end_offset": 158}, {"id": 1092850, "label": "ENT", "start_offset": 186, "end_offset": 234}, {"id": 1092851, "label": "ENT", "start_offset": 260, "end_offset": 284}, {"id": 1092852, "label": "ENT", "start_offset": 293, "end_offset": 312}, {"id": 1092853, "label": "ENT", "start_offset": 323, "end_offset": 341}, {"id": 1092854, "label": "ENT", "start_offset": 381, "end_offset": 389}, {"id": 1092855, "label": "ENT", "start_offset": 416, "end_offset": 449}, {"id": 1092856, "label": "ENT", "start_offset": 472, "end_offset": 481}, {"id": 1092857, "label": "ENT", "start_offset": 536, "end_offset": 552}], "relations": [{"id": 176037, "from_id": 1092853, "to_id": 1092852, "type": "USED-FOR"}, {"id": 176038, "from_id": 1092848, "to_id": 1092849, "type": "USED-FOR"}, {"id": 176039, "from_id": 1092850, "to_id": 1092851, "type": "USED-FOR"}, {"id": 176040, "from_id": 1092854, "to_id": 1092853, "type": "USED-FOR"}, {"id": 176041, "from_id": 1092852, "to_id": 1092856, "type": "COREF"}, {"id": 176042, "from_id": 1092856, "to_id": 1092857, "type": "COREF"}, {"id": 176043, "from_id": 1092855, "to_id": 1092856, "type": "EVALUATE-FOR"}, {"id": 176044, "from_id": 1092855, "to_id": 1092857, "type": "EVALUATE-FOR"}]}
{"id": "H01-1040", "text": "   In this paper we show how two standard outputs from  information extraction (IE) systems  -  named entity annotations  and  scenario templates  - can be used to enhance access to  text collections  via a standard  text browser  . We describe how this information is used in a  prototype system  designed to support  information workers  ' access to a  pharmaceutical news archive  as part of their  industry watch  function. We also report results of a preliminary,  qualitative user evaluation  of the system, which while broadly positive indicates further work needs to be done on the  interface  to make  users  aware of the increased potential of  IE-enhanced text browsers  . ", "Comments": [], "entities": [{"id": 1092874, "label": "ENT", "start_offset": 42, "end_offset": 49}, {"id": 1092875, "label": "ENT", "start_offset": 56, "end_offset": 91}, {"id": 1092876, "label": "ENT", "start_offset": 96, "end_offset": 120}, {"id": 1092877, "label": "ENT", "start_offset": 127, "end_offset": 145}, {"id": 1092878, "label": "ENT", "start_offset": 183, "end_offset": 199}, {"id": 1092879, "label": "ENT", "start_offset": 217, "end_offset": 229}, {"id": 1092880, "label": "ENT", "start_offset": 280, "end_offset": 296}, {"id": 1092881, "label": "ENT", "start_offset": 355, "end_offset": 382}, {"id": 1092882, "label": "ENT", "start_offset": 470, "end_offset": 497}, {"id": 1092883, "label": "ENT", "start_offset": 506, "end_offset": 512}, {"id": 1092884, "label": "ENT", "start_offset": 655, "end_offset": 680}], "relations": [{"id": 176056, "from_id": 1092876, "to_id": 1092874, "type": "HYPONYM-OF"}, {"id": 176057, "from_id": 1092877, "to_id": 1092874, "type": "HYPONYM-OF"}, {"id": 176058, "from_id": 1092876, "to_id": 1092877, "type": "CONJUNCTION"}, {"id": 176059, "from_id": 1092879, "to_id": 1092878, "type": "USED-FOR"}, {"id": 176060, "from_id": 1092880, "to_id": 1092881, "type": "USED-FOR"}, {"id": 176061, "from_id": 1092883, "to_id": 1092880, "type": "COREF"}, {"id": 176062, "from_id": 1092882, "to_id": 1092883, "type": "EVALUATE-FOR"}, {"id": 176063, "from_id": 1092884, "to_id": 1092883, "type": "COREF"}, {"id": 176064, "from_id": 1092874, "to_id": 1092878, "type": "USED-FOR"}]}
{"id": "IJCAI_2016_412_abs", "text": "Along with the increasing requirements, the hash-tag recommendation task for microblogs has been receiving considerable attention in recent years. Various researchers have studied the problem from different aspects. However, most of these methods usually need handcrafted features. Motivated by the successful use of convolutional neural networks (CNNs) for many natural language processing tasks, in this paper, we adopt CNNs to perform the hashtag recommendation problem. To incorporate the trigger words whose effectiveness have been experimentally evaluated in several previous works, we propose a novel architecture with an attention mechanism. The results of experiments on the data collected from a real world microblogging service demonstrated that the proposed model outperforms state-of-the-art methods. By incorporating trigger words into the consideration, the relative improvement of the proposed method over the state-of-the-art method is around 9.4% in the F1-score.", "Comments": [], "entities": [{"id": 1092933, "label": "ENT", "start_offset": 44, "end_offset": 72}, {"id": 1092934, "label": "ENT", "start_offset": 77, "end_offset": 87}, {"id": 1092935, "label": "ENT", "start_offset": 260, "end_offset": 280}, {"id": 1092936, "label": "ENT", "start_offset": 317, "end_offset": 353}, {"id": 1092937, "label": "ENT", "start_offset": 363, "end_offset": 396}, {"id": 1092938, "label": "ENT", "start_offset": 422, "end_offset": 426}, {"id": 1092939, "label": "ENT", "start_offset": 442, "end_offset": 472}, {"id": 1092940, "label": "ENT", "start_offset": 493, "end_offset": 506}, {"id": 1092941, "label": "ENT", "start_offset": 608, "end_offset": 620}, {"id": 1092942, "label": "ENT", "start_offset": 629, "end_offset": 648}, {"id": 1092943, "label": "ENT", "start_offset": 684, "end_offset": 688}, {"id": 1092944, "label": "ENT", "start_offset": 770, "end_offset": 775}, {"id": 1092945, "label": "ENT", "start_offset": 788, "end_offset": 812}, {"id": 1092946, "label": "ENT", "start_offset": 831, "end_offset": 844}, {"id": 1092947, "label": "ENT", "start_offset": 910, "end_offset": 916}, {"id": 1092948, "label": "ENT", "start_offset": 926, "end_offset": 949}, {"id": 1092949, "label": "ENT", "start_offset": 972, "end_offset": 980}], "relations": [{"id": 176109, "from_id": 1092933, "to_id": 1092934, "type": "USED-FOR"}, {"id": 176110, "from_id": 1092949, "to_id": 1092948, "type": "EVALUATE-FOR"}, {"id": 176111, "from_id": 1092948, "to_id": 1092938, "type": "COREF"}, {"id": 176112, "from_id": 1092936, "to_id": 1092937, "type": "USED-FOR"}, {"id": 176113, "from_id": 1092938, "to_id": 1092936, "type": "COREF"}, {"id": 176114, "from_id": 1092942, "to_id": 1092941, "type": "FEATURE-OF"}, {"id": 176115, "from_id": 1092941, "to_id": 1092940, "type": "USED-FOR"}, {"id": 176116, "from_id": 1092941, "to_id": 1092944, "type": "COREF"}, {"id": 176117, "from_id": 1092944, "to_id": 1092945, "type": "COMPARE"}, {"id": 176118, "from_id": 1092939, "to_id": 1092933, "type": "COREF"}, {"id": 176119, "from_id": 1092947, "to_id": 1092944, "type": "COREF"}, {"id": 176120, "from_id": 1092947, "to_id": 1092948, "type": "COMPARE"}, {"id": 176121, "from_id": 1092943, "to_id": 1092944, "type": "EVALUATE-FOR"}]}
{"id": "CVPR_2003_30_abs", "text": "This paper presents a novel representation for three-dimensional objects in terms of affine-invariant image patches and their spatial relationships. Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction, allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint. The proposed approach does not require a separate segmentation stage and is applicable to cluttered scenes. Preliminary modeling and recognition results are presented.", "Comments": [], "entities": [{"id": 1092950, "label": "ENT", "start_offset": 28, "end_offset": 42}, {"id": 1092951, "label": "ENT", "start_offset": 47, "end_offset": 72}, {"id": 1092952, "label": "ENT", "start_offset": 85, "end_offset": 115}, {"id": 1092953, "label": "ENT", "start_offset": 126, "end_offset": 147}, {"id": 1092954, "label": "ENT", "start_offset": 149, "end_offset": 171}, {"id": 1092955, "label": "ENT", "start_offset": 226, "end_offset": 251}, {"id": 1092956, "label": "ENT", "start_offset": 281, "end_offset": 289}, {"id": 1092957, "label": "ENT", "start_offset": 294, "end_offset": 308}, {"id": 1092958, "label": "ENT", "start_offset": 323, "end_offset": 388}, {"id": 1092959, "label": "ENT", "start_offset": 403, "end_offset": 409}, {"id": 1092960, "label": "ENT", "start_offset": 503, "end_offset": 511}, {"id": 1092961, "label": "ENT", "start_offset": 540, "end_offset": 558}, {"id": 1092962, "label": "ENT", "start_offset": 580, "end_offset": 596}, {"id": 1092963, "label": "ENT", "start_offset": 623, "end_offset": 634}], "relations": [{"id": 176122, "from_id": 1092956, "to_id": 1092957, "type": "CONJUNCTION"}, {"id": 176123, "from_id": 1092955, "to_id": 1092956, "type": "USED-FOR"}, {"id": 176124, "from_id": 1092955, "to_id": 1092957, "type": "USED-FOR"}, {"id": 176125, "from_id": 1092954, "to_id": 1092955, "type": "CONJUNCTION"}, {"id": 176126, "from_id": 1092954, "to_id": 1092956, "type": "USED-FOR"}, {"id": 176127, "from_id": 1092954, "to_id": 1092957, "type": "USED-FOR"}, {"id": 176128, "from_id": 1092950, "to_id": 1092951, "type": "USED-FOR"}, {"id": 176129, "from_id": 1092952, "to_id": 1092951, "type": "FEATURE-OF"}, {"id": 176130, "from_id": 1092953, "to_id": 1092952, "type": "FEATURE-OF"}, {"id": 176131, "from_id": 1092959, "to_id": 1092958, "type": "USED-FOR"}, {"id": 176132, "from_id": 1092950, "to_id": 1092960, "type": "COREF"}, {"id": 176133, "from_id": 1092960, "to_id": 1092962, "type": "USED-FOR"}]}
{"id": "C04-1080", "text": " We present a new HMM tagger that exploits  context  on both sides of a  word  to be tagged, and evaluate it in both the  unsupervised and supervised case . Along the way, we present the first comprehensive comparison of  unsupervised methods for part-of-speech tagging , noting that published results to date have not been comparable across  corpora  or  lexicons . Observing that the  quality  of the  lexicon  greatly impacts the  accuracy  that can be achieved by the  algorithms , we present a method of  HMM training  that improves  accuracy  when  training  of  lexical probabilities  is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a  supervised, non-training intensive framework . ", "Comments": [], "entities": [{"id": 1092964, "label": "ENT", "start_offset": 18, "end_offset": 28}, {"id": 1092965, "label": "ENT", "start_offset": 106, "end_offset": 108}, {"id": 1092966, "label": "ENT", "start_offset": 122, "end_offset": 154}, {"id": 1092967, "label": "ENT", "start_offset": 222, "end_offset": 242}, {"id": 1092968, "label": "ENT", "start_offset": 247, "end_offset": 269}, {"id": 1092969, "label": "ENT", "start_offset": 434, "end_offset": 442}, {"id": 1092970, "label": "ENT", "start_offset": 473, "end_offset": 483}, {"id": 1092971, "label": "ENT", "start_offset": 510, "end_offset": 522}, {"id": 1092972, "label": "ENT", "start_offset": 569, "end_offset": 590}, {"id": 1092973, "label": "ENT", "start_offset": 635, "end_offset": 641}, {"id": 1092974, "label": "ENT", "start_offset": 682, "end_offset": 726}], "relations": [{"id": 176134, "from_id": 1092965, "to_id": 1092964, "type": "COREF"}, {"id": 176135, "from_id": 1092967, "to_id": 1092968, "type": "USED-FOR"}, {"id": 176136, "from_id": 1092973, "to_id": 1092964, "type": "COREF"}, {"id": 176137, "from_id": 1092966, "to_id": 1092965, "type": "EVALUATE-FOR"}, {"id": 176138, "from_id": 1092970, "to_id": 1092967, "type": "COREF"}, {"id": 176139, "from_id": 1092974, "to_id": 1092973, "type": "EVALUATE-FOR"}, {"id": 176140, "from_id": 1092969, "to_id": 1092970, "type": "EVALUATE-FOR"}]}
{"id": "IJCAI_2013_4_abs", "text": "In this paper, we state the challenges of high-level program execution in multi-agent settings. We first introduce high-level program execution and the related work. Then we describe the completed work, the future work and its approaches. We conclude with the expected contributions of our research.", "Comments": [], "entities": [{"id": 1092995, "label": "ENT", "start_offset": 42, "end_offset": 70}, {"id": 1092996, "label": "ENT", "start_offset": 115, "end_offset": 143}], "relations": [{"id": 176154, "from_id": 1092995, "to_id": 1092996, "type": "COREF"}]}
{"id": "H01-1055", "text": "   Recent advances in  Automatic Speech Recognition technology  have put the goal of naturally sounding  dialog systems  within reach. However, the improved  speech recognition  has brought to light a new problem: as  dialog systems  understand more of what the  user  tells them, they need to be more sophisticated at responding to the  user  . The issue of  system response  to  users  has been extensively studied by the  natural language generation community  , though rarely in the context of  dialog systems  . We show how research in  generation  can be adapted to  dialog systems  , and how the high cost of hand-crafting  knowledge-based generation systems  can be overcome by employing  machine learning techniques  . ", "Comments": [], "entities": [{"id": 1093017, "label": "ENT", "start_offset": 23, "end_offset": 62}, {"id": 1093018, "label": "ENT", "start_offset": 105, "end_offset": 119}, {"id": 1093019, "label": "ENT", "start_offset": 158, "end_offset": 176}, {"id": 1093020, "label": "ENT", "start_offset": 218, "end_offset": 232}, {"id": 1093021, "label": "ENT", "start_offset": 281, "end_offset": 285}, {"id": 1093022, "label": "ENT", "start_offset": 360, "end_offset": 375}, {"id": 1093023, "label": "ENT", "start_offset": 425, "end_offset": 462}, {"id": 1093024, "label": "ENT", "start_offset": 499, "end_offset": 513}, {"id": 1093025, "label": "ENT", "start_offset": 542, "end_offset": 552}, {"id": 1093026, "label": "ENT", "start_offset": 573, "end_offset": 587}, {"id": 1093027, "label": "ENT", "start_offset": 616, "end_offset": 665}, {"id": 1093028, "label": "ENT", "start_offset": 697, "end_offset": 724}], "relations": [{"id": 176168, "from_id": 1093017, "to_id": 1093018, "type": "USED-FOR"}, {"id": 176169, "from_id": 1093022, "to_id": 1093023, "type": "PART-OF"}, {"id": 176170, "from_id": 1093025, "to_id": 1093026, "type": "USED-FOR"}, {"id": 176171, "from_id": 1093024, "to_id": 1093020, "type": "COREF"}, {"id": 176172, "from_id": 1093020, "to_id": 1093018, "type": "COREF"}, {"id": 176173, "from_id": 1093026, "to_id": 1093020, "type": "COREF"}, {"id": 176174, "from_id": 1093028, "to_id": 1093027, "type": "USED-FOR"}, {"id": 176175, "from_id": 1093021, "to_id": 1093020, "type": "COREF"}, {"id": 176176, "from_id": 1093023, "to_id": 1093024, "type": "COMPARE"}]}
{"id": "I05-3022", "text": " This paper presents a  word segmentation system  in France Telecom R&D Beijing, which uses a unified approach to  word breaking  and  OOV identification . The  output  can be customized to meet different  segmentation standards  through the application of an ordered list of transformation. The  system  participated in all the tracks of the  segmentation bakeoff  --  PK-open ,  PK-closed ,  AS-open ,  AS-closed ,  HK-open ,  HK-closed ,  MSR-open  and  MSR- closed  -- and achieved the  state-of-the-art performance  in  MSR-open ,  MSR-close  and  PK-open  tracks. Analysis of the results shows that each component of the system contributed to the  scores . ", "Comments": [], "entities": [{"id": 1093048, "label": "ENT", "start_offset": 24, "end_offset": 48}, {"id": 1093049, "label": "ENT", "start_offset": 102, "end_offset": 110}, {"id": 1093050, "label": "ENT", "start_offset": 115, "end_offset": 128}, {"id": 1093051, "label": "ENT", "start_offset": 135, "end_offset": 153}, {"id": 1093052, "label": "ENT", "start_offset": 297, "end_offset": 303}, {"id": 1093053, "label": "ENT", "start_offset": 344, "end_offset": 364}, {"id": 1093054, "label": "ENT", "start_offset": 370, "end_offset": 377}, {"id": 1093055, "label": "ENT", "start_offset": 381, "end_offset": 390}, {"id": 1093056, "label": "ENT", "start_offset": 394, "end_offset": 401}, {"id": 1093057, "label": "ENT", "start_offset": 405, "end_offset": 414}, {"id": 1093058, "label": "ENT", "start_offset": 418, "end_offset": 425}, {"id": 1093059, "label": "ENT", "start_offset": 429, "end_offset": 438}, {"id": 1093060, "label": "ENT", "start_offset": 442, "end_offset": 450}, {"id": 1093061, "label": "ENT", "start_offset": 457, "end_offset": 468}, {"id": 1093062, "label": "ENT", "start_offset": 525, "end_offset": 533}, {"id": 1093063, "label": "ENT", "start_offset": 537, "end_offset": 546}, {"id": 1093064, "label": "ENT", "start_offset": 553, "end_offset": 560}, {"id": 1093065, "label": "ENT", "start_offset": 627, "end_offset": 633}], "relations": [{"id": 176193, "from_id": 1093049, "to_id": 1093048, "type": "USED-FOR"}, {"id": 176194, "from_id": 1093050, "to_id": 1093051, "type": "CONJUNCTION"}, {"id": 176195, "from_id": 1093052, "to_id": 1093048, "type": "COREF"}, {"id": 176196, "from_id": 1093054, "to_id": 1093053, "type": "HYPONYM-OF"}, {"id": 176197, "from_id": 1093055, "to_id": 1093053, "type": "HYPONYM-OF"}, {"id": 176198, "from_id": 1093056, "to_id": 1093053, "type": "HYPONYM-OF"}, {"id": 176199, "from_id": 1093057, "to_id": 1093053, "type": "HYPONYM-OF"}, {"id": 176200, "from_id": 1093058, "to_id": 1093053, "type": "HYPONYM-OF"}, {"id": 176201, "from_id": 1093063, "to_id": 1093061, "type": "COREF"}, {"id": 176202, "from_id": 1093064, "to_id": 1093054, "type": "COREF"}, {"id": 176203, "from_id": 1093065, "to_id": 1093052, "type": "COREF"}, {"id": 176204, "from_id": 1093054, "to_id": 1093055, "type": "CONJUNCTION"}, {"id": 176205, "from_id": 1093055, "to_id": 1093056, "type": "CONJUNCTION"}, {"id": 176206, "from_id": 1093056, "to_id": 1093057, "type": "CONJUNCTION"}, {"id": 176207, "from_id": 1093057, "to_id": 1093058, "type": "CONJUNCTION"}, {"id": 176208, "from_id": 1093058, "to_id": 1093059, "type": "CONJUNCTION"}, {"id": 176209, "from_id": 1093059, "to_id": 1093060, "type": "CONJUNCTION"}, {"id": 176210, "from_id": 1093060, "to_id": 1093061, "type": "CONJUNCTION"}, {"id": 176211, "from_id": 1093049, "to_id": 1093050, "type": "USED-FOR"}, {"id": 176212, "from_id": 1093049, "to_id": 1093051, "type": "USED-FOR"}, {"id": 176213, "from_id": 1093059, "to_id": 1093053, "type": "HYPONYM-OF"}, {"id": 176214, "from_id": 1093060, "to_id": 1093053, "type": "HYPONYM-OF"}, {"id": 176215, "from_id": 1093061, "to_id": 1093053, "type": "HYPONYM-OF"}, {"id": 176216, "from_id": 1093060, "to_id": 1093062, "type": "COREF"}, {"id": 176217, "from_id": 1093062, "to_id": 1093063, "type": "CONJUNCTION"}, {"id": 176218, "from_id": 1093063, "to_id": 1093064, "type": "CONJUNCTION"}]}
{"id": "P06-1012", "text": " Instances of a  word  drawn from different  domains  may have different  sense priors  (the proportions of the different  senses  of a  word ). This in turn affects the accuracy of  word sense disambiguation (WSD) systems  trained and applied on different  domains . This paper presents a method to estimate the  sense priors  of  words  drawn from a new  domain , and highlights the importance of using  well calibrated probabilities  when performing these  estimations . By using  well calibrated probabilities , we are able to estimate the  sense priors  effectively to achieve significant improvements in  WSD accuracy . ", "Comments": [], "entities": [{"id": 1093081, "label": "ENT", "start_offset": 74, "end_offset": 86}, {"id": 1093082, "label": "ENT", "start_offset": 170, "end_offset": 178}, {"id": 1093083, "label": "ENT", "start_offset": 183, "end_offset": 222}, {"id": 1093084, "label": "ENT", "start_offset": 290, "end_offset": 296}, {"id": 1093085, "label": "ENT", "start_offset": 314, "end_offset": 337}, {"id": 1093086, "label": "ENT", "start_offset": 352, "end_offset": 363}, {"id": 1093087, "label": "ENT", "start_offset": 406, "end_offset": 435}, {"id": 1093088, "label": "ENT", "start_offset": 460, "end_offset": 471}, {"id": 1093089, "label": "ENT", "start_offset": 484, "end_offset": 513}, {"id": 1093090, "label": "ENT", "start_offset": 545, "end_offset": 557}, {"id": 1093091, "label": "ENT", "start_offset": 611, "end_offset": 623}], "relations": [{"id": 176233, "from_id": 1093087, "to_id": 1093088, "type": "USED-FOR"}, {"id": 176234, "from_id": 1093089, "to_id": 1093090, "type": "USED-FOR"}, {"id": 176235, "from_id": 1093082, "to_id": 1093083, "type": "EVALUATE-FOR"}, {"id": 176236, "from_id": 1093084, "to_id": 1093085, "type": "USED-FOR"}, {"id": 176237, "from_id": 1093086, "to_id": 1093085, "type": "FEATURE-OF"}]}
{"id": "E95-1033", "text": " We provide a unified account of  sentence-level and text-level anaphora  within the framework of a  dependency-based grammar model . Criteria for  anaphora resolution  within  sentence boundaries  rephrase major concepts from  GB's binding theory , while those for  text-level anaphora  incorporate an adapted version of a  Grosz-Sidner-style focus model . ", "Comments": [], "entities": [{"id": 1093092, "label": "ENT", "start_offset": 34, "end_offset": 72}, {"id": 1093093, "label": "ENT", "start_offset": 101, "end_offset": 131}, {"id": 1093094, "label": "ENT", "start_offset": 134, "end_offset": 142}, {"id": 1093095, "label": "ENT", "start_offset": 148, "end_offset": 196}, {"id": 1093096, "label": "ENT", "start_offset": 228, "end_offset": 247}, {"id": 1093097, "label": "ENT", "start_offset": 256, "end_offset": 261}, {"id": 1093098, "label": "ENT", "start_offset": 267, "end_offset": 286}, {"id": 1093099, "label": "ENT", "start_offset": 325, "end_offset": 355}], "relations": [{"id": 176238, "from_id": 1093093, "to_id": 1093092, "type": "USED-FOR"}, {"id": 176239, "from_id": 1093097, "to_id": 1093094, "type": "COREF"}, {"id": 176240, "from_id": 1093097, "to_id": 1093098, "type": "USED-FOR"}, {"id": 176241, "from_id": 1093099, "to_id": 1093097, "type": "PART-OF"}, {"id": 176242, "from_id": 1093098, "to_id": 1093092, "type": "HYPONYM-OF"}, {"id": 176243, "from_id": 1093095, "to_id": 1093092, "type": "HYPONYM-OF"}, {"id": 176244, "from_id": 1093094, "to_id": 1093095, "type": "USED-FOR"}, {"id": 176245, "from_id": 1093096, "to_id": 1093094, "type": "USED-FOR"}]}
{"id": "A97-1021", "text": " We focus on the problem of building large  repositories  of  lexical conceptual structure (LCS) representations  for  verbs  in multiple  languages . One of the main results of this work is the definition of a relation between  broad semantic classes  and  LCS meaning components . Our  acquisition program - LEXICALL -  takes, as input, the result of previous work on  verb classification  and  thematic grid tagging , and outputs  LCS representations  for different  languages . These  representations  have been ported into  English, Arabic and Spanish lexicons , each containing approximately 9000  verbs . We are currently using these  lexicons  in an  operational foreign language tutoring  and  machine translation .  ", "Comments": [], "entities": [{"id": 1093100, "label": "ENT", "start_offset": 62, "end_offset": 112}, {"id": 1093101, "label": "ENT", "start_offset": 229, "end_offset": 251}, {"id": 1093102, "label": "ENT", "start_offset": 258, "end_offset": 280}, {"id": 1093103, "label": "ENT", "start_offset": 288, "end_offset": 320}, {"id": 1093104, "label": "ENT", "start_offset": 371, "end_offset": 390}, {"id": 1093105, "label": "ENT", "start_offset": 397, "end_offset": 418}, {"id": 1093106, "label": "ENT", "start_offset": 434, "end_offset": 453}, {"id": 1093107, "label": "ENT", "start_offset": 489, "end_offset": 504}, {"id": 1093108, "label": "ENT", "start_offset": 529, "end_offset": 565}, {"id": 1093109, "label": "ENT", "start_offset": 642, "end_offset": 650}, {"id": 1093110, "label": "ENT", "start_offset": 659, "end_offset": 696}, {"id": 1093111, "label": "ENT", "start_offset": 703, "end_offset": 722}], "relations": [{"id": 176246, "from_id": 1093107, "to_id": 1093108, "type": "USED-FOR"}, {"id": 176247, "from_id": 1093109, "to_id": 1093110, "type": "USED-FOR"}, {"id": 176248, "from_id": 1093107, "to_id": 1093106, "type": "COREF"}, {"id": 176249, "from_id": 1093109, "to_id": 1093108, "type": "COREF"}, {"id": 176250, "from_id": 1093101, "to_id": 1093102, "type": "CONJUNCTION"}, {"id": 176251, "from_id": 1093103, "to_id": 1093106, "type": "USED-FOR"}, {"id": 176252, "from_id": 1093104, "to_id": 1093103, "type": "USED-FOR"}, {"id": 176253, "from_id": 1093105, "to_id": 1093103, "type": "USED-FOR"}, {"id": 176254, "from_id": 1093109, "to_id": 1093111, "type": "USED-FOR"}, {"id": 176255, "from_id": 1093106, "to_id": 1093100, "type": "COREF"}, {"id": 176256, "from_id": 1093110, "to_id": 1093111, "type": "CONJUNCTION"}, {"id": 176257, "from_id": 1093104, "to_id": 1093105, "type": "CONJUNCTION"}]}
{"id": "N04-2005", "text": " The  translation  of  English text  into  American Sign Language (ASL) animation  tests the limits of  traditional MT architectural designs . A new  semantic representation  is proposed that uses  virtual reality 3D scene modeling software  to produce  spatially complex ASL phenomena  called \" classifier predicates .\" The model acts as an  interlingua  within a new  multi-pathway MT architecture design  that also incorporates  transfer  and  direct approaches  into a single system. ", "Comments": [], "entities": [{"id": 1093112, "label": "ENT", "start_offset": 6, "end_offset": 81}, {"id": 1093113, "label": "ENT", "start_offset": 116, "end_offset": 140}, {"id": 1093114, "label": "ENT", "start_offset": 150, "end_offset": 173}, {"id": 1093115, "label": "ENT", "start_offset": 198, "end_offset": 240}, {"id": 1093116, "label": "ENT", "start_offset": 254, "end_offset": 285}, {"id": 1093117, "label": "ENT", "start_offset": 296, "end_offset": 317}, {"id": 1093118, "label": "ENT", "start_offset": 325, "end_offset": 330}, {"id": 1093119, "label": "ENT", "start_offset": 343, "end_offset": 354}, {"id": 1093120, "label": "ENT", "start_offset": 370, "end_offset": 406}, {"id": 1093121, "label": "ENT", "start_offset": 432, "end_offset": 440}, {"id": 1093122, "label": "ENT", "start_offset": 447, "end_offset": 464}, {"id": 1093123, "label": "ENT", "start_offset": 480, "end_offset": 486}], "relations": [{"id": 176258, "from_id": 1093115, "to_id": 1093114, "type": "USED-FOR"}, {"id": 176259, "from_id": 1093114, "to_id": 1093116, "type": "USED-FOR"}, {"id": 176260, "from_id": 1093117, "to_id": 1093116, "type": "HYPONYM-OF"}, {"id": 176261, "from_id": 1093121, "to_id": 1093123, "type": "PART-OF"}, {"id": 176262, "from_id": 1093122, "to_id": 1093123, "type": "PART-OF"}, {"id": 176263, "from_id": 1093121, "to_id": 1093122, "type": "CONJUNCTION"}, {"id": 176264, "from_id": 1093123, "to_id": 1093118, "type": "COREF"}, {"id": 176265, "from_id": 1093113, "to_id": 1093112, "type": "USED-FOR"}]}
{"id": "P04-2010", "text": " This paper presents a novel  ensemble learning approach  to resolving  German pronouns .  Boosting , the method in question, combines the moderately accurate  hypotheses  of several  classifiers  to form a highly accurate one. Experiments show that this approach is superior to a single  decision-tree classifier . Furthermore, we present a  standalone system  that resolves  pronouns  in  unannotated text  by using a fully automatic sequence of  preprocessing modules  that mimics the  manual annotation process . Although the system performs well within a limited  textual domain , further research is needed to make it effective for  open-domain question answering  and  text summarisation . ", "Comments": [], "entities": [{"id": 1093171, "label": "ENT", "start_offset": 30, "end_offset": 56}, {"id": 1093172, "label": "ENT", "start_offset": 72, "end_offset": 87}, {"id": 1093173, "label": "ENT", "start_offset": 91, "end_offset": 99}, {"id": 1093174, "label": "ENT", "start_offset": 184, "end_offset": 195}, {"id": 1093175, "label": "ENT", "start_offset": 255, "end_offset": 263}, {"id": 1093176, "label": "ENT", "start_offset": 289, "end_offset": 313}, {"id": 1093177, "label": "ENT", "start_offset": 343, "end_offset": 360}, {"id": 1093178, "label": "ENT", "start_offset": 377, "end_offset": 385}, {"id": 1093179, "label": "ENT", "start_offset": 391, "end_offset": 407}, {"id": 1093180, "label": "ENT", "start_offset": 449, "end_offset": 470}, {"id": 1093181, "label": "ENT", "start_offset": 489, "end_offset": 514}, {"id": 1093182, "label": "ENT", "start_offset": 530, "end_offset": 536}, {"id": 1093183, "label": "ENT", "start_offset": 569, "end_offset": 583}, {"id": 1093184, "label": "ENT", "start_offset": 621, "end_offset": 623}, {"id": 1093185, "label": "ENT", "start_offset": 639, "end_offset": 669}, {"id": 1093186, "label": "ENT", "start_offset": 676, "end_offset": 694}], "relations": [{"id": 176308, "from_id": 1093171, "to_id": 1093172, "type": "USED-FOR"}, {"id": 176309, "from_id": 1093177, "to_id": 1093179, "type": "USED-FOR"}, {"id": 176310, "from_id": 1093183, "to_id": 1093185, "type": "COMPARE"}, {"id": 176311, "from_id": 1093173, "to_id": 1093175, "type": "COREF"}, {"id": 176312, "from_id": 1093175, "to_id": 1093176, "type": "COMPARE"}, {"id": 176313, "from_id": 1093177, "to_id": 1093178, "type": "USED-FOR"}, {"id": 176314, "from_id": 1093178, "to_id": 1093179, "type": "PART-OF"}, {"id": 176315, "from_id": 1093180, "to_id": 1093181, "type": "USED-FOR"}, {"id": 176316, "from_id": 1093180, "to_id": 1093177, "type": "USED-FOR"}, {"id": 176317, "from_id": 1093177, "to_id": 1093182, "type": "COREF"}, {"id": 176318, "from_id": 1093183, "to_id": 1093182, "type": "EVALUATE-FOR"}, {"id": 176319, "from_id": 1093182, "to_id": 1093184, "type": "COREF"}, {"id": 176320, "from_id": 1093184, "to_id": 1093185, "type": "USED-FOR"}, {"id": 176321, "from_id": 1093184, "to_id": 1093186, "type": "USED-FOR"}, {"id": 176322, "from_id": 1093185, "to_id": 1093186, "type": "CONJUNCTION"}, {"id": 176323, "from_id": 1093171, "to_id": 1093177, "type": "COREF"}]}
{"id": "H05-1117", "text": " Following recent developments in the  automatic evaluation  of  machine translation  and  document summarization  , we present a similar approach, implemented in a measure called  POURPRE  , for  automatically evaluating answers to definition questions  . Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for  scoring system output  is an impediment to progress in the field, which we address with this work. Experiments with the  TREC 2003 and TREC 2004 QA tracks  indicate that  rankings  produced by our metric correlate highly with  official rankings  , and that  POURPRE  outperforms direct application of existing metrics. ", "Comments": [], "entities": [{"id": 1093208, "label": "ENT", "start_offset": 39, "end_offset": 59}, {"id": 1093209, "label": "ENT", "start_offset": 65, "end_offset": 84}, {"id": 1093210, "label": "ENT", "start_offset": 91, "end_offset": 113}, {"id": 1093211, "label": "ENT", "start_offset": 138, "end_offset": 146}, {"id": 1093212, "label": "ENT", "start_offset": 165, "end_offset": 172}, {"id": 1093213, "label": "ENT", "start_offset": 181, "end_offset": 188}, {"id": 1093214, "label": "ENT", "start_offset": 197, "end_offset": 253}, {"id": 1093215, "label": "ENT", "start_offset": 587, "end_offset": 620}, {"id": 1093216, "label": "ENT", "start_offset": 637, "end_offset": 645}, {"id": 1093217, "label": "ENT", "start_offset": 663, "end_offset": 669}, {"id": 1093218, "label": "ENT", "start_offset": 693, "end_offset": 710}, {"id": 1093219, "label": "ENT", "start_offset": 724, "end_offset": 731}, {"id": 1093220, "label": "ENT", "start_offset": 776, "end_offset": 783}], "relations": [{"id": 176340, "from_id": 1093208, "to_id": 1093209, "type": "EVALUATE-FOR"}, {"id": 176341, "from_id": 1093208, "to_id": 1093210, "type": "EVALUATE-FOR"}, {"id": 176342, "from_id": 1093209, "to_id": 1093210, "type": "CONJUNCTION"}, {"id": 176343, "from_id": 1093213, "to_id": 1093212, "type": "COREF"}, {"id": 176344, "from_id": 1093212, "to_id": 1093214, "type": "USED-FOR"}, {"id": 176345, "from_id": 1093219, "to_id": 1093213, "type": "COREF"}, {"id": 176346, "from_id": 1093217, "to_id": 1093213, "type": "COREF"}, {"id": 176347, "from_id": 1093217, "to_id": 1093216, "type": "USED-FOR"}, {"id": 176348, "from_id": 1093215, "to_id": 1093217, "type": "EVALUATE-FOR"}, {"id": 176349, "from_id": 1093217, "to_id": 1093219, "type": "COREF"}, {"id": 176350, "from_id": 1093219, "to_id": 1093220, "type": "COMPARE"}, {"id": 176351, "from_id": 1093212, "to_id": 1093211, "type": "USED-FOR"}, {"id": 176352, "from_id": 1093215, "to_id": 1093219, "type": "EVALUATE-FOR"}, {"id": 176353, "from_id": 1093215, "to_id": 1093220, "type": "EVALUATE-FOR"}]}
{"id": "H05-1101", "text": "   This paper investigates some  computational problems  associated with  probabilistic translation models  that have recently been adopted in the literature on  machine translation  . These  models  can be viewed as pairs of  probabilistic context-free grammars  working in a 'synchronous' way. Two  hardness  results for the class  NP  are reported, along with an  exponential time lower-bound  for certain classes of algorithms that are currently used in the literature. ", "Comments": [], "entities": [{"id": 1093234, "label": "ENT", "start_offset": 33, "end_offset": 55}, {"id": 1093235, "label": "ENT", "start_offset": 74, "end_offset": 106}, {"id": 1093236, "label": "ENT", "start_offset": 162, "end_offset": 181}, {"id": 1093237, "label": "ENT", "start_offset": 192, "end_offset": 198}, {"id": 1093238, "label": "ENT", "start_offset": 227, "end_offset": 262}, {"id": 1093239, "label": "ENT", "start_offset": 367, "end_offset": 395}], "relations": [{"id": 176367, "from_id": 1093234, "to_id": 1093235, "type": "FEATURE-OF"}, {"id": 176368, "from_id": 1093238, "to_id": 1093237, "type": "FEATURE-OF"}, {"id": 176369, "from_id": 1093235, "to_id": 1093236, "type": "USED-FOR"}, {"id": 176370, "from_id": 1093237, "to_id": 1093235, "type": "COREF"}]}
{"id": "ECCV_2014_50_abs", "text": "This paper presents an approach to estimate the intrinsic texture properties (albedo, shading, normal) of scenes from multiple view acquisition under unknown illumination conditions. We introduce the concept of intrinsic textures, which are pixel-resolution surface textures representing the intrinsic appearance parameters of a scene. Unlike previous video relighting methods, the approach does not assume regions of uniform albedo, which makes it applicable to richly textured scenes. We show that intrinsic image methods can be used to refine an initial, low-frequency shading estimate based on a global lighting reconstruction from an original texture and coarse scene geometry in order to resolve the inherent global ambiguity in shading. The method is applied to relight-ing of free-viewpoint rendering from multiple view video capture. This demonstrates relighting with reproduction of fine surface detail. ", "Comments": [], "entities": [{"id": 1093240, "label": "ENT", "start_offset": 23, "end_offset": 31}, {"id": 1093241, "label": "ENT", "start_offset": 48, "end_offset": 112}, {"id": 1093242, "label": "ENT", "start_offset": 118, "end_offset": 143}, {"id": 1093243, "label": "ENT", "start_offset": 150, "end_offset": 181}, {"id": 1093244, "label": "ENT", "start_offset": 211, "end_offset": 229}, {"id": 1093245, "label": "ENT", "start_offset": 241, "end_offset": 274}, {"id": 1093246, "label": "ENT", "start_offset": 292, "end_offset": 323}, {"id": 1093247, "label": "ENT", "start_offset": 352, "end_offset": 376}, {"id": 1093248, "label": "ENT", "start_offset": 382, "end_offset": 390}, {"id": 1093249, "label": "ENT", "start_offset": 418, "end_offset": 432}, {"id": 1093250, "label": "ENT", "start_offset": 446, "end_offset": 448}, {"id": 1093251, "label": "ENT", "start_offset": 463, "end_offset": 485}, {"id": 1093252, "label": "ENT", "start_offset": 500, "end_offset": 523}, {"id": 1093253, "label": "ENT", "start_offset": 549, "end_offset": 588}, {"id": 1093254, "label": "ENT", "start_offset": 600, "end_offset": 630}, {"id": 1093255, "label": "ENT", "start_offset": 648, "end_offset": 681}, {"id": 1093256, "label": "ENT", "start_offset": 706, "end_offset": 742}, {"id": 1093257, "label": "ENT", "start_offset": 748, "end_offset": 754}, {"id": 1093258, "label": "ENT", "start_offset": 769, "end_offset": 808}, {"id": 1093259, "label": "ENT", "start_offset": 814, "end_offset": 841}, {"id": 1093260, "label": "ENT", "start_offset": 861, "end_offset": 871}, {"id": 1093261, "label": "ENT", "start_offset": 877, "end_offset": 912}], "relations": [{"id": 176371, "from_id": 1093243, "to_id": 1093242, "type": "FEATURE-OF"}, {"id": 176372, "from_id": 1093242, "to_id": 1093241, "type": "USED-FOR"}, {"id": 176373, "from_id": 1093240, "to_id": 1093241, "type": "USED-FOR"}, {"id": 176374, "from_id": 1093245, "to_id": 1093244, "type": "COREF"}, {"id": 176375, "from_id": 1093248, "to_id": 1093247, "type": "COMPARE"}, {"id": 176376, "from_id": 1093248, "to_id": 1093250, "type": "COREF"}, {"id": 176377, "from_id": 1093250, "to_id": 1093251, "type": "USED-FOR"}, {"id": 176378, "from_id": 1093252, "to_id": 1093253, "type": "USED-FOR"}, {"id": 176379, "from_id": 1093254, "to_id": 1093252, "type": "USED-FOR"}, {"id": 176380, "from_id": 1093255, "to_id": 1093254, "type": "FEATURE-OF"}, {"id": 176381, "from_id": 1093253, "to_id": 1093256, "type": "USED-FOR"}, {"id": 176382, "from_id": 1093252, "to_id": 1093257, "type": "COREF"}, {"id": 176383, "from_id": 1093257, "to_id": 1093258, "type": "USED-FOR"}, {"id": 176384, "from_id": 1093261, "to_id": 1093260, "type": "FEATURE-OF"}, {"id": 176385, "from_id": 1093259, "to_id": 1093258, "type": "USED-FOR"}, {"id": 176386, "from_id": 1093260, "to_id": 1093258, "type": "COREF"}, {"id": 176387, "from_id": 1093240, "to_id": 1093248, "type": "COREF"}, {"id": 176388, "from_id": 1093248, "to_id": 1093257, "type": "COREF"}]}
{"id": "J05-1003", "text": " This article considers approaches which rerank the output of an existing  probabilistic parser  . The base  parser  produces a set of  candidate parses  for each input  sentence  , with associated  probabilities  that define an initial  ranking  of these  parses  . A second  model  then attempts to improve upon this initial  ranking  , using additional  features  of the  tree  as evidence. The strength of our approach is that it allows a  tree  to be represented as an arbitrary set of  features  , without concerns about how these  features  interact or overlap and without the need to define a  derivation  or a  generative model  which takes these  features  into account . We introduce a new method for the  reranking task  , based on the  boosting approach  to  ranking problems  described in Freund et al. (1998). We apply the  boosting method  to  parsing  the  Wall Street Journal treebank  . The method combined the  log-likelihood  under a  baseline model  (that of Collins [1999]) with evidence from an additional 500,000  features  over  parse trees  that were not included in the original  model  . The new  model  achieved 89.75%  F-measure  , a 13% relative decrease in  F-measure  error over the  baseline model's score  of 88.2%. The article also introduces a new algorithm for the  boosting approach  which takes advantage of the  sparsity of the feature space  in the  parsing data  . Experiments show significant efficiency gains for the new algorithm over the obvious  implementation  of the  boosting approach  . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on  feature selection methods  within  log-linear (maximum-entropy) models  . Although the experiments in this article are on  natural language parsing (NLP)  , the approach should be applicable to many other  NLP problems  which are naturally framed as  ranking tasks  , for example,  speech recognition  ,  machine translation  , or  natural language generation  . ", "Comments": [], "entities": [{"id": 1093281, "label": "ENT", "start_offset": 24, "end_offset": 34}, {"id": 1093282, "label": "ENT", "start_offset": 75, "end_offset": 95}, {"id": 1093283, "label": "ENT", "start_offset": 109, "end_offset": 115}, {"id": 1093284, "label": "ENT", "start_offset": 136, "end_offset": 152}, {"id": 1093285, "label": "ENT", "start_offset": 238, "end_offset": 245}, {"id": 1093286, "label": "ENT", "start_offset": 257, "end_offset": 263}, {"id": 1093287, "label": "ENT", "start_offset": 277, "end_offset": 282}, {"id": 1093288, "label": "ENT", "start_offset": 328, "end_offset": 335}, {"id": 1093289, "label": "ENT", "start_offset": 357, "end_offset": 365}, {"id": 1093290, "label": "ENT", "start_offset": 375, "end_offset": 379}, {"id": 1093291, "label": "ENT", "start_offset": 414, "end_offset": 422}, {"id": 1093292, "label": "ENT", "start_offset": 431, "end_offset": 433}, {"id": 1093293, "label": "ENT", "start_offset": 444, "end_offset": 448}, {"id": 1093294, "label": "ENT", "start_offset": 492, "end_offset": 500}, {"id": 1093295, "label": "ENT", "start_offset": 538, "end_offset": 546}, {"id": 1093296, "label": "ENT", "start_offset": 602, "end_offset": 612}, {"id": 1093297, "label": "ENT", "start_offset": 620, "end_offset": 636}, {"id": 1093298, "label": "ENT", "start_offset": 657, "end_offset": 665}, {"id": 1093299, "label": "ENT", "start_offset": 701, "end_offset": 707}, {"id": 1093300, "label": "ENT", "start_offset": 717, "end_offset": 731}, {"id": 1093301, "label": "ENT", "start_offset": 749, "end_offset": 766}, {"id": 1093302, "label": "ENT", "start_offset": 772, "end_offset": 788}, {"id": 1093303, "label": "ENT", "start_offset": 839, "end_offset": 854}, {"id": 1093304, "label": "ENT", "start_offset": 860, "end_offset": 867}, {"id": 1093305, "label": "ENT", "start_offset": 874, "end_offset": 902}, {"id": 1093306, "label": "ENT", "start_offset": 910, "end_offset": 916}, {"id": 1093307, "label": "ENT", "start_offset": 931, "end_offset": 945}, {"id": 1093308, "label": "ENT", "start_offset": 956, "end_offset": 970}, {"id": 1093309, "label": "ENT", "start_offset": 1039, "end_offset": 1047}, {"id": 1093310, "label": "ENT", "start_offset": 1055, "end_offset": 1066}, {"id": 1093311, "label": "ENT", "start_offset": 1108, "end_offset": 1113}, {"id": 1093312, "label": "ENT", "start_offset": 1126, "end_offset": 1131}, {"id": 1093313, "label": "ENT", "start_offset": 1150, "end_offset": 1159}, {"id": 1093314, "label": "ENT", "start_offset": 1191, "end_offset": 1200}, {"id": 1093315, "label": "ENT", "start_offset": 1218, "end_offset": 1232}, {"id": 1093316, "label": "ENT", "start_offset": 1286, "end_offset": 1295}, {"id": 1093317, "label": "ENT", "start_offset": 1305, "end_offset": 1322}, {"id": 1093318, "label": "ENT", "start_offset": 1354, "end_offset": 1383}, {"id": 1093319, "label": "ENT", "start_offset": 1393, "end_offset": 1405}, {"id": 1093320, "label": "ENT", "start_offset": 1467, "end_offset": 1476}, {"id": 1093321, "label": "ENT", "start_offset": 1519, "end_offset": 1536}, {"id": 1093322, "label": "ENT", "start_offset": 1558, "end_offset": 1564}, {"id": 1093323, "label": "ENT", "start_offset": 1652, "end_offset": 1677}, {"id": 1093324, "label": "ENT", "start_offset": 1687, "end_offset": 1722}, {"id": 1093325, "label": "ENT", "start_offset": 1775, "end_offset": 1805}, {"id": 1093326, "label": "ENT", "start_offset": 1858, "end_offset": 1870}, {"id": 1093327, "label": "ENT", "start_offset": 1903, "end_offset": 1916}, {"id": 1093328, "label": "ENT", "start_offset": 1934, "end_offset": 1952}, {"id": 1093329, "label": "ENT", "start_offset": 1957, "end_offset": 1976}, {"id": 1093330, "label": "ENT", "start_offset": 1984, "end_offset": 2011}], "relations": [{"id": 176409, "from_id": 1093298, "to_id": 1093297, "type": "USED-FOR"}, {"id": 176410, "from_id": 1093301, "to_id": 1093302, "type": "USED-FOR"}, {"id": 176411, "from_id": 1093313, "to_id": 1093312, "type": "EVALUATE-FOR"}, {"id": 176412, "from_id": 1093318, "to_id": 1093319, "type": "FEATURE-OF"}, {"id": 176413, "from_id": 1093323, "to_id": 1093324, "type": "PART-OF"}, {"id": 176414, "from_id": 1093283, "to_id": 1093282, "type": "COREF"}, {"id": 176415, "from_id": 1093286, "to_id": 1093284, "type": "COREF"}, {"id": 176416, "from_id": 1093289, "to_id": 1093287, "type": "USED-FOR"}, {"id": 176417, "from_id": 1093287, "to_id": 1093288, "type": "USED-FOR"}, {"id": 176418, "from_id": 1093292, "to_id": 1093291, "type": "COREF"}, {"id": 176419, "from_id": 1093291, "to_id": 1093281, "type": "COREF"}, {"id": 176420, "from_id": 1093295, "to_id": 1093294, "type": "COREF"}, {"id": 176421, "from_id": 1093299, "to_id": 1093300, "type": "USED-FOR"}, {"id": 176422, "from_id": 1093301, "to_id": 1093299, "type": "USED-FOR"}, {"id": 176423, "from_id": 1093303, "to_id": 1093304, "type": "USED-FOR"}, {"id": 176424, "from_id": 1093306, "to_id": 1093303, "type": "COREF"}, {"id": 176425, "from_id": 1093307, "to_id": 1093308, "type": "CONJUNCTION"}, {"id": 176426, "from_id": 1093307, "to_id": 1093306, "type": "PART-OF"}, {"id": 176427, "from_id": 1093311, "to_id": 1093308, "type": "COREF"}, {"id": 176428, "from_id": 1093312, "to_id": 1093306, "type": "COREF"}, {"id": 176429, "from_id": 1093316, "to_id": 1093317, "type": "USED-FOR"}, {"id": 176430, "from_id": 1093318, "to_id": 1093316, "type": "USED-FOR"}, {"id": 176431, "from_id": 1093320, "to_id": 1093316, "type": "COREF"}, {"id": 176432, "from_id": 1093320, "to_id": 1093321, "type": "COMPARE"}, {"id": 176433, "from_id": 1093322, "to_id": 1093320, "type": "COREF"}, {"id": 176434, "from_id": 1093328, "to_id": 1093326, "type": "HYPONYM-OF"}, {"id": 176435, "from_id": 1093329, "to_id": 1093326, "type": "HYPONYM-OF"}, {"id": 176436, "from_id": 1093330, "to_id": 1093326, "type": "HYPONYM-OF"}, {"id": 176437, "from_id": 1093328, "to_id": 1093329, "type": "CONJUNCTION"}, {"id": 176438, "from_id": 1093329, "to_id": 1093330, "type": "CONJUNCTION"}, {"id": 176439, "from_id": 1093303, "to_id": 1093301, "type": "COREF"}, {"id": 176440, "from_id": 1093321, "to_id": 1093317, "type": "COREF"}, {"id": 176441, "from_id": 1093285, "to_id": 1093288, "type": "COREF"}, {"id": 176442, "from_id": 1093300, "to_id": 1093302, "type": "COREF"}, {"id": 176443, "from_id": 1093302, "to_id": 1093327, "type": "COREF"}, {"id": 176444, "from_id": 1093305, "to_id": 1093303, "type": "USED-FOR"}, {"id": 176445, "from_id": 1093317, "to_id": 1093312, "type": "COREF"}, {"id": 176446, "from_id": 1093315, "to_id": 1093312, "type": "COMPARE"}, {"id": 176447, "from_id": 1093315, "to_id": 1093308, "type": "COREF"}, {"id": 176448, "from_id": 1093327, "to_id": 1093326, "type": "USED-FOR"}, {"id": 176449, "from_id": 1093328, "to_id": 1093327, "type": "HYPONYM-OF"}, {"id": 176450, "from_id": 1093329, "to_id": 1093327, "type": "HYPONYM-OF"}, {"id": 176451, "from_id": 1093330, "to_id": 1093327, "type": "HYPONYM-OF"}, {"id": 176452, "from_id": 1093313, "to_id": 1093314, "type": "COREF"}, {"id": 176453, "from_id": 1093314, "to_id": 1093315, "type": "EVALUATE-FOR"}, {"id": 176454, "from_id": 1093285, "to_id": 1093286, "type": "FEATURE-OF"}]}
{"id": "E06-1031", "text": " Most state-of-the-art  evaluation measures  for  machine translation  assign high  costs  to movements of  word  blocks. In many cases though such movements still result in correct or almost correct  sentences  . In this paper, we will present a new  evaluation measure  which explicitly models  block reordering  as an  edit operation  . Our  measure  can be exactly calculated in  quadratic time  . Furthermore, we will show how some  evaluation measures  can be improved by the introduction of  word-dependent substitution costs  . The correlation of the new  measure  with  human judgment  has been investigated systematically on two different  language pairs  . The experimental results will show that it significantly outperforms state-of-the-art approaches in  sentence-level correlation  . Results from experiments with  word dependent substitution costs  will demonstrate an additional increase of correlation between  automatic evaluation measures  and  human judgment  . ", "Comments": [], "entities": [{"id": 1093331, "label": "ENT", "start_offset": 24, "end_offset": 43}, {"id": 1093332, "label": "ENT", "start_offset": 50, "end_offset": 69}, {"id": 1093333, "label": "ENT", "start_offset": 252, "end_offset": 270}, {"id": 1093334, "label": "ENT", "start_offset": 297, "end_offset": 313}, {"id": 1093335, "label": "ENT", "start_offset": 322, "end_offset": 336}, {"id": 1093336, "label": "ENT", "start_offset": 345, "end_offset": 352}, {"id": 1093337, "label": "ENT", "start_offset": 384, "end_offset": 398}, {"id": 1093338, "label": "ENT", "start_offset": 438, "end_offset": 457}, {"id": 1093339, "label": "ENT", "start_offset": 499, "end_offset": 532}, {"id": 1093340, "label": "ENT", "start_offset": 564, "end_offset": 571}, {"id": 1093341, "label": "ENT", "start_offset": 579, "end_offset": 593}, {"id": 1093342, "label": "ENT", "start_offset": 650, "end_offset": 664}, {"id": 1093343, "label": "ENT", "start_offset": 708, "end_offset": 710}, {"id": 1093344, "label": "ENT", "start_offset": 754, "end_offset": 764}, {"id": 1093345, "label": "ENT", "start_offset": 769, "end_offset": 795}, {"id": 1093346, "label": "ENT", "start_offset": 830, "end_offset": 863}, {"id": 1093347, "label": "ENT", "start_offset": 929, "end_offset": 958}, {"id": 1093348, "label": "ENT", "start_offset": 965, "end_offset": 979}], "relations": [{"id": 176455, "from_id": 1093331, "to_id": 1093332, "type": "EVALUATE-FOR"}, {"id": 176456, "from_id": 1093333, "to_id": 1093334, "type": "USED-FOR"}, {"id": 176457, "from_id": 1093339, "to_id": 1093338, "type": "USED-FOR"}, {"id": 176458, "from_id": 1093347, "to_id": 1093348, "type": "CONJUNCTION"}, {"id": 176459, "from_id": 1093336, "to_id": 1093333, "type": "COREF"}, {"id": 176460, "from_id": 1093337, "to_id": 1093336, "type": "FEATURE-OF"}, {"id": 176461, "from_id": 1093340, "to_id": 1093341, "type": "COMPARE"}, {"id": 176462, "from_id": 1093340, "to_id": 1093338, "type": "COREF"}, {"id": 176463, "from_id": 1093338, "to_id": 1093336, "type": "COREF"}, {"id": 176464, "from_id": 1093343, "to_id": 1093340, "type": "COREF"}, {"id": 176465, "from_id": 1093343, "to_id": 1093344, "type": "COMPARE"}, {"id": 176466, "from_id": 1093345, "to_id": 1093344, "type": "EVALUATE-FOR"}, {"id": 176467, "from_id": 1093347, "to_id": 1093343, "type": "COREF"}, {"id": 176468, "from_id": 1093335, "to_id": 1093334, "type": "USED-FOR"}, {"id": 176469, "from_id": 1093345, "to_id": 1093343, "type": "EVALUATE-FOR"}]}
{"id": "C88-2132", "text": "    Chart parsing  is  directional  in the sense that it works from the starting point (usually the beginning of the sentence) extending its activity usually in a rightward manner. We shall introduce the concept of a  chart  that works outward from  islands  and makes sense of as much of the  sentence  as it is actually possible, and after that will lead to predictions of missing  fragments  . So, for any place where the easily identifiable  fragments  occur in the  sentence  , the process will extend to both the left and the right of the  islands  , until possibly completely missing  fragments  are reached. At that point, by virtue of the fact that both a left and a right context were found,  heuristics  can be introduced that predict the nature of the missing  fragments  . ", "Comments": [], "entities": [], "relations": []}
{"id": "I05-6010", "text": " This article is devoted to the problem of  quantifying noun groups  in  German . After a thorough description of the phenomena, the results of  corpus-based investigations  are described. Moreover, some examples are given that underline the necessity of integrating some kind of information other than  grammar sensu stricto  into the  treebank . We argue that a more sophisticated and fine-grained  annotation  in the  tree-bank  would have very positve effects on  stochastic parsers  trained on the  tree-bank  and on  grammars  induced from the  treebank , and it would make the  treebank  more valuable as a  source of data  for  theoretical linguistic investigations . The information gained from  corpus research  and the analyses that are proposed are realized in the framework of  SILVA , a  parsing  and  extraction tool  for  German text corpora . ", "Comments": [], "entities": [{"id": 1093349, "label": "ENT", "start_offset": 44, "end_offset": 79}, {"id": 1093350, "label": "ENT", "start_offset": 304, "end_offset": 325}, {"id": 1093351, "label": "ENT", "start_offset": 337, "end_offset": 345}, {"id": 1093352, "label": "ENT", "start_offset": 387, "end_offset": 411}, {"id": 1093353, "label": "ENT", "start_offset": 421, "end_offset": 430}, {"id": 1093354, "label": "ENT", "start_offset": 468, "end_offset": 486}, {"id": 1093355, "label": "ENT", "start_offset": 504, "end_offset": 513}, {"id": 1093356, "label": "ENT", "start_offset": 523, "end_offset": 531}, {"id": 1093357, "label": "ENT", "start_offset": 551, "end_offset": 559}, {"id": 1093358, "label": "ENT", "start_offset": 585, "end_offset": 593}, {"id": 1093359, "label": "ENT", "start_offset": 636, "end_offset": 673}, {"id": 1093360, "label": "ENT", "start_offset": 791, "end_offset": 796}, {"id": 1093361, "label": "ENT", "start_offset": 802, "end_offset": 831}, {"id": 1093362, "label": "ENT", "start_offset": 838, "end_offset": 857}], "relations": [{"id": 176470, "from_id": 1093358, "to_id": 1093359, "type": "USED-FOR"}, {"id": 176471, "from_id": 1093355, "to_id": 1093354, "type": "USED-FOR"}, {"id": 176472, "from_id": 1093357, "to_id": 1093356, "type": "USED-FOR"}, {"id": 176473, "from_id": 1093360, "to_id": 1093361, "type": "HYPONYM-OF"}, {"id": 176474, "from_id": 1093362, "to_id": 1093360, "type": "USED-FOR"}, {"id": 176475, "from_id": 1093358, "to_id": 1093357, "type": "COREF"}, {"id": 176476, "from_id": 1093357, "to_id": 1093355, "type": "COREF"}, {"id": 176477, "from_id": 1093352, "to_id": 1093354, "type": "USED-FOR"}, {"id": 176478, "from_id": 1093353, "to_id": 1093355, "type": "COREF"}, {"id": 176479, "from_id": 1093353, "to_id": 1093351, "type": "COREF"}]}
{"id": "M91-1029", "text": " The  PRC Adaptive Knowledge-based Text Understanding System (PAKTUS)  has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools, including a  core English lexicon ,  grammar , and concept representations, for building  natural language processing (NLP) systems  for  text understanding . Systems built with  PAKTUS  are intended to generate input to knowledge based systems ordata base systems. Input to the  NLP system  is typically derived from an existing  electronic message stream , such as a news wire.  PAKTUS  supports the adaptation of the generic core to a variety of domains:  JINTACCS messages ,  RAINFORM messages ,  news reports  about a specific type of event, such as financial transfers or terrorist acts, etc., by acquiring  sublanguage and domain-specific grammar ,  words, conceptual mappings , and  discourse patterns . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success. ", "Comments": [], "entities": [{"id": 1093363, "label": "ENT", "start_offset": 6, "end_offset": 69}, {"id": 1093364, "label": "ENT", "start_offset": 195, "end_offset": 201}, {"id": 1093365, "label": "ENT", "start_offset": 225, "end_offset": 245}, {"id": 1093366, "label": "ENT", "start_offset": 249, "end_offset": 256}, {"id": 1093367, "label": "ENT", "start_offset": 263, "end_offset": 286}, {"id": 1093368, "label": "ENT", "start_offset": 302, "end_offset": 343}, {"id": 1093369, "label": "ENT", "start_offset": 350, "end_offset": 368}, {"id": 1093370, "label": "ENT", "start_offset": 391, "end_offset": 397}, {"id": 1093371, "label": "ENT", "start_offset": 433, "end_offset": 456}, {"id": 1093372, "label": "ENT", "start_offset": 492, "end_offset": 502}, {"id": 1093373, "label": "ENT", "start_offset": 543, "end_offset": 568}, {"id": 1093374, "label": "ENT", "start_offset": 581, "end_offset": 590}, {"id": 1093375, "label": "ENT", "start_offset": 593, "end_offset": 599}, {"id": 1093376, "label": "ENT", "start_offset": 671, "end_offset": 688}, {"id": 1093377, "label": "ENT", "start_offset": 692, "end_offset": 709}, {"id": 1093378, "label": "ENT", "start_offset": 713, "end_offset": 725}, {"id": 1093379, "label": "ENT", "start_offset": 752, "end_offset": 757}, {"id": 1093380, "label": "ENT", "start_offset": 767, "end_offset": 786}, {"id": 1093381, "label": "ENT", "start_offset": 790, "end_offset": 804}, {"id": 1093382, "label": "ENT", "start_offset": 826, "end_offset": 865}, {"id": 1093383, "label": "ENT", "start_offset": 869, "end_offset": 874}, {"id": 1093384, "label": "ENT", "start_offset": 876, "end_offset": 895}, {"id": 1093385, "label": "ENT", "start_offset": 903, "end_offset": 921}], "relations": [{"id": 176480, "from_id": 1093368, "to_id": 1093369, "type": "USED-FOR"}, {"id": 176481, "from_id": 1093375, "to_id": 1093376, "type": "USED-FOR"}, {"id": 176482, "from_id": 1093364, "to_id": 1093363, "type": "COREF"}, {"id": 176483, "from_id": 1093365, "to_id": 1093364, "type": "PART-OF"}, {"id": 176484, "from_id": 1093366, "to_id": 1093364, "type": "PART-OF"}, {"id": 176485, "from_id": 1093364, "to_id": 1093368, "type": "USED-FOR"}, {"id": 176486, "from_id": 1093367, "to_id": 1093364, "type": "PART-OF"}, {"id": 176487, "from_id": 1093370, "to_id": 1093364, "type": "COREF"}, {"id": 176488, "from_id": 1093372, "to_id": 1093368, "type": "COREF"}, {"id": 176489, "from_id": 1093373, "to_id": 1093372, "type": "USED-FOR"}, {"id": 176490, "from_id": 1093374, "to_id": 1093373, "type": "HYPONYM-OF"}, {"id": 176491, "from_id": 1093375, "to_id": 1093370, "type": "COREF"}, {"id": 176492, "from_id": 1093375, "to_id": 1093377, "type": "USED-FOR"}, {"id": 176493, "from_id": 1093375, "to_id": 1093378, "type": "USED-FOR"}, {"id": 176494, "from_id": 1093376, "to_id": 1093377, "type": "CONJUNCTION"}, {"id": 176495, "from_id": 1093377, "to_id": 1093378, "type": "CONJUNCTION"}, {"id": 176496, "from_id": 1093380, "to_id": 1093379, "type": "HYPONYM-OF"}, {"id": 176497, "from_id": 1093381, "to_id": 1093379, "type": "HYPONYM-OF"}, {"id": 176498, "from_id": 1093382, "to_id": 1093375, "type": "USED-FOR"}, {"id": 176499, "from_id": 1093385, "to_id": 1093375, "type": "USED-FOR"}, {"id": 176500, "from_id": 1093383, "to_id": 1093375, "type": "USED-FOR"}, {"id": 176501, "from_id": 1093384, "to_id": 1093375, "type": "USED-FOR"}, {"id": 176502, "from_id": 1093382, "to_id": 1093383, "type": "CONJUNCTION"}, {"id": 176503, "from_id": 1093383, "to_id": 1093384, "type": "CONJUNCTION"}, {"id": 176504, "from_id": 1093384, "to_id": 1093385, "type": "CONJUNCTION"}, {"id": 176505, "from_id": 1093380, "to_id": 1093381, "type": "CONJUNCTION"}, {"id": 176506, "from_id": 1093370, "to_id": 1093371, "type": "USED-FOR"}, {"id": 176507, "from_id": 1093379, "to_id": 1093378, "type": "FEATURE-OF"}]}
{"id": "P05-1057", "text": " We present a framework for  word alignment  based on  log-linear models . All  knowledge sources  are treated as  feature functions , which depend on the  source langauge sentence , the  target language sentence  and possible additional variables.  Log-linear models  allow  statistical alignment models  to be easily extended by incorporating  syntactic information . In this paper, we use  IBM Model 3 alignment probabilities ,  POS correspondence , and  bilingual dictionary coverage  as  features . Our experiments show that  log-linear models  significantly outperform  IBM translation models . ", "Comments": [], "entities": [{"id": 1093398, "label": "ENT", "start_offset": 14, "end_offset": 23}, {"id": 1093399, "label": "ENT", "start_offset": 29, "end_offset": 43}, {"id": 1093400, "label": "ENT", "start_offset": 55, "end_offset": 72}, {"id": 1093401, "label": "ENT", "start_offset": 80, "end_offset": 97}, {"id": 1093402, "label": "ENT", "start_offset": 115, "end_offset": 132}, {"id": 1093403, "label": "ENT", "start_offset": 250, "end_offset": 267}, {"id": 1093404, "label": "ENT", "start_offset": 276, "end_offset": 304}, {"id": 1093405, "label": "ENT", "start_offset": 346, "end_offset": 367}, {"id": 1093406, "label": "ENT", "start_offset": 393, "end_offset": 428}, {"id": 1093407, "label": "ENT", "start_offset": 432, "end_offset": 450}, {"id": 1093408, "label": "ENT", "start_offset": 458, "end_offset": 487}, {"id": 1093409, "label": "ENT", "start_offset": 493, "end_offset": 501}, {"id": 1093410, "label": "ENT", "start_offset": 531, "end_offset": 548}, {"id": 1093411, "label": "ENT", "start_offset": 576, "end_offset": 598}], "relations": [{"id": 176517, "from_id": 1093401, "to_id": 1093402, "type": "USED-FOR"}, {"id": 176518, "from_id": 1093403, "to_id": 1093404, "type": "USED-FOR"}, {"id": 176519, "from_id": 1093408, "to_id": 1093409, "type": "USED-FOR"}, {"id": 176520, "from_id": 1093410, "to_id": 1093411, "type": "COMPARE"}, {"id": 176521, "from_id": 1093398, "to_id": 1093399, "type": "USED-FOR"}, {"id": 176522, "from_id": 1093400, "to_id": 1093398, "type": "USED-FOR"}, {"id": 176523, "from_id": 1093405, "to_id": 1093403, "type": "USED-FOR"}, {"id": 176524, "from_id": 1093407, "to_id": 1093409, "type": "USED-FOR"}, {"id": 176525, "from_id": 1093406, "to_id": 1093409, "type": "USED-FOR"}, {"id": 176526, "from_id": 1093406, "to_id": 1093407, "type": "CONJUNCTION"}, {"id": 176527, "from_id": 1093407, "to_id": 1093408, "type": "CONJUNCTION"}, {"id": 176528, "from_id": 1093403, "to_id": 1093410, "type": "COREF"}]}
{"id": "INTERSPEECH_2015_9_abs", "text": "We describe the use of text data scraped from the web to augment language models for Automatic Speech Recognition and Keyword Search for Low Resource Languages. We scrape text from multiple genres including blogs, online news, translated TED talks, and subtitles. Using linearly interpolated language models, we find that blogs and movie subtitles are more relevant for language modeling of conversational telephone speech and obtain large reductions in out-of-vocabulary keywords. Furthermore, we show that the web data can improve Term Error Rate Performance by 3.8% absolute and Maximum Term-Weighted Value in Keyword Search by 0.0076-0.1059 absolute points. Much of the gain comes from the reduction of out-of-vocabulary items.", "Comments": [], "entities": [{"id": 1093412, "label": "ENT", "start_offset": 23, "end_offset": 32}, {"id": 1093413, "label": "ENT", "start_offset": 50, "end_offset": 53}, {"id": 1093414, "label": "ENT", "start_offset": 65, "end_offset": 80}, {"id": 1093415, "label": "ENT", "start_offset": 85, "end_offset": 113}, {"id": 1093416, "label": "ENT", "start_offset": 118, "end_offset": 132}, {"id": 1093417, "label": "ENT", "start_offset": 137, "end_offset": 159}, {"id": 1093418, "label": "ENT", "start_offset": 190, "end_offset": 196}, {"id": 1093419, "label": "ENT", "start_offset": 207, "end_offset": 212}, {"id": 1093420, "label": "ENT", "start_offset": 214, "end_offset": 225}, {"id": 1093421, "label": "ENT", "start_offset": 227, "end_offset": 247}, {"id": 1093422, "label": "ENT", "start_offset": 253, "end_offset": 262}, {"id": 1093423, "label": "ENT", "start_offset": 270, "end_offset": 307}, {"id": 1093424, "label": "ENT", "start_offset": 322, "end_offset": 327}, {"id": 1093425, "label": "ENT", "start_offset": 332, "end_offset": 347}, {"id": 1093426, "label": "ENT", "start_offset": 370, "end_offset": 422}, {"id": 1093427, "label": "ENT", "start_offset": 454, "end_offset": 480}, {"id": 1093428, "label": "ENT", "start_offset": 512, "end_offset": 520}, {"id": 1093429, "label": "ENT", "start_offset": 533, "end_offset": 560}, {"id": 1093430, "label": "ENT", "start_offset": 582, "end_offset": 609}, {"id": 1093431, "label": "ENT", "start_offset": 613, "end_offset": 627}, {"id": 1093432, "label": "ENT", "start_offset": 694, "end_offset": 730}], "relations": [{"id": 176529, "from_id": 1093414, "to_id": 1093415, "type": "USED-FOR"}, {"id": 176530, "from_id": 1093417, "to_id": 1093416, "type": "USED-FOR"}, {"id": 176531, "from_id": 1093413, "to_id": 1093412, "type": "FEATURE-OF"}, {"id": 176532, "from_id": 1093412, "to_id": 1093414, "type": "USED-FOR"}, {"id": 176533, "from_id": 1093419, "to_id": 1093418, "type": "HYPONYM-OF"}, {"id": 176534, "from_id": 1093420, "to_id": 1093418, "type": "HYPONYM-OF"}, {"id": 176535, "from_id": 1093421, "to_id": 1093418, "type": "HYPONYM-OF"}, {"id": 176536, "from_id": 1093422, "to_id": 1093418, "type": "HYPONYM-OF"}, {"id": 176537, "from_id": 1093419, "to_id": 1093420, "type": "CONJUNCTION"}, {"id": 176538, "from_id": 1093420, "to_id": 1093421, "type": "CONJUNCTION"}, {"id": 176539, "from_id": 1093421, "to_id": 1093422, "type": "CONJUNCTION"}, {"id": 176540, "from_id": 1093425, "to_id": 1093426, "type": "USED-FOR"}, {"id": 176541, "from_id": 1093424, "to_id": 1093426, "type": "USED-FOR"}, {"id": 176542, "from_id": 1093424, "to_id": 1093425, "type": "CONJUNCTION"}, {"id": 176543, "from_id": 1093430, "to_id": 1093431, "type": "EVALUATE-FOR"}, {"id": 176544, "from_id": 1093419, "to_id": 1093424, "type": "COREF"}, {"id": 176545, "from_id": 1093422, "to_id": 1093425, "type": "COREF"}, {"id": 176546, "from_id": 1093415, "to_id": 1093416, "type": "CONJUNCTION"}, {"id": 176547, "from_id": 1093428, "to_id": 1093431, "type": "USED-FOR"}, {"id": 176548, "from_id": 1093429, "to_id": 1093431, "type": "EVALUATE-FOR"}, {"id": 176549, "from_id": 1093414, "to_id": 1093416, "type": "USED-FOR"}, {"id": 176550, "from_id": 1093417, "to_id": 1093415, "type": "USED-FOR"}, {"id": 176551, "from_id": 1093423, "to_id": 1093426, "type": "USED-FOR"}]}
{"id": "I08-1043", "text": " This study presents a  method to automatically acquire paraphrases  using  bilingual corpora , which utilizes the  bilingual dependency relations  obtained by projecting a  monolingual dependency parse  onto the other language sentence based on  statistical alignment techniques . Since the  paraphrasing method  is capable of clearly disambiguating the  sense  of an original  phrase  using the  bilingual context  of  dependency relation , it would be possible to obtain interchangeable  paraphrases  under a given  context . Also, we provide an advanced method to acquire  generalized translation knowledge  using the extracted  paraphrases . We applied the method to acquire the  generalized translation knowledge  for  Korean-English translation . Through experiments with  parallel corpora  of a  Korean and English language pairs , we show that our  paraphrasing method  effectively extracts  paraphrases  with high  precision , 94.3% and 84.6% respectively for  Korean  and  English , and the  translation knowledge  extracted from the  bilingual corpora  could be generalized successfully using the  paraphrases  with the 12.5%  compression ratio . ", "Comments": [], "entities": [{"id": 1093468, "label": "ENT", "start_offset": 24, "end_offset": 67}, {"id": 1093469, "label": "ENT", "start_offset": 76, "end_offset": 93}, {"id": 1093470, "label": "ENT", "start_offset": 116, "end_offset": 146}, {"id": 1093471, "label": "ENT", "start_offset": 174, "end_offset": 202}, {"id": 1093472, "label": "ENT", "start_offset": 247, "end_offset": 279}, {"id": 1093473, "label": "ENT", "start_offset": 293, "end_offset": 312}, {"id": 1093474, "label": "ENT", "start_offset": 398, "end_offset": 440}, {"id": 1093475, "label": "ENT", "start_offset": 491, "end_offset": 502}, {"id": 1093476, "label": "ENT", "start_offset": 558, "end_offset": 564}, {"id": 1093477, "label": "ENT", "start_offset": 577, "end_offset": 610}, {"id": 1093478, "label": "ENT", "start_offset": 633, "end_offset": 644}, {"id": 1093479, "label": "ENT", "start_offset": 662, "end_offset": 668}, {"id": 1093480, "label": "ENT", "start_offset": 685, "end_offset": 718}, {"id": 1093481, "label": "ENT", "start_offset": 725, "end_offset": 751}, {"id": 1093482, "label": "ENT", "start_offset": 780, "end_offset": 837}, {"id": 1093483, "label": "ENT", "start_offset": 858, "end_offset": 877}, {"id": 1093484, "label": "ENT", "start_offset": 901, "end_offset": 912}, {"id": 1093485, "label": "ENT", "start_offset": 925, "end_offset": 934}, {"id": 1093486, "label": "ENT", "start_offset": 971, "end_offset": 977}, {"id": 1093487, "label": "ENT", "start_offset": 984, "end_offset": 991}, {"id": 1093488, "label": "ENT", "start_offset": 1003, "end_offset": 1024}, {"id": 1093489, "label": "ENT", "start_offset": 1046, "end_offset": 1063}, {"id": 1093490, "label": "ENT", "start_offset": 1110, "end_offset": 1121}, {"id": 1093491, "label": "ENT", "start_offset": 1139, "end_offset": 1156}], "relations": [{"id": 176571, "from_id": 1093469, "to_id": 1093468, "type": "USED-FOR"}, {"id": 176572, "from_id": 1093472, "to_id": 1093470, "type": "USED-FOR"}, {"id": 176573, "from_id": 1093473, "to_id": 1093468, "type": "COREF"}, {"id": 176574, "from_id": 1093470, "to_id": 1093468, "type": "USED-FOR"}, {"id": 176575, "from_id": 1093471, "to_id": 1093470, "type": "USED-FOR"}, {"id": 176576, "from_id": 1093476, "to_id": 1093477, "type": "USED-FOR"}, {"id": 176577, "from_id": 1093479, "to_id": 1093476, "type": "COREF"}, {"id": 176578, "from_id": 1093479, "to_id": 1093480, "type": "USED-FOR"}, {"id": 176579, "from_id": 1093480, "to_id": 1093481, "type": "USED-FOR"}, {"id": 176580, "from_id": 1093483, "to_id": 1093473, "type": "COREF"}, {"id": 176581, "from_id": 1093489, "to_id": 1093488, "type": "USED-FOR"}, {"id": 176582, "from_id": 1093489, "to_id": 1093469, "type": "COREF"}, {"id": 176583, "from_id": 1093490, "to_id": 1093484, "type": "COREF"}, {"id": 176584, "from_id": 1093474, "to_id": 1093473, "type": "USED-FOR"}, {"id": 176585, "from_id": 1093478, "to_id": 1093476, "type": "USED-FOR"}, {"id": 176586, "from_id": 1093482, "to_id": 1093489, "type": "COREF"}, {"id": 176587, "from_id": 1093485, "to_id": 1093483, "type": "EVALUATE-FOR"}, {"id": 176588, "from_id": 1093483, "to_id": 1093484, "type": "USED-FOR"}, {"id": 176589, "from_id": 1093486, "to_id": 1093487, "type": "CONJUNCTION"}, {"id": 176590, "from_id": 1093490, "to_id": 1093488, "type": "USED-FOR"}, {"id": 176591, "from_id": 1093491, "to_id": 1093488, "type": "EVALUATE-FOR"}]}
{"id": "H89-2066", "text": " The goal of this research is to develop a  spoken language system  that will demonstrate the usefulness of  voice input  for  interactive problem solving . The system will accept  continuous speech , and will handle  multiple speakers  without  explicit speaker enrollment . Combining  speech recognition  and  natural language processing  to achieve  speech understanding , the system will be demonstrated in an  application domain  relevant to the DoD. The objective of this project is to develop a  robust and high-performance speech recognition system  using a  segment-based approach  to  phonetic recognition . The  recognition system  will eventually be integrated with  natural language processing  to achieve  spoken language understanding . ", "Comments": [], "entities": [{"id": 1093492, "label": "ENT", "start_offset": 44, "end_offset": 66}, {"id": 1093493, "label": "ENT", "start_offset": 109, "end_offset": 120}, {"id": 1093494, "label": "ENT", "start_offset": 127, "end_offset": 154}, {"id": 1093495, "label": "ENT", "start_offset": 181, "end_offset": 198}, {"id": 1093496, "label": "ENT", "start_offset": 287, "end_offset": 305}, {"id": 1093497, "label": "ENT", "start_offset": 312, "end_offset": 339}, {"id": 1093498, "label": "ENT", "start_offset": 353, "end_offset": 373}, {"id": 1093499, "label": "ENT", "start_offset": 380, "end_offset": 386}, {"id": 1093500, "label": "ENT", "start_offset": 503, "end_offset": 556}, {"id": 1093501, "label": "ENT", "start_offset": 567, "end_offset": 589}, {"id": 1093502, "label": "ENT", "start_offset": 595, "end_offset": 615}, {"id": 1093503, "label": "ENT", "start_offset": 623, "end_offset": 641}, {"id": 1093504, "label": "ENT", "start_offset": 679, "end_offset": 706}, {"id": 1093505, "label": "ENT", "start_offset": 720, "end_offset": 749}], "relations": [{"id": 176592, "from_id": 1093493, "to_id": 1093494, "type": "USED-FOR"}, {"id": 176593, "from_id": 1093497, "to_id": 1093498, "type": "USED-FOR"}, {"id": 176594, "from_id": 1093501, "to_id": 1093502, "type": "USED-FOR"}, {"id": 176595, "from_id": 1093504, "to_id": 1093505, "type": "USED-FOR"}, {"id": 176596, "from_id": 1093492, "to_id": 1093494, "type": "USED-FOR"}, {"id": 176597, "from_id": 1093496, "to_id": 1093497, "type": "CONJUNCTION"}, {"id": 176598, "from_id": 1093499, "to_id": 1093492, "type": "COREF"}, {"id": 176599, "from_id": 1093500, "to_id": 1093492, "type": "COREF"}, {"id": 176600, "from_id": 1093501, "to_id": 1093500, "type": "USED-FOR"}, {"id": 176601, "from_id": 1093503, "to_id": 1093500, "type": "COREF"}, {"id": 176602, "from_id": 1093504, "to_id": 1093503, "type": "CONJUNCTION"}, {"id": 176603, "from_id": 1093503, "to_id": 1093505, "type": "USED-FOR"}, {"id": 176604, "from_id": 1093505, "to_id": 1093498, "type": "COREF"}, {"id": 176605, "from_id": 1093504, "to_id": 1093497, "type": "COREF"}, {"id": 176606, "from_id": 1093496, "to_id": 1093498, "type": "USED-FOR"}, {"id": 176607, "from_id": 1093502, "to_id": 1093500, "type": "USED-FOR"}]}
{"id": "H90-1011", "text": " This paper describes a particular approach to  parsing  that utilizes recent advances in  unification-based parsing  and in  classification-based knowledge representation . As  unification-based grammatical frameworks  are extended to handle richer descriptions of  linguistic information , they begin to share many of the properties that have been developed in  KL-ONE-like knowledge representation systems . This commonality suggests that some of the  classification-based representation techniques  can be applied to  unification-based linguistic descriptions . This merging supports the integration of  semantic and syntactic information  into the same system, simultaneously subject to the same types of processes, in an efficient manner. The result is expected to be more  efficient parsing  due to the increased organization of knowledge. The use of a  KL-ONE style representation  for  parsing  and  semantic interpretation  was first explored in the  PSI-KLONE system  [2], in which  parsing  is characterized as an inference process called  incremental description refinement . ", "Comments": [], "entities": [{"id": 1093539, "label": "ENT", "start_offset": 35, "end_offset": 43}, {"id": 1093540, "label": "ENT", "start_offset": 48, "end_offset": 55}, {"id": 1093541, "label": "ENT", "start_offset": 91, "end_offset": 116}, {"id": 1093542, "label": "ENT", "start_offset": 126, "end_offset": 171}, {"id": 1093543, "label": "ENT", "start_offset": 178, "end_offset": 218}, {"id": 1093544, "label": "ENT", "start_offset": 267, "end_offset": 289}, {"id": 1093545, "label": "ENT", "start_offset": 292, "end_offset": 296}, {"id": 1093546, "label": "ENT", "start_offset": 364, "end_offset": 408}, {"id": 1093547, "label": "ENT", "start_offset": 455, "end_offset": 501}, {"id": 1093548, "label": "ENT", "start_offset": 522, "end_offset": 563}, {"id": 1093549, "label": "ENT", "start_offset": 608, "end_offset": 642}, {"id": 1093550, "label": "ENT", "start_offset": 658, "end_offset": 664}, {"id": 1093551, "label": "ENT", "start_offset": 790, "end_offset": 797}, {"id": 1093552, "label": "ENT", "start_offset": 861, "end_offset": 888}, {"id": 1093553, "label": "ENT", "start_offset": 895, "end_offset": 902}, {"id": 1093554, "label": "ENT", "start_offset": 909, "end_offset": 932}, {"id": 1093555, "label": "ENT", "start_offset": 961, "end_offset": 977}, {"id": 1093556, "label": "ENT", "start_offset": 994, "end_offset": 1001}, {"id": 1093557, "label": "ENT", "start_offset": 1026, "end_offset": 1043}, {"id": 1093558, "label": "ENT", "start_offset": 1052, "end_offset": 1086}], "relations": [{"id": 176629, "from_id": 1093547, "to_id": 1093548, "type": "USED-FOR"}, {"id": 176630, "from_id": 1093552, "to_id": 1093553, "type": "USED-FOR"}, {"id": 176631, "from_id": 1093539, "to_id": 1093540, "type": "USED-FOR"}, {"id": 176632, "from_id": 1093541, "to_id": 1093539, "type": "USED-FOR"}, {"id": 176633, "from_id": 1093542, "to_id": 1093541, "type": "CONJUNCTION"}, {"id": 176634, "from_id": 1093542, "to_id": 1093539, "type": "USED-FOR"}, {"id": 176635, "from_id": 1093543, "to_id": 1093544, "type": "USED-FOR"}, {"id": 176636, "from_id": 1093545, "to_id": 1093543, "type": "COREF"}, {"id": 176637, "from_id": 1093546, "to_id": 1093545, "type": "USED-FOR"}, {"id": 176638, "from_id": 1093549, "to_id": 1093550, "type": "USED-FOR"}, {"id": 176639, "from_id": 1093552, "to_id": 1093554, "type": "USED-FOR"}, {"id": 176640, "from_id": 1093553, "to_id": 1093554, "type": "CONJUNCTION"}, {"id": 176641, "from_id": 1093555, "to_id": 1093552, "type": "USED-FOR"}, {"id": 176642, "from_id": 1093556, "to_id": 1093553, "type": "COREF"}, {"id": 176643, "from_id": 1093558, "to_id": 1093557, "type": "HYPONYM-OF"}, {"id": 176644, "from_id": 1093552, "to_id": 1093546, "type": "COREF"}, {"id": 176645, "from_id": 1093553, "to_id": 1093551, "type": "COREF"}, {"id": 176646, "from_id": 1093550, "to_id": 1093539, "type": "COREF"}, {"id": 176647, "from_id": 1093558, "to_id": 1093556, "type": "USED-FOR"}]}
{"id": "W07-0208", "text": " We argue in favor of the the use of  labeled directed graph  to represent various types of  linguistic structures , and illustrate how this allows one to view  NLP tasks  as  graph transformations . We present a general method for learning such  transformations  from an  annotated corpus  and describe experiments with two applications of the method:  identification of non-local depenencies  (using  Penn Treebank data ) and  semantic role labeling  (using  Proposition Bank data ). ", "Comments": [], "entities": [{"id": 1093559, "label": "ENT", "start_offset": 38, "end_offset": 60}, {"id": 1093560, "label": "ENT", "start_offset": 93, "end_offset": 114}, {"id": 1093561, "label": "ENT", "start_offset": 136, "end_offset": 140}, {"id": 1093562, "label": "ENT", "start_offset": 161, "end_offset": 170}, {"id": 1093563, "label": "ENT", "start_offset": 176, "end_offset": 197}, {"id": 1093564, "label": "ENT", "start_offset": 221, "end_offset": 227}, {"id": 1093565, "label": "ENT", "start_offset": 247, "end_offset": 262}, {"id": 1093566, "label": "ENT", "start_offset": 273, "end_offset": 289}, {"id": 1093567, "label": "ENT", "start_offset": 325, "end_offset": 337}, {"id": 1093568, "label": "ENT", "start_offset": 345, "end_offset": 351}, {"id": 1093569, "label": "ENT", "start_offset": 354, "end_offset": 393}, {"id": 1093570, "label": "ENT", "start_offset": 403, "end_offset": 421}, {"id": 1093571, "label": "ENT", "start_offset": 429, "end_offset": 451}, {"id": 1093572, "label": "ENT", "start_offset": 461, "end_offset": 482}], "relations": [{"id": 176648, "from_id": 1093570, "to_id": 1093569, "type": "USED-FOR"}, {"id": 176649, "from_id": 1093572, "to_id": 1093571, "type": "USED-FOR"}, {"id": 176650, "from_id": 1093559, "to_id": 1093560, "type": "USED-FOR"}, {"id": 176651, "from_id": 1093559, "to_id": 1093562, "type": "USED-FOR"}, {"id": 176652, "from_id": 1093561, "to_id": 1093559, "type": "COREF"}, {"id": 176653, "from_id": 1093561, "to_id": 1093562, "type": "USED-FOR"}, {"id": 176654, "from_id": 1093564, "to_id": 1093565, "type": "USED-FOR"}, {"id": 176655, "from_id": 1093565, "to_id": 1093563, "type": "COREF"}, {"id": 176656, "from_id": 1093566, "to_id": 1093564, "type": "USED-FOR"}, {"id": 176657, "from_id": 1093568, "to_id": 1093564, "type": "COREF"}, {"id": 176658, "from_id": 1093569, "to_id": 1093567, "type": "HYPONYM-OF"}, {"id": 176659, "from_id": 1093571, "to_id": 1093567, "type": "HYPONYM-OF"}, {"id": 176660, "from_id": 1093568, "to_id": 1093567, "type": "USED-FOR"}]}
{"id": "H01-1042", "text": " The purpose of this research is to test the efficacy of applying  automated evaluation techniques  , originally devised for the evaluation of  human language learners  , to the  output  of  machine translation (MT) systems  . We believe that these  evaluation techniques  will provide information about both the  human language learning process  , the  translation process  and the  development  of  machine translation systems  . This, the first experiment in a series of experiments, looks at the  intelligibility  of  MT output  . A  language learning experiment  showed that  assessors  can differentiate  native from non-native language essays  in less than 100  words  . Even more illuminating was the factors on which the  assessors  made their decisions. We tested this to see if similar criteria could be elicited from duplicating the experiment using  machine translation output  . Subjects were given a set of up to six extracts of  translated newswire text  . Some of the extracts were  expert human translations  , others were  machine translation outputs  . The subjects were given three minutes per extract to determine whether they believed the sample output to be an  expert human translation  or a  machine translation  . Additionally, they were asked to mark the  word  at which they made this decision. The results of this experiment, along with a preliminary analysis of the factors involved in the decision making process will be presented here. ", "Comments": [], "entities": [{"id": 1093584, "label": "ENT", "start_offset": 67, "end_offset": 98}, {"id": 1093585, "label": "ENT", "start_offset": 129, "end_offset": 167}, {"id": 1093586, "label": "ENT", "start_offset": 191, "end_offset": 223}, {"id": 1093587, "label": "ENT", "start_offset": 250, "end_offset": 271}, {"id": 1093588, "label": "ENT", "start_offset": 314, "end_offset": 345}, {"id": 1093589, "label": "ENT", "start_offset": 354, "end_offset": 373}, {"id": 1093590, "label": "ENT", "start_offset": 401, "end_offset": 428}, {"id": 1093591, "label": "ENT", "start_offset": 538, "end_offset": 555}, {"id": 1093592, "label": "ENT", "start_offset": 581, "end_offset": 590}, {"id": 1093593, "label": "ENT", "start_offset": 623, "end_offset": 649}, {"id": 1093594, "label": "ENT", "start_offset": 731, "end_offset": 740}, {"id": 1093595, "label": "ENT", "start_offset": 863, "end_offset": 889}, {"id": 1093596, "label": "ENT", "start_offset": 945, "end_offset": 969}, {"id": 1093597, "label": "ENT", "start_offset": 1000, "end_offset": 1025}, {"id": 1093598, "label": "ENT", "start_offset": 1042, "end_offset": 1069}, {"id": 1093599, "label": "ENT", "start_offset": 1186, "end_offset": 1210}, {"id": 1093600, "label": "ENT", "start_offset": 1218, "end_offset": 1237}], "relations": [{"id": 176671, "from_id": 1093587, "to_id": 1093584, "type": "COREF"}, {"id": 176672, "from_id": 1093587, "to_id": 1093588, "type": "USED-FOR"}, {"id": 176673, "from_id": 1093587, "to_id": 1093589, "type": "USED-FOR"}, {"id": 176674, "from_id": 1093588, "to_id": 1093589, "type": "CONJUNCTION"}, {"id": 176675, "from_id": 1093598, "to_id": 1093597, "type": "CONJUNCTION"}, {"id": 176676, "from_id": 1093590, "to_id": 1093586, "type": "COREF"}, {"id": 176677, "from_id": 1093584, "to_id": 1093585, "type": "USED-FOR"}, {"id": 176678, "from_id": 1093587, "to_id": 1093590, "type": "USED-FOR"}, {"id": 176679, "from_id": 1093589, "to_id": 1093590, "type": "CONJUNCTION"}, {"id": 176680, "from_id": 1093592, "to_id": 1093587, "type": "COREF"}, {"id": 176681, "from_id": 1093594, "to_id": 1093592, "type": "COREF"}, {"id": 176682, "from_id": 1093599, "to_id": 1093600, "type": "COMPARE"}, {"id": 176683, "from_id": 1093591, "to_id": 1093592, "type": "EVALUATE-FOR"}]}
{"id": "ICCV_2013_47_abs", "text": "Light fields are image-based representations that use densely sampled rays as a scene description. In this paper, we explore geometric structures of 3D lines in ray space for improving light field triangulation and stereo matching. The triangulation problem aims to fill in the ray space with continuous and non-overlapping simplices anchored at sampled points (rays). Such a triangulation provides a piecewise-linear interpolant useful for light field super-resolution. We show that the light field space is largely bi-linear due to 3D line segments in the scene, and direct tri-angulation of these bilinear subspaces leads to large errors. We instead present a simple but effective algorithm to first map bilinear subspaces to line constraints and then apply Constrained Delaunay Triangulation (CDT). Based on our analysis, we further develop a novel line-assisted graph-cut (LAGC) algorithm that effectively encodes 3D line constraints into light field stereo matching. Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform state-of-the-art solutions in accuracy and visual quality.", "Comments": [], "entities": [{"id": 1093601, "label": "ENT", "start_offset": 0, "end_offset": 12}, {"id": 1093602, "label": "ENT", "start_offset": 17, "end_offset": 44}, {"id": 1093603, "label": "ENT", "start_offset": 54, "end_offset": 74}, {"id": 1093604, "label": "ENT", "start_offset": 80, "end_offset": 97}, {"id": 1093605, "label": "ENT", "start_offset": 125, "end_offset": 157}, {"id": 1093606, "label": "ENT", "start_offset": 161, "end_offset": 170}, {"id": 1093607, "label": "ENT", "start_offset": 185, "end_offset": 210}, {"id": 1093608, "label": "ENT", "start_offset": 215, "end_offset": 230}, {"id": 1093609, "label": "ENT", "start_offset": 236, "end_offset": 257}, {"id": 1093610, "label": "ENT", "start_offset": 278, "end_offset": 287}, {"id": 1093611, "label": "ENT", "start_offset": 293, "end_offset": 333}, {"id": 1093612, "label": "ENT", "start_offset": 376, "end_offset": 389}, {"id": 1093613, "label": "ENT", "start_offset": 401, "end_offset": 429}, {"id": 1093614, "label": "ENT", "start_offset": 441, "end_offset": 469}, {"id": 1093615, "label": "ENT", "start_offset": 488, "end_offset": 505}, {"id": 1093616, "label": "ENT", "start_offset": 534, "end_offset": 550}, {"id": 1093617, "label": "ENT", "start_offset": 600, "end_offset": 618}, {"id": 1093618, "label": "ENT", "start_offset": 707, "end_offset": 725}, {"id": 1093619, "label": "ENT", "start_offset": 729, "end_offset": 745}, {"id": 1093620, "label": "ENT", "start_offset": 761, "end_offset": 801}, {"id": 1093621, "label": "ENT", "start_offset": 853, "end_offset": 893}, {"id": 1093622, "label": "ENT", "start_offset": 919, "end_offset": 938}, {"id": 1093623, "label": "ENT", "start_offset": 944, "end_offset": 971}, {"id": 1093624, "label": "ENT", "start_offset": 988, "end_offset": 1011}, {"id": 1093625, "label": "ENT", "start_offset": 1031, "end_offset": 1064}, {"id": 1093626, "label": "ENT", "start_offset": 1076, "end_offset": 1102}, {"id": 1093627, "label": "ENT", "start_offset": 1106, "end_offset": 1114}, {"id": 1093628, "label": "ENT", "start_offset": 1119, "end_offset": 1133}], "relations": [{"id": 176684, "from_id": 1093606, "to_id": 1093605, "type": "FEATURE-OF"}, {"id": 176685, "from_id": 1093605, "to_id": 1093607, "type": "USED-FOR"}, {"id": 176686, "from_id": 1093607, "to_id": 1093608, "type": "CONJUNCTION"}, {"id": 176687, "from_id": 1093605, "to_id": 1093608, "type": "USED-FOR"}, {"id": 176688, "from_id": 1093607, "to_id": 1093609, "type": "COREF"}, {"id": 176689, "from_id": 1093613, "to_id": 1093614, "type": "USED-FOR"}, {"id": 176690, "from_id": 1093612, "to_id": 1093613, "type": "USED-FOR"}, {"id": 176691, "from_id": 1093609, "to_id": 1093612, "type": "COREF"}, {"id": 176692, "from_id": 1093625, "to_id": 1093626, "type": "COMPARE"}, {"id": 176693, "from_id": 1093627, "to_id": 1093625, "type": "EVALUATE-FOR"}, {"id": 176694, "from_id": 1093628, "to_id": 1093625, "type": "EVALUATE-FOR"}, {"id": 176695, "from_id": 1093627, "to_id": 1093626, "type": "EVALUATE-FOR"}, {"id": 176696, "from_id": 1093628, "to_id": 1093626, "type": "EVALUATE-FOR"}, {"id": 176697, "from_id": 1093624, "to_id": 1093625, "type": "EVALUATE-FOR"}, {"id": 176698, "from_id": 1093624, "to_id": 1093626, "type": "EVALUATE-FOR"}, {"id": 176699, "from_id": 1093621, "to_id": 1093625, "type": "HYPONYM-OF"}, {"id": 176700, "from_id": 1093620, "to_id": 1093625, "type": "HYPONYM-OF"}]}
{"id": "ICASSP_2011_598_abs", "text": "In this paper we evaluate four objective measures of speech with regards to intelligibility prediction of synthesized speech in diverse noisy situations. We evaluated three intel-ligibility measures, the Dau measure, the glimpse proportion and the Speech Intelligibility Index (SII) and a quality measure , the Perceptual Evaluation of Speech Quality (PESQ). For the generation of synthesized speech we used a state of the art HMM-based speech synthesis system. The noisy conditions comprised four additive noises. The measures were compared with subjective intelligibility scores obtained in listening tests. The results show the Dau and the glimpse measures to be the best predictors of intelligibility, with correlations of around 0.83 to subjective scores. All measures gave less accurate predictions of intelligibility for synthetic speech than have previously been found for natural speech; in particular the SII measure. In additional experiments, we processed the synthesized speech by an ideal binary mask before adding noise. The Glimpse measure gave the most accurate intelligibility predictions in this situation.", "Comments": [], "entities": [{"id": 1093629, "label": "ENT", "start_offset": 41, "end_offset": 59}, {"id": 1093630, "label": "ENT", "start_offset": 76, "end_offset": 102}, {"id": 1093631, "label": "ENT", "start_offset": 106, "end_offset": 124}, {"id": 1093632, "label": "ENT", "start_offset": 128, "end_offset": 152}, {"id": 1093633, "label": "ENT", "start_offset": 173, "end_offset": 198}, {"id": 1093634, "label": "ENT", "start_offset": 204, "end_offset": 215}, {"id": 1093635, "label": "ENT", "start_offset": 221, "end_offset": 239}, {"id": 1093636, "label": "ENT", "start_offset": 248, "end_offset": 282}, {"id": 1093637, "label": "ENT", "start_offset": 289, "end_offset": 304}, {"id": 1093638, "label": "ENT", "start_offset": 311, "end_offset": 357}, {"id": 1093639, "label": "ENT", "start_offset": 367, "end_offset": 399}, {"id": 1093640, "label": "ENT", "start_offset": 427, "end_offset": 460}, {"id": 1093641, "label": "ENT", "start_offset": 466, "end_offset": 482}, {"id": 1093642, "label": "ENT", "start_offset": 498, "end_offset": 513}, {"id": 1093643, "label": "ENT", "start_offset": 519, "end_offset": 527}, {"id": 1093644, "label": "ENT", "start_offset": 547, "end_offset": 580}, {"id": 1093645, "label": "ENT", "start_offset": 631, "end_offset": 634}, {"id": 1093646, "label": "ENT", "start_offset": 643, "end_offset": 659}, {"id": 1093647, "label": "ENT", "start_offset": 675, "end_offset": 704}, {"id": 1093648, "label": "ENT", "start_offset": 711, "end_offset": 723}, {"id": 1093649, "label": "ENT", "start_offset": 742, "end_offset": 759}, {"id": 1093650, "label": "ENT", "start_offset": 765, "end_offset": 773}, {"id": 1093651, "label": "ENT", "start_offset": 793, "end_offset": 823}, {"id": 1093652, "label": "ENT", "start_offset": 828, "end_offset": 844}, {"id": 1093653, "label": "ENT", "start_offset": 881, "end_offset": 895}, {"id": 1093654, "label": "ENT", "start_offset": 915, "end_offset": 926}, {"id": 1093655, "label": "ENT", "start_offset": 972, "end_offset": 990}, {"id": 1093656, "label": "ENT", "start_offset": 997, "end_offset": 1014}, {"id": 1093657, "label": "ENT", "start_offset": 1040, "end_offset": 1055}, {"id": 1093658, "label": "ENT", "start_offset": 1079, "end_offset": 1106}], "relations": [{"id": 176701, "from_id": 1093638, "to_id": 1093637, "type": "HYPONYM-OF"}, {"id": 176702, "from_id": 1093634, "to_id": 1093635, "type": "CONJUNCTION"}, {"id": 176703, "from_id": 1093635, "to_id": 1093636, "type": "CONJUNCTION"}, {"id": 176704, "from_id": 1093634, "to_id": 1093633, "type": "HYPONYM-OF"}, {"id": 176705, "from_id": 1093635, "to_id": 1093633, "type": "HYPONYM-OF"}, {"id": 176706, "from_id": 1093636, "to_id": 1093633, "type": "HYPONYM-OF"}, {"id": 176707, "from_id": 1093640, "to_id": 1093639, "type": "USED-FOR"}, {"id": 176708, "from_id": 1093642, "to_id": 1093641, "type": "PART-OF"}, {"id": 176709, "from_id": 1093633, "to_id": 1093629, "type": "HYPONYM-OF"}, {"id": 176710, "from_id": 1093637, "to_id": 1093629, "type": "HYPONYM-OF"}, {"id": 176711, "from_id": 1093629, "to_id": 1093643, "type": "COREF"}, {"id": 176712, "from_id": 1093643, "to_id": 1093644, "type": "COMPARE"}, {"id": 176713, "from_id": 1093634, "to_id": 1093645, "type": "COREF"}, {"id": 176714, "from_id": 1093635, "to_id": 1093646, "type": "COREF"}, {"id": 176715, "from_id": 1093645, "to_id": 1093646, "type": "CONJUNCTION"}, {"id": 176716, "from_id": 1093643, "to_id": 1093650, "type": "COREF"}, {"id": 176717, "from_id": 1093652, "to_id": 1093651, "type": "USED-FOR"}, {"id": 176718, "from_id": 1093630, "to_id": 1093651, "type": "COREF"}, {"id": 176719, "from_id": 1093652, "to_id": 1093653, "type": "COMPARE"}, {"id": 176720, "from_id": 1093654, "to_id": 1093650, "type": "HYPONYM-OF"}, {"id": 176721, "from_id": 1093650, "to_id": 1093651, "type": "EVALUATE-FOR"}, {"id": 176722, "from_id": 1093657, "to_id": 1093658, "type": "USED-FOR"}, {"id": 176723, "from_id": 1093651, "to_id": 1093658, "type": "COREF"}, {"id": 176724, "from_id": 1093656, "to_id": 1093655, "type": "USED-FOR"}, {"id": 176725, "from_id": 1093657, "to_id": 1093646, "type": "COREF"}, {"id": 176726, "from_id": 1093631, "to_id": 1093630, "type": "USED-FOR"}, {"id": 176727, "from_id": 1093629, "to_id": 1093630, "type": "EVALUATE-FOR"}, {"id": 176728, "from_id": 1093632, "to_id": 1093631, "type": "FEATURE-OF"}, {"id": 176729, "from_id": 1093648, "to_id": 1093646, "type": "EVALUATE-FOR"}, {"id": 176730, "from_id": 1093648, "to_id": 1093645, "type": "EVALUATE-FOR"}, {"id": 176731, "from_id": 1093646, "to_id": 1093647, "type": "HYPONYM-OF"}, {"id": 176732, "from_id": 1093645, "to_id": 1093647, "type": "HYPONYM-OF"}, {"id": 176733, "from_id": 1093646, "to_id": 1093649, "type": "COMPARE"}, {"id": 176734, "from_id": 1093645, "to_id": 1093649, "type": "COMPARE"}, {"id": 176735, "from_id": 1093633, "to_id": 1093637, "type": "CONJUNCTION"}]}
{"id": "ICCV_2013_25_abs", "text": "We present a scanning method that recovers dense sub-pixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as mi-cro phase shifting and modulated phase shifting.", "Comments": [], "entities": [{"id": 1093711, "label": "ENT", "start_offset": 13, "end_offset": 28}, {"id": 1093712, "label": "ENT", "start_offset": 43, "end_offset": 90}, {"id": 1093713, "label": "ENT", "start_offset": 113, "end_offset": 136}, {"id": 1093714, "label": "ENT", "start_offset": 172, "end_offset": 189}, {"id": 1093715, "label": "ENT", "start_offset": 191, "end_offset": 208}, {"id": 1093716, "label": "ENT", "start_offset": 244, "end_offset": 258}, {"id": 1093717, "label": "ENT", "start_offset": 302, "end_offset": 323}, {"id": 1093718, "label": "ENT", "start_offset": 332, "end_offset": 373}, {"id": 1093719, "label": "ENT", "start_offset": 388, "end_offset": 398}, {"id": 1093720, "label": "ENT", "start_offset": 402, "end_offset": 419}, {"id": 1093721, "label": "ENT", "start_offset": 424, "end_offset": 445}, {"id": 1093722, "label": "ENT", "start_offset": 496, "end_offset": 502}, {"id": 1093723, "label": "ENT", "start_offset": 512, "end_offset": 526}, {"id": 1093724, "label": "ENT", "start_offset": 537, "end_offset": 555}, {"id": 1093725, "label": "ENT", "start_offset": 566, "end_offset": 568}, {"id": 1093726, "label": "ENT", "start_offset": 599, "end_offset": 628}, {"id": 1093727, "label": "ENT", "start_offset": 656, "end_offset": 680}, {"id": 1093728, "label": "ENT", "start_offset": 689, "end_offset": 710}, {"id": 1093729, "label": "ENT", "start_offset": 715, "end_offset": 739}], "relations": [{"id": 176781, "from_id": 1093711, "to_id": 1093712, "type": "USED-FOR"}, {"id": 176782, "from_id": 1093719, "to_id": 1093718, "type": "EVALUATE-FOR"}, {"id": 176783, "from_id": 1093716, "to_id": 1093715, "type": "USED-FOR"}, {"id": 176784, "from_id": 1093722, "to_id": 1093711, "type": "COREF"}, {"id": 176785, "from_id": 1093722, "to_id": 1093723, "type": "USED-FOR"}, {"id": 176786, "from_id": 1093725, "to_id": 1093722, "type": "COREF"}, {"id": 176787, "from_id": 1093725, "to_id": 1093726, "type": "USED-FOR"}, {"id": 176788, "from_id": 1093724, "to_id": 1093723, "type": "FEATURE-OF"}, {"id": 176789, "from_id": 1093728, "to_id": 1093727, "type": "HYPONYM-OF"}, {"id": 176790, "from_id": 1093729, "to_id": 1093727, "type": "HYPONYM-OF"}, {"id": 176791, "from_id": 1093720, "to_id": 1093719, "type": "FEATURE-OF"}, {"id": 176792, "from_id": 1093721, "to_id": 1093719, "type": "FEATURE-OF"}, {"id": 176793, "from_id": 1093720, "to_id": 1093721, "type": "CONJUNCTION"}, {"id": 176794, "from_id": 1093728, "to_id": 1093729, "type": "CONJUNCTION"}]}
{"id": "N03-1026", "text": " We present an application of  ambiguity packing and stochastic disambiguation techniques  for  Lexical-Functional Grammars (LFG)  to the domain of  sentence condensation  . Our system incorporates a  linguistic parser/generator  for  LFG  , a  transfer component  for  parse reduction  operating on  packed parse forests  , and a  maximum-entropy model  for  stochastic output selection  . Furthermore, we propose the use of standard  parser evaluation methods  for automatically evaluating the  summarization  quality of  sentence condensation systems  . An  experimental evaluation  of  summarization  quality shows a close correlation between the  automatic parse-based evaluation  and a  manual evaluation  of generated  strings  . Overall  summarization  quality of the proposed system is state-of-the-art, with guaranteed  grammaticality  of the  system output  due to the use of a  constraint-based parser/generator  . ", "Comments": [], "entities": [{"id": 1093730, "label": "ENT", "start_offset": 31, "end_offset": 89}, {"id": 1093731, "label": "ENT", "start_offset": 96, "end_offset": 129}, {"id": 1093732, "label": "ENT", "start_offset": 149, "end_offset": 170}, {"id": 1093733, "label": "ENT", "start_offset": 178, "end_offset": 184}, {"id": 1093734, "label": "ENT", "start_offset": 201, "end_offset": 228}, {"id": 1093735, "label": "ENT", "start_offset": 235, "end_offset": 238}, {"id": 1093736, "label": "ENT", "start_offset": 245, "end_offset": 263}, {"id": 1093737, "label": "ENT", "start_offset": 270, "end_offset": 285}, {"id": 1093738, "label": "ENT", "start_offset": 301, "end_offset": 321}, {"id": 1093739, "label": "ENT", "start_offset": 332, "end_offset": 353}, {"id": 1093740, "label": "ENT", "start_offset": 360, "end_offset": 387}, {"id": 1093741, "label": "ENT", "start_offset": 436, "end_offset": 461}, {"id": 1093742, "label": "ENT", "start_offset": 497, "end_offset": 519}, {"id": 1093743, "label": "ENT", "start_offset": 524, "end_offset": 553}, {"id": 1093744, "label": "ENT", "start_offset": 590, "end_offset": 612}, {"id": 1093745, "label": "ENT", "start_offset": 652, "end_offset": 684}, {"id": 1093746, "label": "ENT", "start_offset": 693, "end_offset": 710}, {"id": 1093747, "label": "ENT", "start_offset": 746, "end_offset": 768}, {"id": 1093748, "label": "ENT", "start_offset": 785, "end_offset": 791}, {"id": 1093749, "label": "ENT", "start_offset": 830, "end_offset": 844}, {"id": 1093750, "label": "ENT", "start_offset": 890, "end_offset": 923}], "relations": [{"id": 176795, "from_id": 1093730, "to_id": 1093731, "type": "USED-FOR"}, {"id": 176796, "from_id": 1093734, "to_id": 1093735, "type": "USED-FOR"}, {"id": 176797, "from_id": 1093736, "to_id": 1093737, "type": "USED-FOR"}, {"id": 176798, "from_id": 1093739, "to_id": 1093740, "type": "USED-FOR"}, {"id": 176799, "from_id": 1093730, "to_id": 1093732, "type": "USED-FOR"}, {"id": 176800, "from_id": 1093734, "to_id": 1093733, "type": "PART-OF"}, {"id": 176801, "from_id": 1093736, "to_id": 1093733, "type": "PART-OF"}, {"id": 176802, "from_id": 1093734, "to_id": 1093736, "type": "CONJUNCTION"}, {"id": 176803, "from_id": 1093738, "to_id": 1093737, "type": "USED-FOR"}, {"id": 176804, "from_id": 1093736, "to_id": 1093739, "type": "CONJUNCTION"}, {"id": 176805, "from_id": 1093745, "to_id": 1093746, "type": "COMPARE"}, {"id": 176806, "from_id": 1093748, "to_id": 1093743, "type": "COREF"}, {"id": 176807, "from_id": 1093750, "to_id": 1093748, "type": "USED-FOR"}, {"id": 176808, "from_id": 1093741, "to_id": 1093745, "type": "COREF"}, {"id": 176809, "from_id": 1093743, "to_id": 1093733, "type": "COREF"}, {"id": 176810, "from_id": 1093747, "to_id": 1093748, "type": "EVALUATE-FOR"}, {"id": 176811, "from_id": 1093733, "to_id": 1093730, "type": "COREF"}, {"id": 176812, "from_id": 1093744, "to_id": 1093747, "type": "COREF"}, {"id": 176813, "from_id": 1093742, "to_id": 1093743, "type": "EVALUATE-FOR"}, {"id": 176814, "from_id": 1093741, "to_id": 1093742, "type": "EVALUATE-FOR"}, {"id": 176815, "from_id": 1093744, "to_id": 1093742, "type": "COREF"}, {"id": 176816, "from_id": 1093744, "to_id": 1093742, "type": "COREF"}, {"id": 176817, "from_id": 1093744, "to_id": 1093745, "type": "EVALUATE-FOR"}, {"id": 176818, "from_id": 1093749, "to_id": 1093748, "type": "EVALUATE-FOR"}, {"id": 176819, "from_id": 1093731, "to_id": 1093735, "type": "COREF"}, {"id": 176820, "from_id": 1093739, "to_id": 1093733, "type": "PART-OF"}]}
{"id": "NIPS_2014_18_abs", "text": "We revisit the classical decision-theoretic problem of weighted expert voting from a statistical learning perspective. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed. Experimental results are provided to illustrate the theory.", "Comments": [], "entities": [{"id": 1093763, "label": "ENT", "start_offset": 15, "end_offset": 77}, {"id": 1093764, "label": "ENT", "start_offset": 85, "end_offset": 117}, {"id": 1093765, "label": "ENT", "start_offset": 207, "end_offset": 239}, {"id": 1093766, "label": "ENT", "start_offset": 280, "end_offset": 304}, {"id": 1093767, "label": "ENT", "start_offset": 314, "end_offset": 335}, {"id": 1093768, "label": "ENT", "start_offset": 344, "end_offset": 356}, {"id": 1093769, "label": "ENT", "start_offset": 367, "end_offset": 384}, {"id": 1093770, "label": "ENT", "start_offset": 461, "end_offset": 478}], "relations": [{"id": 176830, "from_id": 1093764, "to_id": 1093763, "type": "USED-FOR"}, {"id": 176831, "from_id": 1093767, "to_id": 1093768, "type": "USED-FOR"}]}
{"id": "P03-1033", "text": " We address appropriate  user modeling  in order to generate  cooperative responses  to each  user  in  spoken dialogue systems  . Unlike previous studies that focus on  user  's  knowledge  or typical kinds of  users  , the  user model  we propose is more comprehensive. Specifically, we set up three dimensions of  user models  :  skill level  to the system,  knowledge level  on the  target domain  and the degree of  hastiness  . Moreover, the  models  are automatically derived by  decision tree learning  using real  dialogue data  collected by the system. We obtained reasonable  classification accuracy  for all dimensions.  Dialogue strategies  based on the  user modeling  are implemented in  Kyoto city bus information system  that has been developed at our laboratory. Experimental evaluation shows that the  cooperative responses  adaptive to  individual users  serve as good guidance for  novice users  without increasing the  dialogue duration  for  skilled users  . ", "Comments": [], "entities": [{"id": 1093790, "label": "ENT", "start_offset": 25, "end_offset": 38}, {"id": 1093791, "label": "ENT", "start_offset": 62, "end_offset": 83}, {"id": 1093792, "label": "ENT", "start_offset": 104, "end_offset": 127}, {"id": 1093793, "label": "ENT", "start_offset": 147, "end_offset": 154}, {"id": 1093794, "label": "ENT", "start_offset": 226, "end_offset": 236}, {"id": 1093795, "label": "ENT", "start_offset": 317, "end_offset": 328}, {"id": 1093796, "label": "ENT", "start_offset": 449, "end_offset": 455}, {"id": 1093797, "label": "ENT", "start_offset": 487, "end_offset": 509}, {"id": 1093798, "label": "ENT", "start_offset": 517, "end_offset": 536}, {"id": 1093799, "label": "ENT", "start_offset": 555, "end_offset": 561}, {"id": 1093800, "label": "ENT", "start_offset": 587, "end_offset": 610}, {"id": 1093801, "label": "ENT", "start_offset": 633, "end_offset": 652}, {"id": 1093802, "label": "ENT", "start_offset": 668, "end_offset": 681}, {"id": 1093803, "label": "ENT", "start_offset": 703, "end_offset": 736}, {"id": 1093804, "label": "ENT", "start_offset": 821, "end_offset": 842}, {"id": 1093805, "label": "ENT", "start_offset": 941, "end_offset": 958}], "relations": [{"id": 176850, "from_id": 1093790, "to_id": 1093791, "type": "USED-FOR"}, {"id": 176851, "from_id": 1093797, "to_id": 1093796, "type": "USED-FOR"}, {"id": 176852, "from_id": 1093802, "to_id": 1093801, "type": "USED-FOR"}, {"id": 176853, "from_id": 1093790, "to_id": 1093792, "type": "PART-OF"}, {"id": 176854, "from_id": 1093794, "to_id": 1093790, "type": "COREF"}, {"id": 176855, "from_id": 1093794, "to_id": 1093795, "type": "COREF"}, {"id": 176856, "from_id": 1093795, "to_id": 1093796, "type": "COREF"}, {"id": 176857, "from_id": 1093801, "to_id": 1093803, "type": "USED-FOR"}, {"id": 176858, "from_id": 1093798, "to_id": 1093797, "type": "USED-FOR"}, {"id": 176859, "from_id": 1093796, "to_id": 1093802, "type": "COREF"}, {"id": 176860, "from_id": 1093793, "to_id": 1093794, "type": "COMPARE"}, {"id": 176861, "from_id": 1093799, "to_id": 1093798, "type": "USED-FOR"}]}
{"id": "E06-1018", "text": " In this paper a novel solution to automatic and  unsupervised word sense induction (WSI)  is introduced. It represents an instantiation of the  one sense per collocation observation  (Gale et al., 1992). Like most existing approaches it utilizes  clustering of word co-occurrences  . This approach differs from other approaches to  WSI  in that it enhances the effect of the  one sense per collocation observation  by using triplets of  words  instead of pairs. The combination with a  two-step clustering process  using  sentence co-occurrences  as  features  allows for accurate results. Additionally, a novel and likewise automatic and  unsupervised evaluation method  inspired by Schutze's (1992) idea of evaluation of  word sense disambiguation algorithms  is employed. Offering advantages like reproducability and independency of a given biased  gold standard  it also enables  automatic parameter optimization  of the  WSI algorithm  . ", "Comments": [], "entities": [{"id": 1093806, "label": "ENT", "start_offset": 23, "end_offset": 31}, {"id": 1093807, "label": "ENT", "start_offset": 35, "end_offset": 89}, {"id": 1093808, "label": "ENT", "start_offset": 106, "end_offset": 108}, {"id": 1093809, "label": "ENT", "start_offset": 145, "end_offset": 182}, {"id": 1093810, "label": "ENT", "start_offset": 235, "end_offset": 237}, {"id": 1093811, "label": "ENT", "start_offset": 248, "end_offset": 281}, {"id": 1093812, "label": "ENT", "start_offset": 290, "end_offset": 298}, {"id": 1093813, "label": "ENT", "start_offset": 318, "end_offset": 328}, {"id": 1093814, "label": "ENT", "start_offset": 333, "end_offset": 336}, {"id": 1093815, "label": "ENT", "start_offset": 346, "end_offset": 348}, {"id": 1093816, "label": "ENT", "start_offset": 377, "end_offset": 414}, {"id": 1093817, "label": "ENT", "start_offset": 425, "end_offset": 443}, {"id": 1093818, "label": "ENT", "start_offset": 487, "end_offset": 514}, {"id": 1093819, "label": "ENT", "start_offset": 523, "end_offset": 546}, {"id": 1093820, "label": "ENT", "start_offset": 552, "end_offset": 560}, {"id": 1093821, "label": "ENT", "start_offset": 626, "end_offset": 671}, {"id": 1093822, "label": "ENT", "start_offset": 725, "end_offset": 761}, {"id": 1093823, "label": "ENT", "start_offset": 885, "end_offset": 917}, {"id": 1093824, "label": "ENT", "start_offset": 927, "end_offset": 940}], "relations": [{"id": 176862, "from_id": 1093819, "to_id": 1093818, "type": "USED-FOR"}, {"id": 176863, "from_id": 1093808, "to_id": 1093806, "type": "COREF"}, {"id": 176864, "from_id": 1093810, "to_id": 1093808, "type": "COREF"}, {"id": 176865, "from_id": 1093811, "to_id": 1093810, "type": "USED-FOR"}, {"id": 176866, "from_id": 1093812, "to_id": 1093813, "type": "COMPARE"}, {"id": 176867, "from_id": 1093812, "to_id": 1093814, "type": "USED-FOR"}, {"id": 176868, "from_id": 1093813, "to_id": 1093814, "type": "USED-FOR"}, {"id": 176869, "from_id": 1093815, "to_id": 1093812, "type": "COREF"}, {"id": 176870, "from_id": 1093812, "to_id": 1093810, "type": "COREF"}, {"id": 176871, "from_id": 1093815, "to_id": 1093816, "type": "USED-FOR"}, {"id": 176872, "from_id": 1093823, "to_id": 1093824, "type": "USED-FOR"}, {"id": 176873, "from_id": 1093816, "to_id": 1093809, "type": "COREF"}, {"id": 176874, "from_id": 1093808, "to_id": 1093809, "type": "HYPONYM-OF"}, {"id": 176875, "from_id": 1093817, "to_id": 1093815, "type": "USED-FOR"}, {"id": 176876, "from_id": 1093806, "to_id": 1093807, "type": "USED-FOR"}, {"id": 176877, "from_id": 1093807, "to_id": 1093814, "type": "COREF"}, {"id": 176878, "from_id": 1093821, "to_id": 1093822, "type": "EVALUATE-FOR"}, {"id": 176879, "from_id": 1093824, "to_id": 1093822, "type": "COREF"}]}
{"id": "P03-1068", "text": " We describe the ongoing construction of a large,  semantically annotated corpus  resource as reliable basis for the large-scale  acquisition of word-semantic information  , e.g. the construction of  domain-independent lexica  . The backbone of the  annotation  are  semantic roles  in the  frame semantics paradigm  . We report experiences and evaluate the  annotated data  from the first project stage. On this basis, we discuss the problems of  vagueness  and  ambiguity  in  semantic annotation  . ", "Comments": [], "entities": [{"id": 1093891, "label": "ENT", "start_offset": 51, "end_offset": 80}, {"id": 1093892, "label": "ENT", "start_offset": 117, "end_offset": 170}, {"id": 1093893, "label": "ENT", "start_offset": 183, "end_offset": 225}, {"id": 1093894, "label": "ENT", "start_offset": 250, "end_offset": 260}, {"id": 1093895, "label": "ENT", "start_offset": 267, "end_offset": 281}, {"id": 1093896, "label": "ENT", "start_offset": 291, "end_offset": 315}, {"id": 1093897, "label": "ENT", "start_offset": 359, "end_offset": 373}, {"id": 1093898, "label": "ENT", "start_offset": 448, "end_offset": 457}, {"id": 1093899, "label": "ENT", "start_offset": 464, "end_offset": 473}, {"id": 1093900, "label": "ENT", "start_offset": 479, "end_offset": 498}], "relations": [{"id": 176933, "from_id": 1093895, "to_id": 1093896, "type": "PART-OF"}, {"id": 176934, "from_id": 1093898, "to_id": 1093899, "type": "CONJUNCTION"}, {"id": 176935, "from_id": 1093899, "to_id": 1093900, "type": "FEATURE-OF"}, {"id": 176936, "from_id": 1093898, "to_id": 1093900, "type": "FEATURE-OF"}, {"id": 176937, "from_id": 1093893, "to_id": 1093892, "type": "HYPONYM-OF"}, {"id": 176938, "from_id": 1093891, "to_id": 1093892, "type": "USED-FOR"}]}
{"id": "W05-1308", "text": " In this paper, we present a  fully automated extraction system , named  IntEx , to identify  gene and protein interactions  in  biomedical text . Our approach is based on first splitting  complex sentences  into  simple clausal structures  made up of  syntactic roles . Then, tagging  biological entities  with the help of  biomedical and linguistic ontologies . Finally, extracting  complete interactions  by analyzing the matching contents of  syntactic roles  and their linguistically significant combinations. Our  extraction system  handles  complex sentences  and extracts  multiple and nested interactions  specified in a  sentence . Experimental evaluations with two other state of the art  extraction systems  indicate that the  IntEx system  achieves better  performance  without the labor intensive  pattern engineering requirement . ", "Comments": [], "entities": [{"id": 1094064, "label": "ENT", "start_offset": 30, "end_offset": 63}, {"id": 1094065, "label": "ENT", "start_offset": 73, "end_offset": 78}, {"id": 1094066, "label": "ENT", "start_offset": 94, "end_offset": 123}, {"id": 1094067, "label": "ENT", "start_offset": 129, "end_offset": 144}, {"id": 1094068, "label": "ENT", "start_offset": 151, "end_offset": 159}, {"id": 1094069, "label": "ENT", "start_offset": 253, "end_offset": 268}, {"id": 1094070, "label": "ENT", "start_offset": 286, "end_offset": 305}, {"id": 1094071, "label": "ENT", "start_offset": 325, "end_offset": 361}, {"id": 1094072, "label": "ENT", "start_offset": 447, "end_offset": 462}, {"id": 1094073, "label": "ENT", "start_offset": 520, "end_offset": 537}, {"id": 1094074, "label": "ENT", "start_offset": 581, "end_offset": 613}, {"id": 1094075, "label": "ENT", "start_offset": 700, "end_offset": 718}, {"id": 1094076, "label": "ENT", "start_offset": 739, "end_offset": 751}, {"id": 1094077, "label": "ENT", "start_offset": 812, "end_offset": 843}], "relations": [{"id": 177057, "from_id": 1094076, "to_id": 1094075, "type": "COMPARE"}, {"id": 177058, "from_id": 1094065, "to_id": 1094064, "type": "HYPONYM-OF"}, {"id": 177059, "from_id": 1094064, "to_id": 1094066, "type": "USED-FOR"}, {"id": 177060, "from_id": 1094067, "to_id": 1094066, "type": "USED-FOR"}, {"id": 177061, "from_id": 1094068, "to_id": 1094064, "type": "COREF"}, {"id": 177062, "from_id": 1094071, "to_id": 1094070, "type": "USED-FOR"}, {"id": 177063, "from_id": 1094073, "to_id": 1094068, "type": "COREF"}, {"id": 177064, "from_id": 1094073, "to_id": 1094074, "type": "USED-FOR"}, {"id": 177065, "from_id": 1094076, "to_id": 1094073, "type": "COREF"}]}
{"id": "CVPR_2016_406_abs", "text": "Image matching is a fundamental problem in Computer Vision. In the context of feature-based matching, SIFT and its variants have long excelled in a wide array of applications. However, for ultra-wide baselines, as in the case of aerial images captured under large camera rotations, the appearance variation goes beyond the reach of SIFT and RANSAC. In this paper we propose a data-driven, deep learning-based approach that sidesteps local correspondence by framing the problem as a classification task. Furthermore , we demonstrate that local correspondences can still be useful. To do so we incorporate an attention mechanism to produce a set of probable matches, which allows us to further increase performance. We train our models on a dataset of urban aerial imagery consisting of 'same' and 'different' pairs, collected for this purpose, and characterize the problem via a human study with annotations from Amazon Mechanical Turk. We demonstrate that our models outperform the state-of-the-art on ultra-wide baseline matching and approach human accuracy.", "Comments": [], "entities": [{"id": 1094078, "label": "ENT", "start_offset": 0, "end_offset": 14}, {"id": 1094079, "label": "ENT", "start_offset": 43, "end_offset": 58}, {"id": 1094080, "label": "ENT", "start_offset": 78, "end_offset": 100}, {"id": 1094081, "label": "ENT", "start_offset": 102, "end_offset": 106}, {"id": 1094082, "label": "ENT", "start_offset": 189, "end_offset": 209}, {"id": 1094083, "label": "ENT", "start_offset": 229, "end_offset": 242}, {"id": 1094084, "label": "ENT", "start_offset": 258, "end_offset": 280}, {"id": 1094085, "label": "ENT", "start_offset": 286, "end_offset": 306}, {"id": 1094086, "label": "ENT", "start_offset": 332, "end_offset": 336}, {"id": 1094087, "label": "ENT", "start_offset": 341, "end_offset": 347}, {"id": 1094088, "label": "ENT", "start_offset": 376, "end_offset": 417}, {"id": 1094089, "label": "ENT", "start_offset": 433, "end_offset": 453}, {"id": 1094090, "label": "ENT", "start_offset": 469, "end_offset": 476}, {"id": 1094091, "label": "ENT", "start_offset": 482, "end_offset": 501}, {"id": 1094092, "label": "ENT", "start_offset": 537, "end_offset": 558}, {"id": 1094093, "label": "ENT", "start_offset": 607, "end_offset": 626}, {"id": 1094094, "label": "ENT", "start_offset": 727, "end_offset": 733}, {"id": 1094095, "label": "ENT", "start_offset": 739, "end_offset": 770}, {"id": 1094096, "label": "ENT", "start_offset": 864, "end_offset": 871}, {"id": 1094097, "label": "ENT", "start_offset": 878, "end_offset": 889}, {"id": 1094098, "label": "ENT", "start_offset": 895, "end_offset": 934}, {"id": 1094099, "label": "ENT", "start_offset": 960, "end_offset": 966}, {"id": 1094100, "label": "ENT", "start_offset": 982, "end_offset": 998}, {"id": 1094101, "label": "ENT", "start_offset": 1002, "end_offset": 1030}, {"id": 1094102, "label": "ENT", "start_offset": 1044, "end_offset": 1058}], "relations": [{"id": 177066, "from_id": 1094078, "to_id": 1094079, "type": "HYPONYM-OF"}, {"id": 177067, "from_id": 1094095, "to_id": 1094094, "type": "USED-FOR"}, {"id": 177068, "from_id": 1094099, "to_id": 1094094, "type": "COREF"}, {"id": 177069, "from_id": 1094099, "to_id": 1094102, "type": "COMPARE"}, {"id": 177070, "from_id": 1094097, "to_id": 1094096, "type": "USED-FOR"}, {"id": 177071, "from_id": 1094098, "to_id": 1094097, "type": "USED-FOR"}, {"id": 177072, "from_id": 1094091, "to_id": 1094090, "type": "USED-FOR"}, {"id": 177073, "from_id": 1094081, "to_id": 1094080, "type": "USED-FOR"}, {"id": 177074, "from_id": 1094084, "to_id": 1094083, "type": "FEATURE-OF"}, {"id": 177075, "from_id": 1094081, "to_id": 1094086, "type": "COREF"}, {"id": 177076, "from_id": 1094086, "to_id": 1094087, "type": "CONJUNCTION"}, {"id": 177077, "from_id": 1094099, "to_id": 1094100, "type": "COMPARE"}, {"id": 177078, "from_id": 1094101, "to_id": 1094100, "type": "EVALUATE-FOR"}, {"id": 177079, "from_id": 1094101, "to_id": 1094099, "type": "EVALUATE-FOR"}]}
{"id": "ECCV_2006_13_abs", "text": "In spite of over two decades of intense research, illumination and pose invariance remain prohibitively challenging aspects of face recognition for most practical applications. The objective of this work is to recognize faces using video sequences both for training and recognition input, in a realistic, unconstrained setup in which lighting, pose and user motion pattern have a wide variability and face images are of low resolution. In particular there are three areas of novelty: (i) we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation, learnt offline, to generalize in the presence of extreme illumination changes; (ii) we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses; and (iii) we introduce an accurate video sequence \" reillumination \" algorithm to achieve robustness to face motion patterns in video. We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme illumination, pose and head motion variation. On this challenging data set our system consistently demonstrated a nearly perfect recognition rate (over 99.7% on all three databases), significantly out-performing state-of-the-art commercial software and methods from the literature.", "Comments": [], "entities": [{"id": 1094123, "label": "ENT", "start_offset": 50, "end_offset": 62}, {"id": 1094124, "label": "ENT", "start_offset": 67, "end_offset": 82}, {"id": 1094125, "label": "ENT", "start_offset": 127, "end_offset": 143}, {"id": 1094126, "label": "ENT", "start_offset": 232, "end_offset": 247}, {"id": 1094127, "label": "ENT", "start_offset": 334, "end_offset": 342}, {"id": 1094128, "label": "ENT", "start_offset": 344, "end_offset": 348}, {"id": 1094129, "label": "ENT", "start_offset": 353, "end_offset": 372}, {"id": 1094130, "label": "ENT", "start_offset": 401, "end_offset": 412}, {"id": 1094131, "label": "ENT", "start_offset": 424, "end_offset": 434}, {"id": 1094132, "label": "ENT", "start_offset": 502, "end_offset": 519}, {"id": 1094133, "label": "ENT", "start_offset": 523, "end_offset": 538}, {"id": 1094134, "label": "ENT", "start_offset": 562, "end_offset": 579}, {"id": 1094135, "label": "ENT", "start_offset": 583, "end_offset": 616}, {"id": 1094136, "label": "ENT", "start_offset": 667, "end_offset": 695}, {"id": 1094137, "label": "ENT", "start_offset": 713, "end_offset": 723}, {"id": 1094138, "label": "ENT", "start_offset": 727, "end_offset": 775}, {"id": 1094139, "label": "ENT", "start_offset": 782, "end_offset": 813}, {"id": 1094140, "label": "ENT", "start_offset": 839, "end_offset": 856}, {"id": 1094141, "label": "ENT", "start_offset": 893, "end_offset": 936}, {"id": 1094142, "label": "ENT", "start_offset": 948, "end_offset": 958}, {"id": 1094143, "label": "ENT", "start_offset": 962, "end_offset": 982}, {"id": 1094144, "label": "ENT", "start_offset": 986, "end_offset": 991}, {"id": 1094145, "label": "ENT", "start_offset": 1007, "end_offset": 1041}, {"id": 1094146, "label": "ENT", "start_offset": 1064, "end_offset": 1070}, {"id": 1094147, "label": "ENT", "start_offset": 1132, "end_offset": 1147}, {"id": 1094148, "label": "ENT", "start_offset": 1161, "end_offset": 1173}, {"id": 1094149, "label": "ENT", "start_offset": 1175, "end_offset": 1179}, {"id": 1094150, "label": "ENT", "start_offset": 1184, "end_offset": 1205}, {"id": 1094151, "label": "ENT", "start_offset": 1227, "end_offset": 1235}, {"id": 1094152, "label": "ENT", "start_offset": 1240, "end_offset": 1246}, {"id": 1094153, "label": "ENT", "start_offset": 1290, "end_offset": 1306}, {"id": 1094154, "label": "ENT", "start_offset": 1332, "end_offset": 1341}, {"id": 1094155, "label": "ENT", "start_offset": 1390, "end_offset": 1409}, {"id": 1094156, "label": "ENT", "start_offset": 1414, "end_offset": 1421}], "relations": [{"id": 177098, "from_id": 1094123, "to_id": 1094124, "type": "CONJUNCTION"}, {"id": 177099, "from_id": 1094124, "to_id": 1094125, "type": "PART-OF"}, {"id": 177100, "from_id": 1094123, "to_id": 1094125, "type": "PART-OF"}, {"id": 177101, "from_id": 1094131, "to_id": 1094130, "type": "FEATURE-OF"}, {"id": 177102, "from_id": 1094128, "to_id": 1094129, "type": "CONJUNCTION"}, {"id": 177103, "from_id": 1094127, "to_id": 1094128, "type": "CONJUNCTION"}, {"id": 177104, "from_id": 1094134, "to_id": 1094135, "type": "USED-FOR"}, {"id": 177105, "from_id": 1094132, "to_id": 1094133, "type": "USED-FOR"}, {"id": 177106, "from_id": 1094138, "to_id": 1094139, "type": "CONJUNCTION"}, {"id": 177107, "from_id": 1094137, "to_id": 1094138, "type": "FEATURE-OF"}, {"id": 177108, "from_id": 1094148, "to_id": 1094149, "type": "CONJUNCTION"}, {"id": 177109, "from_id": 1094149, "to_id": 1094150, "type": "CONJUNCTION"}, {"id": 177110, "from_id": 1094146, "to_id": 1094145, "type": "USED-FOR"}, {"id": 177111, "from_id": 1094134, "to_id": 1094136, "type": "USED-FOR"}, {"id": 177112, "from_id": 1094132, "to_id": 1094134, "type": "CONJUNCTION"}, {"id": 177113, "from_id": 1094148, "to_id": 1094147, "type": "FEATURE-OF"}, {"id": 177114, "from_id": 1094149, "to_id": 1094147, "type": "FEATURE-OF"}, {"id": 177115, "from_id": 1094150, "to_id": 1094147, "type": "FEATURE-OF"}, {"id": 177116, "from_id": 1094147, "to_id": 1094145, "type": "EVALUATE-FOR"}, {"id": 177117, "from_id": 1094145, "to_id": 1094152, "type": "COREF"}, {"id": 177118, "from_id": 1094147, "to_id": 1094151, "type": "COREF"}, {"id": 177119, "from_id": 1094151, "to_id": 1094152, "type": "EVALUATE-FOR"}, {"id": 177120, "from_id": 1094153, "to_id": 1094152, "type": "EVALUATE-FOR"}, {"id": 177121, "from_id": 1094152, "to_id": 1094155, "type": "COMPARE"}, {"id": 177122, "from_id": 1094152, "to_id": 1094156, "type": "COMPARE"}, {"id": 177123, "from_id": 1094155, "to_id": 1094156, "type": "CONJUNCTION"}, {"id": 177124, "from_id": 1094143, "to_id": 1094144, "type": "PART-OF"}, {"id": 177125, "from_id": 1094143, "to_id": 1094142, "type": "FEATURE-OF"}, {"id": 177126, "from_id": 1094142, "to_id": 1094141, "type": "EVALUATE-FOR"}]}
{"id": "CVPR_2008_257_abs", "text": "Graphical models such as Bayesian Networks (BNs) are being increasingly applied to various computer vision problems. One bottleneck in using BN is that learning the BN model parameters often requires a large amount of reliable and representative training data, which proves to be difficult to acquire for many computer vision tasks. On the other hand, there is often available qualitative prior knowledge about the model. Such knowledge comes either from domain experts based on their experience or from various physical or geometric constraints that govern the objects we try to model. Unlike the quantitative prior, the qualitative prior is often ignored due to the difficulty of incorporating them into the model learning process. In this paper, we introduce a closed-form solution to systematically combine the limited training data with some generic qualitative knowledge for BN parameter learning. To validate our method, we compare it with the Maximum Likelihood (ML) estimation method under sparse data and with the Expectation Maximization (EM) algorithm under incomplete data respectively. To further demonstrate its applications for computer vision, we apply it to learn a BN model for facial Action Unit (AU) recognition from real image data. The experimental results show that with simple and generic qualitative constraints and using only a small amount of training data, our method can robustly and accurately estimate the BN model parameters.", "Comments": [], "entities": [{"id": 1094157, "label": "ENT", "start_offset": 0, "end_offset": 16}, {"id": 1094158, "label": "ENT", "start_offset": 25, "end_offset": 48}, {"id": 1094159, "label": "ENT", "start_offset": 91, "end_offset": 115}, {"id": 1094160, "label": "ENT", "start_offset": 141, "end_offset": 143}, {"id": 1094161, "label": "ENT", "start_offset": 165, "end_offset": 184}, {"id": 1094162, "label": "ENT", "start_offset": 231, "end_offset": 259}, {"id": 1094163, "label": "ENT", "start_offset": 310, "end_offset": 331}, {"id": 1094164, "label": "ENT", "start_offset": 377, "end_offset": 404}, {"id": 1094165, "label": "ENT", "start_offset": 415, "end_offset": 420}, {"id": 1094166, "label": "ENT", "start_offset": 427, "end_offset": 436}, {"id": 1094167, "label": "ENT", "start_offset": 455, "end_offset": 469}, {"id": 1094168, "label": "ENT", "start_offset": 512, "end_offset": 545}, {"id": 1094169, "label": "ENT", "start_offset": 598, "end_offset": 616}, {"id": 1094170, "label": "ENT", "start_offset": 622, "end_offset": 639}, {"id": 1094171, "label": "ENT", "start_offset": 696, "end_offset": 700}, {"id": 1094172, "label": "ENT", "start_offset": 710, "end_offset": 732}, {"id": 1094173, "label": "ENT", "start_offset": 764, "end_offset": 784}, {"id": 1094174, "label": "ENT", "start_offset": 815, "end_offset": 836}, {"id": 1094175, "label": "ENT", "start_offset": 855, "end_offset": 876}, {"id": 1094176, "label": "ENT", "start_offset": 881, "end_offset": 902}, {"id": 1094177, "label": "ENT", "start_offset": 920, "end_offset": 926}, {"id": 1094178, "label": "ENT", "start_offset": 939, "end_offset": 941}, {"id": 1094179, "label": "ENT", "start_offset": 951, "end_offset": 992}, {"id": 1094180, "label": "ENT", "start_offset": 999, "end_offset": 1010}, {"id": 1094181, "label": "ENT", "start_offset": 1024, "end_offset": 1063}, {"id": 1094182, "label": "ENT", "start_offset": 1070, "end_offset": 1085}, {"id": 1094183, "label": "ENT", "start_offset": 1144, "end_offset": 1159}, {"id": 1094184, "label": "ENT", "start_offset": 1170, "end_offset": 1172}, {"id": 1094185, "label": "ENT", "start_offset": 1184, "end_offset": 1192}, {"id": 1094186, "label": "ENT", "start_offset": 1197, "end_offset": 1232}, {"id": 1094187, "label": "ENT", "start_offset": 1238, "end_offset": 1253}, {"id": 1094188, "label": "ENT", "start_offset": 1306, "end_offset": 1337}, {"id": 1094189, "label": "ENT", "start_offset": 1371, "end_offset": 1384}, {"id": 1094190, "label": "ENT", "start_offset": 1390, "end_offset": 1396}, {"id": 1094191, "label": "ENT", "start_offset": 1438, "end_offset": 1457}], "relations": [{"id": 177127, "from_id": 1094158, "to_id": 1094157, "type": "HYPONYM-OF"}, {"id": 177128, "from_id": 1094160, "to_id": 1094158, "type": "COREF"}, {"id": 177129, "from_id": 1094157, "to_id": 1094159, "type": "USED-FOR"}, {"id": 177130, "from_id": 1094162, "to_id": 1094161, "type": "USED-FOR"}, {"id": 177131, "from_id": 1094162, "to_id": 1094163, "type": "USED-FOR"}, {"id": 177132, "from_id": 1094166, "to_id": 1094164, "type": "COREF"}, {"id": 177133, "from_id": 1094164, "to_id": 1094165, "type": "FEATURE-OF"}, {"id": 177134, "from_id": 1094170, "to_id": 1094166, "type": "COREF"}, {"id": 177135, "from_id": 1094168, "to_id": 1094166, "type": "USED-FOR"}, {"id": 177136, "from_id": 1094169, "to_id": 1094170, "type": "COMPARE"}, {"id": 177137, "from_id": 1094167, "to_id": 1094168, "type": "CONJUNCTION"}, {"id": 177138, "from_id": 1094171, "to_id": 1094170, "type": "COREF"}, {"id": 177139, "from_id": 1094171, "to_id": 1094172, "type": "PART-OF"}, {"id": 177140, "from_id": 1094175, "to_id": 1094176, "type": "USED-FOR"}, {"id": 177141, "from_id": 1094178, "to_id": 1094179, "type": "COMPARE"}, {"id": 177142, "from_id": 1094180, "to_id": 1094178, "type": "USED-FOR"}, {"id": 177143, "from_id": 1094180, "to_id": 1094179, "type": "USED-FOR"}, {"id": 177144, "from_id": 1094182, "to_id": 1094181, "type": "USED-FOR"}, {"id": 177145, "from_id": 1094178, "to_id": 1094181, "type": "COMPARE"}, {"id": 177146, "from_id": 1094184, "to_id": 1094185, "type": "USED-FOR"}, {"id": 177147, "from_id": 1094185, "to_id": 1094186, "type": "USED-FOR"}, {"id": 177148, "from_id": 1094188, "to_id": 1094189, "type": "CONJUNCTION"}, {"id": 177149, "from_id": 1094189, "to_id": 1094190, "type": "USED-FOR"}, {"id": 177150, "from_id": 1094188, "to_id": 1094190, "type": "USED-FOR"}, {"id": 177151, "from_id": 1094190, "to_id": 1094191, "type": "USED-FOR"}, {"id": 177152, "from_id": 1094160, "to_id": 1094185, "type": "COREF"}, {"id": 177153, "from_id": 1094183, "to_id": 1094159, "type": "COREF"}, {"id": 177154, "from_id": 1094163, "to_id": 1094159, "type": "COREF"}, {"id": 177155, "from_id": 1094167, "to_id": 1094166, "type": "USED-FOR"}, {"id": 177156, "from_id": 1094174, "to_id": 1094175, "type": "CONJUNCTION"}, {"id": 177157, "from_id": 1094174, "to_id": 1094176, "type": "USED-FOR"}, {"id": 177158, "from_id": 1094176, "to_id": 1094173, "type": "USED-FOR"}, {"id": 177159, "from_id": 1094177, "to_id": 1094178, "type": "COREF"}, {"id": 177160, "from_id": 1094173, "to_id": 1094177, "type": "COREF"}, {"id": 177161, "from_id": 1094182, "to_id": 1094178, "type": "USED-FOR"}, {"id": 177162, "from_id": 1094178, "to_id": 1094184, "type": "COREF"}, {"id": 177163, "from_id": 1094184, "to_id": 1094183, "type": "USED-FOR"}, {"id": 177164, "from_id": 1094190, "to_id": 1094184, "type": "COREF"}, {"id": 177165, "from_id": 1094187, "to_id": 1094186, "type": "USED-FOR"}, {"id": 177166, "from_id": 1094165, "to_id": 1094160, "type": "COREF"}]}
{"id": "H05-1012", "text": " This paper presents a  maximum entropy word alignment algorithm  for  Arabic-English  based on  supervised training data  . We demonstrate that it is feasible to create  training material  for problems in  machine translation  and that a mixture of  supervised and unsupervised methods  yields superior  performance  . The  probabilistic model  used in the  alignment  directly models the  link decisions  . Significant improvement over traditional  word alignment techniques  is shown as well as improvement on several  machine translation tests  . Performance of the algorithm is contrasted with  human annotation performance  . ", "Comments": [], "entities": [{"id": 1094217, "label": "ENT", "start_offset": 24, "end_offset": 64}, {"id": 1094218, "label": "ENT", "start_offset": 71, "end_offset": 85}, {"id": 1094219, "label": "ENT", "start_offset": 97, "end_offset": 121}, {"id": 1094220, "label": "ENT", "start_offset": 171, "end_offset": 188}, {"id": 1094221, "label": "ENT", "start_offset": 207, "end_offset": 226}, {"id": 1094222, "label": "ENT", "start_offset": 251, "end_offset": 286}, {"id": 1094223, "label": "ENT", "start_offset": 325, "end_offset": 344}, {"id": 1094224, "label": "ENT", "start_offset": 359, "end_offset": 368}, {"id": 1094225, "label": "ENT", "start_offset": 391, "end_offset": 405}, {"id": 1094226, "label": "ENT", "start_offset": 451, "end_offset": 476}, {"id": 1094227, "label": "ENT", "start_offset": 522, "end_offset": 547}, {"id": 1094228, "label": "ENT", "start_offset": 570, "end_offset": 579}, {"id": 1094229, "label": "ENT", "start_offset": 600, "end_offset": 616}], "relations": [{"id": 177192, "from_id": 1094219, "to_id": 1094217, "type": "USED-FOR"}, {"id": 177193, "from_id": 1094223, "to_id": 1094224, "type": "USED-FOR"}, {"id": 177194, "from_id": 1094218, "to_id": 1094217, "type": "USED-FOR"}, {"id": 177195, "from_id": 1094220, "to_id": 1094221, "type": "USED-FOR"}, {"id": 177196, "from_id": 1094223, "to_id": 1094225, "type": "USED-FOR"}, {"id": 177197, "from_id": 1094226, "to_id": 1094227, "type": "USED-FOR"}, {"id": 177198, "from_id": 1094228, "to_id": 1094217, "type": "COREF"}, {"id": 177199, "from_id": 1094228, "to_id": 1094229, "type": "COMPARE"}]}
{"id": "NIPS_2002_11_abs", "text": "Although the study of clustering is centered around an intuitively compelling goal, it has been very difficult to develop a unified framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the difficulty in finding such a unification, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-offs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median.", "Comments": [], "entities": [{"id": 1094248, "label": "ENT", "start_offset": 22, "end_offset": 32}, {"id": 1094249, "label": "ENT", "start_offset": 124, "end_offset": 141}, {"id": 1094250, "label": "ENT", "start_offset": 146, "end_offset": 155}, {"id": 1094251, "label": "ENT", "start_offset": 342, "end_offset": 353}, {"id": 1094252, "label": "ENT", "start_offset": 373, "end_offset": 394}, {"id": 1094253, "label": "ENT", "start_offset": 459, "end_offset": 478}, {"id": 1094254, "label": "ENT", "start_offset": 604, "end_offset": 638}, {"id": 1094255, "label": "ENT", "start_offset": 647, "end_offset": 661}, {"id": 1094256, "label": "ENT", "start_offset": 663, "end_offset": 675}, {"id": 1094257, "label": "ENT", "start_offset": 677, "end_offset": 684}, {"id": 1094258, "label": "ENT", "start_offset": 690, "end_offset": 698}], "relations": [{"id": 177214, "from_id": 1094255, "to_id": 1094254, "type": "HYPONYM-OF"}, {"id": 177215, "from_id": 1094256, "to_id": 1094254, "type": "HYPONYM-OF"}, {"id": 177216, "from_id": 1094257, "to_id": 1094254, "type": "HYPONYM-OF"}, {"id": 177217, "from_id": 1094258, "to_id": 1094254, "type": "HYPONYM-OF"}, {"id": 177218, "from_id": 1094255, "to_id": 1094256, "type": "CONJUNCTION"}, {"id": 177219, "from_id": 1094256, "to_id": 1094257, "type": "CONJUNCTION"}, {"id": 177220, "from_id": 1094257, "to_id": 1094258, "type": "CONJUNCTION"}, {"id": 177221, "from_id": 1094249, "to_id": 1094250, "type": "USED-FOR"}, {"id": 177222, "from_id": 1094249, "to_id": 1094251, "type": "COREF"}]}
{"id": "P04-1030", "text": " We present the first application of the  head-driven statistical parsing model  of Collins (1999) as a  simultaneous language model  and  parser  for  large-vocabulary speech recognition . The model is adapted to an  online left to right chart-parser  for  word lattices , integrating acoustic, n-gram, and parser probabilities. The  parser  uses  structural and lexical dependencies  not considered by  n-gram models , conditioning recognition on more linguistically-grounded relationships. Experiments on the  Wall Street Journal treebank  and lattice corpora show  word error rates  competitive with the  standard n-gram language model  while extracting additional  structural information  useful for  speech understanding . ", "Comments": [], "entities": [{"id": 1094259, "label": "ENT", "start_offset": 42, "end_offset": 79}, {"id": 1094260, "label": "ENT", "start_offset": 105, "end_offset": 132}, {"id": 1094261, "label": "ENT", "start_offset": 139, "end_offset": 145}, {"id": 1094262, "label": "ENT", "start_offset": 152, "end_offset": 187}, {"id": 1094263, "label": "ENT", "start_offset": 194, "end_offset": 199}, {"id": 1094264, "label": "ENT", "start_offset": 218, "end_offset": 251}, {"id": 1094265, "label": "ENT", "start_offset": 258, "end_offset": 271}, {"id": 1094266, "label": "ENT", "start_offset": 286, "end_offset": 328}, {"id": 1094267, "label": "ENT", "start_offset": 335, "end_offset": 341}, {"id": 1094268, "label": "ENT", "start_offset": 349, "end_offset": 384}, {"id": 1094269, "label": "ENT", "start_offset": 405, "end_offset": 418}, {"id": 1094270, "label": "ENT", "start_offset": 513, "end_offset": 541}, {"id": 1094271, "label": "ENT", "start_offset": 547, "end_offset": 562}, {"id": 1094272, "label": "ENT", "start_offset": 569, "end_offset": 585}, {"id": 1094273, "label": "ENT", "start_offset": 618, "end_offset": 639}, {"id": 1094274, "label": "ENT", "start_offset": 670, "end_offset": 692}, {"id": 1094275, "label": "ENT", "start_offset": 706, "end_offset": 726}], "relations": [{"id": 177223, "from_id": 1094268, "to_id": 1094267, "type": "USED-FOR"}, {"id": 177224, "from_id": 1094274, "to_id": 1094275, "type": "USED-FOR"}, {"id": 177225, "from_id": 1094259, "to_id": 1094260, "type": "USED-FOR"}, {"id": 177226, "from_id": 1094259, "to_id": 1094263, "type": "COREF"}, {"id": 177227, "from_id": 1094259, "to_id": 1094261, "type": "USED-FOR"}, {"id": 177228, "from_id": 1094264, "to_id": 1094265, "type": "USED-FOR"}, {"id": 177229, "from_id": 1094266, "to_id": 1094264, "type": "PART-OF"}, {"id": 177230, "from_id": 1094264, "to_id": 1094267, "type": "COREF"}, {"id": 177231, "from_id": 1094260, "to_id": 1094261, "type": "CONJUNCTION"}, {"id": 177232, "from_id": 1094260, "to_id": 1094262, "type": "USED-FOR"}, {"id": 177233, "from_id": 1094261, "to_id": 1094262, "type": "USED-FOR"}, {"id": 177234, "from_id": 1094263, "to_id": 1094264, "type": "USED-FOR"}, {"id": 177235, "from_id": 1094272, "to_id": 1094273, "type": "EVALUATE-FOR"}, {"id": 177236, "from_id": 1094271, "to_id": 1094273, "type": "EVALUATE-FOR"}, {"id": 177237, "from_id": 1094270, "to_id": 1094273, "type": "EVALUATE-FOR"}, {"id": 177238, "from_id": 1094270, "to_id": 1094271, "type": "CONJUNCTION"}]}
{"id": "P06-1088", "text": " With performance above 97%  accuracy  for  newspaper text ,  part of speech (pos) tagging  might be considered a solved problem. Previous studies have shown that allowing the  parser  to resolve  pos tag ambiguity  does not improve performance. However, for  grammar formalisms  which use more  fine-grained grammatical categories , for example  tag  and  ccg ,  tagging accuracy  is much lower. In fact, for these  formalisms , premature  ambiguity resolution  makes  parsing  infeasible. We describe a  multi-tagging approach  which maintains a suitable level of  lexical category ambiguity  for accurate and efficient  ccg parsing . We extend this  multi-tagging approach  to the  pos level  to overcome errors introduced by automatically assigned  pos tags . Although  pos tagging accuracy  seems high, maintaining some  pos tag ambiguity  in the  language processing pipeline  results in more accurate  ccg supertagging . ", "Comments": [], "entities": [{"id": 1094276, "label": "ENT", "start_offset": 29, "end_offset": 37}, {"id": 1094277, "label": "ENT", "start_offset": 44, "end_offset": 58}, {"id": 1094278, "label": "ENT", "start_offset": 62, "end_offset": 90}, {"id": 1094279, "label": "ENT", "start_offset": 177, "end_offset": 183}, {"id": 1094280, "label": "ENT", "start_offset": 197, "end_offset": 214}, {"id": 1094281, "label": "ENT", "start_offset": 260, "end_offset": 278}, {"id": 1094282, "label": "ENT", "start_offset": 296, "end_offset": 331}, {"id": 1094283, "label": "ENT", "start_offset": 347, "end_offset": 350}, {"id": 1094284, "label": "ENT", "start_offset": 357, "end_offset": 360}, {"id": 1094285, "label": "ENT", "start_offset": 364, "end_offset": 380}, {"id": 1094286, "label": "ENT", "start_offset": 417, "end_offset": 427}, {"id": 1094287, "label": "ENT", "start_offset": 430, "end_offset": 461}, {"id": 1094288, "label": "ENT", "start_offset": 470, "end_offset": 477}, {"id": 1094289, "label": "ENT", "start_offset": 506, "end_offset": 528}, {"id": 1094290, "label": "ENT", "start_offset": 567, "end_offset": 593}, {"id": 1094291, "label": "ENT", "start_offset": 623, "end_offset": 634}, {"id": 1094292, "label": "ENT", "start_offset": 653, "end_offset": 675}, {"id": 1094293, "label": "ENT", "start_offset": 685, "end_offset": 694}, {"id": 1094294, "label": "ENT", "start_offset": 753, "end_offset": 761}, {"id": 1094295, "label": "ENT", "start_offset": 774, "end_offset": 794}, {"id": 1094296, "label": "ENT", "start_offset": 826, "end_offset": 843}, {"id": 1094297, "label": "ENT", "start_offset": 853, "end_offset": 881}, {"id": 1094298, "label": "ENT", "start_offset": 909, "end_offset": 925}], "relations": [{"id": 177239, "from_id": 1094279, "to_id": 1094280, "type": "USED-FOR"}, {"id": 177240, "from_id": 1094282, "to_id": 1094281, "type": "USED-FOR"}, {"id": 177241, "from_id": 1094289, "to_id": 1094291, "type": "USED-FOR"}, {"id": 177242, "from_id": 1094276, "to_id": 1094278, "type": "EVALUATE-FOR"}, {"id": 177243, "from_id": 1094277, "to_id": 1094278, "type": "EVALUATE-FOR"}, {"id": 177244, "from_id": 1094283, "to_id": 1094282, "type": "HYPONYM-OF"}, {"id": 177245, "from_id": 1094284, "to_id": 1094282, "type": "HYPONYM-OF"}, {"id": 177246, "from_id": 1094285, "to_id": 1094281, "type": "EVALUATE-FOR"}, {"id": 177247, "from_id": 1094281, "to_id": 1094286, "type": "COREF"}, {"id": 177248, "from_id": 1094288, "to_id": 1094286, "type": "USED-FOR"}, {"id": 177249, "from_id": 1094290, "to_id": 1094289, "type": "FEATURE-OF"}, {"id": 177250, "from_id": 1094289, "to_id": 1094292, "type": "COREF"}, {"id": 177251, "from_id": 1094292, "to_id": 1094293, "type": "USED-FOR"}, {"id": 177252, "from_id": 1094296, "to_id": 1094297, "type": "FEATURE-OF"}, {"id": 177253, "from_id": 1094296, "to_id": 1094298, "type": "USED-FOR"}]}
{"id": "P05-1018", "text": " This paper considers the problem of automatic assessment of  local coherence . We present a novel  entity-based representation  of  discourse  which is inspired by  Centering Theory  and can be computed automatically from  raw text . We view  coherence assessment  as a  ranking learning problem  and show that the proposed  discourse representation  supports the effective learning of a  ranking function . Our experiments demonstrate that the  induced model  achieves significantly higher  accuracy  than a  state-of-the-art coherence model . ", "Comments": [], "entities": [{"id": 1094299, "label": "ENT", "start_offset": 37, "end_offset": 77}, {"id": 1094300, "label": "ENT", "start_offset": 100, "end_offset": 142}, {"id": 1094301, "label": "ENT", "start_offset": 166, "end_offset": 182}, {"id": 1094302, "label": "ENT", "start_offset": 224, "end_offset": 232}, {"id": 1094303, "label": "ENT", "start_offset": 244, "end_offset": 264}, {"id": 1094304, "label": "ENT", "start_offset": 272, "end_offset": 296}, {"id": 1094305, "label": "ENT", "start_offset": 326, "end_offset": 350}, {"id": 1094306, "label": "ENT", "start_offset": 390, "end_offset": 406}, {"id": 1094307, "label": "ENT", "start_offset": 447, "end_offset": 460}, {"id": 1094308, "label": "ENT", "start_offset": 493, "end_offset": 501}, {"id": 1094309, "label": "ENT", "start_offset": 528, "end_offset": 543}], "relations": [{"id": 177254, "from_id": 1094304, "to_id": 1094303, "type": "USED-FOR"}, {"id": 177255, "from_id": 1094301, "to_id": 1094300, "type": "USED-FOR"}, {"id": 177256, "from_id": 1094302, "to_id": 1094300, "type": "USED-FOR"}, {"id": 177257, "from_id": 1094299, "to_id": 1094303, "type": "COREF"}, {"id": 177258, "from_id": 1094300, "to_id": 1094305, "type": "COREF"}, {"id": 177259, "from_id": 1094305, "to_id": 1094306, "type": "USED-FOR"}, {"id": 177260, "from_id": 1094308, "to_id": 1094307, "type": "EVALUATE-FOR"}, {"id": 177261, "from_id": 1094307, "to_id": 1094309, "type": "COMPARE"}, {"id": 177262, "from_id": 1094308, "to_id": 1094309, "type": "EVALUATE-FOR"}]}
{"id": "A97-1050", "text": " We investigate the utility of an  algorithm for translation lexicon acquisition (SABLE) , used previously on a very large  corpus  to acquire general  translation lexicons , when that  algorithm  is applied to a much smaller  corpus  to produce candidates for  domain-specific translation lexicons . ", "Comments": [], "entities": [{"id": 1094310, "label": "ENT", "start_offset": 35, "end_offset": 44}, {"id": 1094311, "label": "ENT", "start_offset": 49, "end_offset": 88}, {"id": 1094312, "label": "ENT", "start_offset": 143, "end_offset": 172}, {"id": 1094313, "label": "ENT", "start_offset": 186, "end_offset": 195}, {"id": 1094314, "label": "ENT", "start_offset": 262, "end_offset": 298}], "relations": [{"id": 177263, "from_id": 1094313, "to_id": 1094314, "type": "USED-FOR"}, {"id": 177264, "from_id": 1094310, "to_id": 1094311, "type": "USED-FOR"}, {"id": 177265, "from_id": 1094313, "to_id": 1094310, "type": "COREF"}, {"id": 177266, "from_id": 1094310, "to_id": 1094312, "type": "USED-FOR"}]}
{"id": "CVPR_2005_10_abs", "text": "Probabilistic models have been previously shown to be efficient and effective for modeling and recognition of human motion. In particular we focus on methods which represent the human motion model as a triangulated graph. Previous approaches learned models based just on positions and velocities of the body parts while ignoring their appearance. Moreover, a heuristic approach was commonly used to obtain translation invariance. In this paper we suggest an improved approach for learning such models and using them for human motion recognition. The suggested approach combines multiple cues, i.e., positions, velocities and appearance into both the learning and detection phases. Furthermore, we introduce global variables in the model, which can represent global properties such as translation, scale or viewpoint. The model is learned in an unsupervised manner from un-labelled data. We show that the suggested hybrid proba-bilistic model (which combines global variables, like translation , with local variables, like relative positions and appearances of body parts), leads to: (i) faster convergence of learning phase, (ii) robustness to occlusions, and, (iii) higher recognition rate.", "Comments": [], "entities": [{"id": 1094315, "label": "ENT", "start_offset": 0, "end_offset": 20}, {"id": 1094316, "label": "ENT", "start_offset": 82, "end_offset": 122}, {"id": 1094317, "label": "ENT", "start_offset": 178, "end_offset": 196}, {"id": 1094318, "label": "ENT", "start_offset": 202, "end_offset": 220}, {"id": 1094319, "label": "ENT", "start_offset": 250, "end_offset": 256}, {"id": 1094320, "label": "ENT", "start_offset": 271, "end_offset": 280}, {"id": 1094321, "label": "ENT", "start_offset": 285, "end_offset": 295}, {"id": 1094322, "label": "ENT", "start_offset": 335, "end_offset": 345}, {"id": 1094323, "label": "ENT", "start_offset": 359, "end_offset": 377}, {"id": 1094324, "label": "ENT", "start_offset": 406, "end_offset": 428}, {"id": 1094325, "label": "ENT", "start_offset": 467, "end_offset": 475}, {"id": 1094326, "label": "ENT", "start_offset": 494, "end_offset": 500}, {"id": 1094327, "label": "ENT", "start_offset": 511, "end_offset": 515}, {"id": 1094328, "label": "ENT", "start_offset": 520, "end_offset": 544}, {"id": 1094329, "label": "ENT", "start_offset": 560, "end_offset": 568}, {"id": 1094330, "label": "ENT", "start_offset": 587, "end_offset": 591}, {"id": 1094331, "label": "ENT", "start_offset": 599, "end_offset": 608}, {"id": 1094332, "label": "ENT", "start_offset": 610, "end_offset": 620}, {"id": 1094333, "label": "ENT", "start_offset": 625, "end_offset": 635}, {"id": 1094334, "label": "ENT", "start_offset": 650, "end_offset": 679}, {"id": 1094335, "label": "ENT", "start_offset": 707, "end_offset": 723}, {"id": 1094336, "label": "ENT", "start_offset": 731, "end_offset": 736}, {"id": 1094337, "label": "ENT", "start_offset": 758, "end_offset": 775}, {"id": 1094338, "label": "ENT", "start_offset": 784, "end_offset": 795}, {"id": 1094339, "label": "ENT", "start_offset": 797, "end_offset": 802}, {"id": 1094340, "label": "ENT", "start_offset": 806, "end_offset": 815}, {"id": 1094341, "label": "ENT", "start_offset": 821, "end_offset": 826}, {"id": 1094342, "label": "ENT", "start_offset": 844, "end_offset": 863}, {"id": 1094343, "label": "ENT", "start_offset": 869, "end_offset": 885}, {"id": 1094344, "label": "ENT", "start_offset": 914, "end_offset": 941}, {"id": 1094345, "label": "ENT", "start_offset": 958, "end_offset": 974}, {"id": 1094346, "label": "ENT", "start_offset": 981, "end_offset": 992}, {"id": 1094347, "label": "ENT", "start_offset": 1000, "end_offset": 1015}, {"id": 1094348, "label": "ENT", "start_offset": 1022, "end_offset": 1040}, {"id": 1094349, "label": "ENT", "start_offset": 1045, "end_offset": 1070}, {"id": 1094350, "label": "ENT", "start_offset": 1087, "end_offset": 1105}, {"id": 1094351, "label": "ENT", "start_offset": 1109, "end_offset": 1123}, {"id": 1094352, "label": "ENT", "start_offset": 1130, "end_offset": 1140}, {"id": 1094353, "label": "ENT", "start_offset": 1144, "end_offset": 1154}, {"id": 1094354, "label": "ENT", "start_offset": 1174, "end_offset": 1190}], "relations": [{"id": 177267, "from_id": 1094315, "to_id": 1094316, "type": "USED-FOR"}, {"id": 177268, "from_id": 1094318, "to_id": 1094317, "type": "USED-FOR"}, {"id": 177269, "from_id": 1094320, "to_id": 1094319, "type": "USED-FOR"}, {"id": 177270, "from_id": 1094321, "to_id": 1094319, "type": "USED-FOR"}, {"id": 177271, "from_id": 1094320, "to_id": 1094321, "type": "CONJUNCTION"}, {"id": 177272, "from_id": 1094323, "to_id": 1094324, "type": "USED-FOR"}, {"id": 177273, "from_id": 1094326, "to_id": 1094327, "type": "COREF"}, {"id": 177274, "from_id": 1094327, "to_id": 1094328, "type": "USED-FOR"}, {"id": 177275, "from_id": 1094325, "to_id": 1094326, "type": "USED-FOR"}, {"id": 177276, "from_id": 1094326, "to_id": 1094317, "type": "COREF"}, {"id": 177277, "from_id": 1094329, "to_id": 1094325, "type": "COREF"}, {"id": 177278, "from_id": 1094329, "to_id": 1094334, "type": "USED-FOR"}, {"id": 177279, "from_id": 1094331, "to_id": 1094330, "type": "HYPONYM-OF"}, {"id": 177280, "from_id": 1094332, "to_id": 1094330, "type": "HYPONYM-OF"}, {"id": 177281, "from_id": 1094333, "to_id": 1094330, "type": "HYPONYM-OF"}, {"id": 177282, "from_id": 1094331, "to_id": 1094332, "type": "CONJUNCTION"}, {"id": 177283, "from_id": 1094332, "to_id": 1094333, "type": "CONJUNCTION"}, {"id": 177284, "from_id": 1094329, "to_id": 1094336, "type": "COREF"}, {"id": 177285, "from_id": 1094335, "to_id": 1094336, "type": "USED-FOR"}, {"id": 177286, "from_id": 1094338, "to_id": 1094337, "type": "HYPONYM-OF"}, {"id": 177287, "from_id": 1094339, "to_id": 1094337, "type": "HYPONYM-OF"}, {"id": 177288, "from_id": 1094335, "to_id": 1094337, "type": "USED-FOR"}, {"id": 177289, "from_id": 1094340, "to_id": 1094337, "type": "HYPONYM-OF"}, {"id": 177290, "from_id": 1094338, "to_id": 1094339, "type": "CONJUNCTION"}, {"id": 177291, "from_id": 1094339, "to_id": 1094340, "type": "CONJUNCTION"}, {"id": 177292, "from_id": 1094341, "to_id": 1094336, "type": "COREF"}, {"id": 177293, "from_id": 1094342, "to_id": 1094341, "type": "USED-FOR"}, {"id": 177294, "from_id": 1094343, "to_id": 1094342, "type": "USED-FOR"}, {"id": 177295, "from_id": 1094345, "to_id": 1094344, "type": "USED-FOR"}, {"id": 177296, "from_id": 1094346, "to_id": 1094345, "type": "HYPONYM-OF"}, {"id": 177297, "from_id": 1094348, "to_id": 1094347, "type": "HYPONYM-OF"}, {"id": 177298, "from_id": 1094349, "to_id": 1094347, "type": "HYPONYM-OF"}, {"id": 177299, "from_id": 1094348, "to_id": 1094349, "type": "CONJUNCTION"}, {"id": 177300, "from_id": 1094350, "to_id": 1094351, "type": "FEATURE-OF"}, {"id": 177301, "from_id": 1094350, "to_id": 1094352, "type": "CONJUNCTION"}, {"id": 177302, "from_id": 1094352, "to_id": 1094354, "type": "CONJUNCTION"}, {"id": 177303, "from_id": 1094329, "to_id": 1094344, "type": "COREF"}]}
{"id": "NIPS_2003_18_abs", "text": "This paper presents an algorithm for learning the time-varying shape of a non-rigid 3D object from uncalibrated 2D tracking data. We model shape motion as a rigid component (rotation and translation) combined with a non-rigid deformation. Reconstruction is ill-posed if arbitrary deformations are allowed. We constrain the problem by assuming that the object shape at each time instant is drawn from a Gaussian distribution. Based on this assumption, the algorithm simultaneously estimates 3D shape and motion for each time frame, learns the parameters of the Gaussian, and robustly fills-in missing data points. We then extend the algorithm to model temporal smoothness in object shape, thus allowing it to handle severe cases of missing data.", "Comments": [], "entities": [{"id": 1094374, "label": "ENT", "start_offset": 23, "end_offset": 32}, {"id": 1094375, "label": "ENT", "start_offset": 37, "end_offset": 93}, {"id": 1094376, "label": "ENT", "start_offset": 99, "end_offset": 128}, {"id": 1094377, "label": "ENT", "start_offset": 139, "end_offset": 151}, {"id": 1094378, "label": "ENT", "start_offset": 157, "end_offset": 172}, {"id": 1094379, "label": "ENT", "start_offset": 174, "end_offset": 182}, {"id": 1094380, "label": "ENT", "start_offset": 187, "end_offset": 198}, {"id": 1094381, "label": "ENT", "start_offset": 216, "end_offset": 237}, {"id": 1094382, "label": "ENT", "start_offset": 239, "end_offset": 253}, {"id": 1094383, "label": "ENT", "start_offset": 270, "end_offset": 292}, {"id": 1094384, "label": "ENT", "start_offset": 352, "end_offset": 364}, {"id": 1094385, "label": "ENT", "start_offset": 402, "end_offset": 423}, {"id": 1094386, "label": "ENT", "start_offset": 455, "end_offset": 464}, {"id": 1094387, "label": "ENT", "start_offset": 490, "end_offset": 509}, {"id": 1094388, "label": "ENT", "start_offset": 560, "end_offset": 568}, {"id": 1094389, "label": "ENT", "start_offset": 632, "end_offset": 641}, {"id": 1094390, "label": "ENT", "start_offset": 651, "end_offset": 686}, {"id": 1094391, "label": "ENT", "start_offset": 702, "end_offset": 704}, {"id": 1094392, "label": "ENT", "start_offset": 731, "end_offset": 743}], "relations": [{"id": 177317, "from_id": 1094374, "to_id": 1094375, "type": "USED-FOR"}, {"id": 177318, "from_id": 1094385, "to_id": 1094384, "type": "USED-FOR"}, {"id": 177319, "from_id": 1094386, "to_id": 1094374, "type": "COREF"}, {"id": 177320, "from_id": 1094388, "to_id": 1094385, "type": "COREF"}, {"id": 177321, "from_id": 1094386, "to_id": 1094387, "type": "USED-FOR"}, {"id": 177322, "from_id": 1094389, "to_id": 1094386, "type": "COREF"}, {"id": 177323, "from_id": 1094389, "to_id": 1094390, "type": "USED-FOR"}, {"id": 177324, "from_id": 1094389, "to_id": 1094391, "type": "COREF"}, {"id": 177325, "from_id": 1094391, "to_id": 1094392, "type": "USED-FOR"}]}
{"id": "C80-1073", "text": "   An attempt has been made to use an  Augmented Transition Network  as a procedural  dialog model  . The development of such a  model  appears to be important in several respects: as a device to represent and to use different  dialog schemata  proposed in empirical  conversation analysis  ; as a device to represent and to use  models of verbal interaction  ; as a device combining knowledge about  dialog schemata  and about  verbal interaction  with knowledge about  task-oriented and goal-directed dialogs  . A standard  ATN  should be further developed in order to account for the  verbal interactions  of  task-oriented dialogs  . ", "Comments": [], "entities": [{"id": 1094399, "label": "ENT", "start_offset": 39, "end_offset": 67}, {"id": 1094400, "label": "ENT", "start_offset": 86, "end_offset": 98}, {"id": 1094401, "label": "ENT", "start_offset": 129, "end_offset": 134}, {"id": 1094402, "label": "ENT", "start_offset": 186, "end_offset": 192}, {"id": 1094403, "label": "ENT", "start_offset": 228, "end_offset": 243}, {"id": 1094404, "label": "ENT", "start_offset": 268, "end_offset": 289}, {"id": 1094405, "label": "ENT", "start_offset": 298, "end_offset": 304}, {"id": 1094406, "label": "ENT", "start_offset": 330, "end_offset": 336}, {"id": 1094407, "label": "ENT", "start_offset": 340, "end_offset": 358}, {"id": 1094408, "label": "ENT", "start_offset": 367, "end_offset": 373}, {"id": 1094409, "label": "ENT", "start_offset": 401, "end_offset": 416}, {"id": 1094410, "label": "ENT", "start_offset": 429, "end_offset": 447}, {"id": 1094411, "label": "ENT", "start_offset": 471, "end_offset": 510}, {"id": 1094412, "label": "ENT", "start_offset": 526, "end_offset": 529}, {"id": 1094413, "label": "ENT", "start_offset": 588, "end_offset": 607}, {"id": 1094414, "label": "ENT", "start_offset": 613, "end_offset": 634}], "relations": [{"id": 177332, "from_id": 1094399, "to_id": 1094400, "type": "HYPONYM-OF"}, {"id": 177333, "from_id": 1094413, "to_id": 1094414, "type": "FEATURE-OF"}, {"id": 177334, "from_id": 1094405, "to_id": 1094401, "type": "COREF"}, {"id": 177335, "from_id": 1094408, "to_id": 1094401, "type": "COREF"}, {"id": 177336, "from_id": 1094412, "to_id": 1094401, "type": "COREF"}, {"id": 177337, "from_id": 1094413, "to_id": 1094410, "type": "COREF"}, {"id": 177338, "from_id": 1094409, "to_id": 1094403, "type": "COREF"}, {"id": 177339, "from_id": 1094412, "to_id": 1094413, "type": "USED-FOR"}, {"id": 177340, "from_id": 1094406, "to_id": 1094407, "type": "USED-FOR"}, {"id": 177341, "from_id": 1094406, "to_id": 1094405, "type": "USED-FOR"}, {"id": 177342, "from_id": 1094410, "to_id": 1094407, "type": "COREF"}, {"id": 177343, "from_id": 1094401, "to_id": 1094399, "type": "COREF"}, {"id": 177344, "from_id": 1094401, "to_id": 1094402, "type": "COREF"}, {"id": 177345, "from_id": 1094403, "to_id": 1094402, "type": "USED-FOR"}, {"id": 177346, "from_id": 1094403, "to_id": 1094404, "type": "USED-FOR"}, {"id": 177347, "from_id": 1094409, "to_id": 1094410, "type": "CONJUNCTION"}]}
{"id": "CVPR_2009_10_abs", "text": "We propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints. Objects are represented as a coherent ensemble of parts that are consistent under 3D viewpoint transformations. Each part is a collection of salient image features. A generative framework is used for learning a model that captures the relative position of parts within each of the discretized viewpoints. Contrary to most of the existing mixture of viewpoints models , our model establishes explicit correspondences of parts across different viewpoints of the object class. Given a new image, detection and classification are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects. Our approach is among the first to propose a generative proba-bilistic framework for 3D object categorization. We test our algorithm on the detection task and the viewpoint classification task by using \" car \" category from both the Savarese et al. 2007 and PASCAL VOC 2006 datasets. We show promising results in both the detection and viewpoint classification tasks on these two challenging datasets.", "Comments": [], "entities": [{"id": 1094415, "label": "ENT", "start_offset": 19, "end_offset": 42}, {"id": 1094416, "label": "ENT", "start_offset": 56, "end_offset": 93}, {"id": 1094417, "label": "ENT", "start_offset": 107, "end_offset": 129}, {"id": 1094418, "label": "ENT", "start_offset": 134, "end_offset": 155}, {"id": 1094419, "label": "ENT", "start_offset": 239, "end_offset": 267}, {"id": 1094420, "label": "ENT", "start_offset": 298, "end_offset": 320}, {"id": 1094421, "label": "ENT", "start_offset": 324, "end_offset": 344}, {"id": 1094422, "label": "ENT", "start_offset": 368, "end_offset": 373}, {"id": 1094423, "label": "ENT", "start_offset": 438, "end_offset": 460}, {"id": 1094424, "label": "ENT", "start_offset": 495, "end_offset": 523}, {"id": 1094425, "label": "ENT", "start_offset": 530, "end_offset": 535}, {"id": 1094426, "label": "ENT", "start_offset": 599, "end_offset": 609}, {"id": 1094427, "label": "ENT", "start_offset": 643, "end_offset": 648}, {"id": 1094428, "label": "ENT", "start_offset": 650, "end_offset": 659}, {"id": 1094429, "label": "ENT", "start_offset": 664, "end_offset": 678}, {"id": 1094430, "label": "ENT", "start_offset": 711, "end_offset": 719}, {"id": 1094431, "label": "ENT", "start_offset": 724, "end_offset": 733}, {"id": 1094432, "label": "ENT", "start_offset": 761, "end_offset": 779}, {"id": 1094433, "label": "ENT", "start_offset": 810, "end_offset": 818}, {"id": 1094434, "label": "ENT", "start_offset": 851, "end_offset": 886}, {"id": 1094435, "label": "ENT", "start_offset": 891, "end_offset": 915}, {"id": 1094436, "label": "ENT", "start_offset": 929, "end_offset": 938}, {"id": 1094437, "label": "ENT", "start_offset": 946, "end_offset": 960}, {"id": 1094438, "label": "ENT", "start_offset": 969, "end_offset": 998}, {"id": 1094439, "label": "ENT", "start_offset": 1064, "end_offset": 1088}, {"id": 1094440, "label": "ENT", "start_offset": 1128, "end_offset": 1172}, {"id": 1094441, "label": "ENT", "start_offset": 1198, "end_offset": 1206}], "relations": [{"id": 177348, "from_id": 1094415, "to_id": 1094416, "type": "USED-FOR"}, {"id": 177349, "from_id": 1094417, "to_id": 1094415, "type": "USED-FOR"}, {"id": 177350, "from_id": 1094418, "to_id": 1094415, "type": "USED-FOR"}, {"id": 177351, "from_id": 1094421, "to_id": 1094422, "type": "USED-FOR"}, {"id": 177352, "from_id": 1094430, "to_id": 1094429, "type": "USED-FOR"}, {"id": 177353, "from_id": 1094431, "to_id": 1094429, "type": "USED-FOR"}, {"id": 177354, "from_id": 1094430, "to_id": 1094428, "type": "USED-FOR"}, {"id": 177355, "from_id": 1094431, "to_id": 1094428, "type": "USED-FOR"}, {"id": 177356, "from_id": 1094434, "to_id": 1094435, "type": "USED-FOR"}, {"id": 177357, "from_id": 1094433, "to_id": 1094425, "type": "COREF"}, {"id": 177358, "from_id": 1094436, "to_id": 1094433, "type": "COREF"}, {"id": 177359, "from_id": 1094439, "to_id": 1094436, "type": "EVALUATE-FOR"}, {"id": 177360, "from_id": 1094439, "to_id": 1094441, "type": "HYPONYM-OF"}, {"id": 177361, "from_id": 1094425, "to_id": 1094424, "type": "COMPARE"}, {"id": 177362, "from_id": 1094427, "to_id": 1094428, "type": "USED-FOR"}, {"id": 177363, "from_id": 1094427, "to_id": 1094429, "type": "USED-FOR"}, {"id": 177364, "from_id": 1094428, "to_id": 1094429, "type": "CONJUNCTION"}, {"id": 177365, "from_id": 1094430, "to_id": 1094431, "type": "CONJUNCTION"}, {"id": 177366, "from_id": 1094434, "to_id": 1094433, "type": "COREF"}, {"id": 177367, "from_id": 1094436, "to_id": 1094437, "type": "USED-FOR"}, {"id": 177368, "from_id": 1094436, "to_id": 1094438, "type": "USED-FOR"}, {"id": 177369, "from_id": 1094437, "to_id": 1094438, "type": "CONJUNCTION"}, {"id": 177370, "from_id": 1094441, "to_id": 1094440, "type": "EVALUATE-FOR"}, {"id": 177371, "from_id": 1094437, "to_id": 1094440, "type": "HYPONYM-OF"}, {"id": 177372, "from_id": 1094438, "to_id": 1094440, "type": "HYPONYM-OF"}, {"id": 177373, "from_id": 1094425, "to_id": 1094422, "type": "COREF"}, {"id": 177374, "from_id": 1094422, "to_id": 1094415, "type": "COREF"}, {"id": 177375, "from_id": 1094417, "to_id": 1094418, "type": "CONJUNCTION"}]}
{"id": "P01-1070", "text": " We describe a set of  supervised machine learning  experiments centering on the construction of  statistical models  of  WH-questions  . These  models  , which are built from  shallow linguistic features  of  questions  , are employed to predict target variables which represent a  user's informational goals  . We report on different aspects of the  predictive performance  of our  models  , including the influence of various  training and testing factors  on  predictive performance  , and examine the relationships among the target variables. ", "Comments": [], "entities": [{"id": 1094509, "label": "ENT", "start_offset": 23, "end_offset": 50}, {"id": 1094510, "label": "ENT", "start_offset": 98, "end_offset": 134}, {"id": 1094511, "label": "ENT", "start_offset": 145, "end_offset": 151}, {"id": 1094512, "label": "ENT", "start_offset": 177, "end_offset": 219}, {"id": 1094513, "label": "ENT", "start_offset": 283, "end_offset": 309}, {"id": 1094514, "label": "ENT", "start_offset": 384, "end_offset": 390}, {"id": 1094515, "label": "ENT", "start_offset": 430, "end_offset": 458}], "relations": [{"id": 177429, "from_id": 1094509, "to_id": 1094510, "type": "USED-FOR"}, {"id": 177430, "from_id": 1094511, "to_id": 1094510, "type": "COREF"}, {"id": 177431, "from_id": 1094512, "to_id": 1094511, "type": "USED-FOR"}, {"id": 177432, "from_id": 1094511, "to_id": 1094514, "type": "COREF"}]}
{"id": "A92-1010", "text": " In our current research into the design of  cognitively well-motivated interfaces  relying primarily on the  display of graphical information , we have observed that  graphical information  alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users' expectations. This can occur due to too much  information  being requested, too little,  information  of the wrong kind, etc. To solve this problem, we are working towards the integration of  natural language generation  to augment the  interaction ", "Comments": [], "entities": [{"id": 1094516, "label": "ENT", "start_offset": 45, "end_offset": 82}, {"id": 1094517, "label": "ENT", "start_offset": 110, "end_offset": 142}, {"id": 1094518, "label": "ENT", "start_offset": 168, "end_offset": 189}, {"id": 1094519, "label": "ENT", "start_offset": 512, "end_offset": 539}, {"id": 1094520, "label": "ENT", "start_offset": 557, "end_offset": 568}], "relations": [{"id": 177433, "from_id": 1094517, "to_id": 1094516, "type": "USED-FOR"}, {"id": 177434, "from_id": 1094519, "to_id": 1094520, "type": "USED-FOR"}, {"id": 177435, "from_id": 1094518, "to_id": 1094517, "type": "COREF"}]}
{"id": "ICML_2016_11_abs", "text": "The robust principal component analysis (robust PCA) problem has been considered in many machine learning applications, where the goal is to decompose the data matrix to a low rank part plus a sparse residual. While current approaches are developed by only considering the low rank plus sparse structure, in many applications, side information of row and/or column entities may also be given, and it is still unclear to what extent could such information help robust PCA. Thus, in this paper, we study the problem of robust PCA with side information, where both prior structure and features of entities are exploited for recovery. We propose a convex problem to incorporate side information in robust PCA and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions. In particular , our guarantee suggests that a substantial amount of low rank matrices, which cannot be recovered by standard robust PCA, become re-coverable by our proposed method. The result theoretically justifies the effectiveness of features in robust PCA. In addition, we conduct synthetic experiments as well as a real application on noisy image classification to show that our method also improves the performance in practice by exploiting side information.", "Comments": [], "entities": [{"id": 1094588, "label": "ENT", "start_offset": 4, "end_offset": 60}, {"id": 1094589, "label": "ENT", "start_offset": 89, "end_offset": 118}, {"id": 1094590, "label": "ENT", "start_offset": 155, "end_offset": 166}, {"id": 1094591, "label": "ENT", "start_offset": 172, "end_offset": 185}, {"id": 1094592, "label": "ENT", "start_offset": 193, "end_offset": 208}, {"id": 1094593, "label": "ENT", "start_offset": 224, "end_offset": 234}, {"id": 1094594, "label": "ENT", "start_offset": 273, "end_offset": 303}, {"id": 1094595, "label": "ENT", "start_offset": 327, "end_offset": 343}, {"id": 1094596, "label": "ENT", "start_offset": 443, "end_offset": 454}, {"id": 1094597, "label": "ENT", "start_offset": 460, "end_offset": 470}, {"id": 1094598, "label": "ENT", "start_offset": 517, "end_offset": 527}, {"id": 1094599, "label": "ENT", "start_offset": 533, "end_offset": 549}, {"id": 1094600, "label": "ENT", "start_offset": 562, "end_offset": 577}, {"id": 1094601, "label": "ENT", "start_offset": 582, "end_offset": 602}, {"id": 1094602, "label": "ENT", "start_offset": 621, "end_offset": 629}, {"id": 1094603, "label": "ENT", "start_offset": 644, "end_offset": 658}, {"id": 1094604, "label": "ENT", "start_offset": 674, "end_offset": 690}, {"id": 1094605, "label": "ENT", "start_offset": 694, "end_offset": 704}, {"id": 1094606, "label": "ENT", "start_offset": 723, "end_offset": 738}, {"id": 1094607, "label": "ENT", "start_offset": 781, "end_offset": 787}, {"id": 1094608, "label": "ENT", "start_offset": 882, "end_offset": 899}, {"id": 1094609, "label": "ENT", "start_offset": 939, "end_offset": 949}, {"id": 1094610, "label": "ENT", "start_offset": 987, "end_offset": 993}, {"id": 1094611, "label": "ENT", "start_offset": 1051, "end_offset": 1059}, {"id": 1094612, "label": "ENT", "start_offset": 1063, "end_offset": 1073}, {"id": 1094613, "label": "ENT", "start_offset": 1154, "end_offset": 1180}, {"id": 1094614, "label": "ENT", "start_offset": 1198, "end_offset": 1204}, {"id": 1094615, "label": "ENT", "start_offset": 1261, "end_offset": 1277}], "relations": [{"id": 177480, "from_id": 1094588, "to_id": 1094589, "type": "USED-FOR"}, {"id": 177481, "from_id": 1094594, "to_id": 1094593, "type": "USED-FOR"}, {"id": 177482, "from_id": 1094588, "to_id": 1094597, "type": "COREF"}, {"id": 177483, "from_id": 1094597, "to_id": 1094598, "type": "COREF"}, {"id": 177484, "from_id": 1094599, "to_id": 1094598, "type": "USED-FOR"}, {"id": 177485, "from_id": 1094600, "to_id": 1094602, "type": "USED-FOR"}, {"id": 177486, "from_id": 1094601, "to_id": 1094602, "type": "USED-FOR"}, {"id": 177487, "from_id": 1094600, "to_id": 1094601, "type": "CONJUNCTION"}, {"id": 177488, "from_id": 1094604, "to_id": 1094605, "type": "PART-OF"}, {"id": 177489, "from_id": 1094598, "to_id": 1094605, "type": "COREF"}, {"id": 177490, "from_id": 1094605, "to_id": 1094609, "type": "COREF"}, {"id": 177491, "from_id": 1094609, "to_id": 1094612, "type": "COREF"}, {"id": 177492, "from_id": 1094615, "to_id": 1094614, "type": "USED-FOR"}, {"id": 177493, "from_id": 1094613, "to_id": 1094614, "type": "EVALUATE-FOR"}, {"id": 177494, "from_id": 1094610, "to_id": 1094608, "type": "USED-FOR"}, {"id": 177495, "from_id": 1094607, "to_id": 1094606, "type": "USED-FOR"}, {"id": 177496, "from_id": 1094607, "to_id": 1094610, "type": "COREF"}, {"id": 177497, "from_id": 1094611, "to_id": 1094612, "type": "FEATURE-OF"}, {"id": 177498, "from_id": 1094610, "to_id": 1094614, "type": "COREF"}, {"id": 177499, "from_id": 1094603, "to_id": 1094604, "type": "USED-FOR"}, {"id": 177500, "from_id": 1094595, "to_id": 1094596, "type": "COREF"}, {"id": 177501, "from_id": 1094596, "to_id": 1094597, "type": "USED-FOR"}, {"id": 177502, "from_id": 1094591, "to_id": 1094590, "type": "PART-OF"}, {"id": 177503, "from_id": 1094591, "to_id": 1094592, "type": "CONJUNCTION"}, {"id": 177504, "from_id": 1094592, "to_id": 1094590, "type": "PART-OF"}]}
{"id": "H05-1064", "text": " We describe a new method for the representation of  NLP structures  within  reranking approaches . We make use of a  conditional log-linear model , with  hidden variables  representing the  assignment  of  lexical items  to  word clusters  or  word senses . The model learns to automatically make these  assignments  based on a  discriminative training criterion .  Training  and  decoding  with the model requires summing over an exponential number of  hidden-variable assignments : the required summations can be computed efficiently and exactly using  dynamic programming . As a case study, we apply the model to  parse reranking . The model gives an  F-measure improvement  of ~1.25% beyond the  base parser , and an ~0.25% improvement beyond  Collins (2000) reranker . Although our experiments are focused on  parsing , the techniques described generalize naturally to  NLP structures  other than  parse trees . ", "Comments": [], "entities": [{"id": 1094616, "label": "ENT", "start_offset": 19, "end_offset": 25}, {"id": 1094617, "label": "ENT", "start_offset": 53, "end_offset": 67}, {"id": 1094618, "label": "ENT", "start_offset": 77, "end_offset": 97}, {"id": 1094619, "label": "ENT", "start_offset": 118, "end_offset": 146}, {"id": 1094620, "label": "ENT", "start_offset": 155, "end_offset": 171}, {"id": 1094621, "label": "ENT", "start_offset": 226, "end_offset": 239}, {"id": 1094622, "label": "ENT", "start_offset": 245, "end_offset": 256}, {"id": 1094623, "label": "ENT", "start_offset": 263, "end_offset": 268}, {"id": 1094624, "label": "ENT", "start_offset": 330, "end_offset": 363}, {"id": 1094625, "label": "ENT", "start_offset": 401, "end_offset": 406}, {"id": 1094626, "label": "ENT", "start_offset": 455, "end_offset": 482}, {"id": 1094627, "label": "ENT", "start_offset": 498, "end_offset": 508}, {"id": 1094628, "label": "ENT", "start_offset": 556, "end_offset": 575}, {"id": 1094629, "label": "ENT", "start_offset": 608, "end_offset": 613}, {"id": 1094630, "label": "ENT", "start_offset": 618, "end_offset": 633}, {"id": 1094631, "label": "ENT", "start_offset": 640, "end_offset": 645}, {"id": 1094632, "label": "ENT", "start_offset": 656, "end_offset": 665}, {"id": 1094633, "label": "ENT", "start_offset": 701, "end_offset": 712}, {"id": 1094634, "label": "ENT", "start_offset": 749, "end_offset": 772}, {"id": 1094635, "label": "ENT", "start_offset": 816, "end_offset": 823}, {"id": 1094636, "label": "ENT", "start_offset": 830, "end_offset": 840}, {"id": 1094637, "label": "ENT", "start_offset": 876, "end_offset": 890}, {"id": 1094638, "label": "ENT", "start_offset": 904, "end_offset": 915}], "relations": [{"id": 177505, "from_id": 1094620, "to_id": 1094619, "type": "USED-FOR"}, {"id": 177506, "from_id": 1094633, "to_id": 1094634, "type": "COMPARE"}, {"id": 177507, "from_id": 1094621, "to_id": 1094622, "type": "CONJUNCTION"}, {"id": 177508, "from_id": 1094619, "to_id": 1094616, "type": "COREF"}, {"id": 177509, "from_id": 1094623, "to_id": 1094619, "type": "COREF"}, {"id": 177510, "from_id": 1094624, "to_id": 1094623, "type": "USED-FOR"}, {"id": 177511, "from_id": 1094625, "to_id": 1094623, "type": "COREF"}, {"id": 177512, "from_id": 1094628, "to_id": 1094627, "type": "USED-FOR"}, {"id": 177513, "from_id": 1094629, "to_id": 1094625, "type": "COREF"}, {"id": 177514, "from_id": 1094629, "to_id": 1094630, "type": "USED-FOR"}, {"id": 177515, "from_id": 1094632, "to_id": 1094631, "type": "EVALUATE-FOR"}, {"id": 177516, "from_id": 1094631, "to_id": 1094629, "type": "COREF"}, {"id": 177517, "from_id": 1094631, "to_id": 1094633, "type": "COMPARE"}, {"id": 177518, "from_id": 1094636, "to_id": 1094631, "type": "COREF"}, {"id": 177519, "from_id": 1094636, "to_id": 1094637, "type": "USED-FOR"}, {"id": 177520, "from_id": 1094636, "to_id": 1094635, "type": "USED-FOR"}, {"id": 177521, "from_id": 1094638, "to_id": 1094637, "type": "CONJUNCTION"}, {"id": 177522, "from_id": 1094616, "to_id": 1094617, "type": "USED-FOR"}, {"id": 177523, "from_id": 1094618, "to_id": 1094617, "type": "FEATURE-OF"}, {"id": 177524, "from_id": 1094636, "to_id": 1094638, "type": "USED-FOR"}]}
{"id": "I05-2013", "text": " We present a  tool , called  ILIMP , which takes as input a  raw text  in  French  and produces as output the same  text  in which every occurrence of the  pronoun il  is tagged either with tag  [ANA]  for  anaphoric  or  [IMP]  for  impersonal  or  expletive . This  tool  is therefore designed to distinguish between the  anaphoric occurrences of il , for which an  anaphora resolution system  has to look for an antecedent, and the  expletive occurrences  of this  pronoun , for which it does not make sense to look for an antecedent. The  precision rate  for  ILIMP  is 97,5%. The few  errors  are analyzed in detail. Other  tasks  using the  method  developed for  ILIMP  are described briefly, as well as the use of  ILIMP  in a modular  syntactic analysis system . ", "Comments": [], "entities": [{"id": 1094661, "label": "ENT", "start_offset": 15, "end_offset": 19}, {"id": 1094662, "label": "ENT", "start_offset": 30, "end_offset": 35}, {"id": 1094663, "label": "ENT", "start_offset": 62, "end_offset": 82}, {"id": 1094664, "label": "ENT", "start_offset": 269, "end_offset": 273}, {"id": 1094665, "label": "ENT", "start_offset": 325, "end_offset": 352}, {"id": 1094666, "label": "ENT", "start_offset": 369, "end_offset": 395}, {"id": 1094667, "label": "ENT", "start_offset": 544, "end_offset": 558}, {"id": 1094668, "label": "ENT", "start_offset": 565, "end_offset": 570}, {"id": 1094669, "label": "ENT", "start_offset": 630, "end_offset": 635}, {"id": 1094670, "label": "ENT", "start_offset": 648, "end_offset": 654}, {"id": 1094671, "label": "ENT", "start_offset": 671, "end_offset": 676}, {"id": 1094672, "label": "ENT", "start_offset": 724, "end_offset": 729}, {"id": 1094673, "label": "ENT", "start_offset": 736, "end_offset": 770}], "relations": [{"id": 177543, "from_id": 1094667, "to_id": 1094668, "type": "EVALUATE-FOR"}, {"id": 177544, "from_id": 1094670, "to_id": 1094669, "type": "USED-FOR"}, {"id": 177545, "from_id": 1094662, "to_id": 1094661, "type": "COREF"}, {"id": 177546, "from_id": 1094661, "to_id": 1094664, "type": "COREF"}, {"id": 177547, "from_id": 1094664, "to_id": 1094665, "type": "USED-FOR"}, {"id": 177548, "from_id": 1094666, "to_id": 1094665, "type": "USED-FOR"}, {"id": 177549, "from_id": 1094670, "to_id": 1094671, "type": "USED-FOR"}, {"id": 177550, "from_id": 1094672, "to_id": 1094671, "type": "COREF"}, {"id": 177551, "from_id": 1094671, "to_id": 1094668, "type": "COREF"}, {"id": 177552, "from_id": 1094668, "to_id": 1094664, "type": "COREF"}, {"id": 177553, "from_id": 1094663, "to_id": 1094661, "type": "USED-FOR"}, {"id": 177554, "from_id": 1094672, "to_id": 1094673, "type": "USED-FOR"}]}
{"id": "A00-2023", "text": " This paper presents a new approach to  statistical sentence generation  in which alternative  phrases  are represented as packed sets of  trees , or  forests , and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent  syntactic information . It also facilitates more efficient  statistical ranking  than a previous approach to  statistical generation . An efficient  ranking algorithm  is described, together with experimental results showing significant improvements over simple enumeration or a  lattice-based approach . ", "Comments": [], "entities": [{"id": 1094674, "label": "ENT", "start_offset": 27, "end_offset": 35}, {"id": 1094675, "label": "ENT", "start_offset": 40, "end_offset": 71}, {"id": 1094676, "label": "ENT", "start_offset": 139, "end_offset": 144}, {"id": 1094677, "label": "ENT", "start_offset": 151, "end_offset": 158}, {"id": 1094678, "label": "ENT", "start_offset": 220, "end_offset": 234}, {"id": 1094679, "label": "ENT", "start_offset": 301, "end_offset": 322}, {"id": 1094680, "label": "ENT", "start_offset": 325, "end_offset": 327}, {"id": 1094681, "label": "ENT", "start_offset": 361, "end_offset": 380}, {"id": 1094682, "label": "ENT", "start_offset": 398, "end_offset": 406}, {"id": 1094683, "label": "ENT", "start_offset": 411, "end_offset": 433}, {"id": 1094684, "label": "ENT", "start_offset": 450, "end_offset": 467}, {"id": 1094685, "label": "ENT", "start_offset": 563, "end_offset": 574}, {"id": 1094686, "label": "ENT", "start_offset": 581, "end_offset": 603}], "relations": [{"id": 177555, "from_id": 1094684, "to_id": 1094686, "type": "COMPARE"}, {"id": 177556, "from_id": 1094684, "to_id": 1094685, "type": "COMPARE"}, {"id": 177557, "from_id": 1094682, "to_id": 1094683, "type": "USED-FOR"}, {"id": 177558, "from_id": 1094680, "to_id": 1094678, "type": "COREF"}, {"id": 177559, "from_id": 1094674, "to_id": 1094675, "type": "USED-FOR"}, {"id": 177560, "from_id": 1094678, "to_id": 1094674, "type": "COREF"}, {"id": 177561, "from_id": 1094680, "to_id": 1094681, "type": "USED-FOR"}, {"id": 177562, "from_id": 1094680, "to_id": 1094682, "type": "COMPARE"}, {"id": 177563, "from_id": 1094685, "to_id": 1094686, "type": "CONJUNCTION"}]}
{"id": "ICASSP_1997_706_abs", "text": "Utterance Verification (UV) is a critical function of an Automatic Speech Recognition (ASR) System working on real applications where spontaneous speech, out-of-vocabulary (OOV) words and acoustic noises are present. In this paper we present a new UV procedure with two major features: a) Confidence tests are applied to decoded string hypotheses obtained from using word and garbage models that represent OOV words and noises. Thus the ASR system is designed to deal with what we refer to as Word Spotting and Noise Spotting capabilities. b) The UV procedure is based on three different confidence tests, two based on acoustic measures and one founded on linguistic information, applied in a hierarchical structure. Experimental results from a real telephone application on a natural number recognition task show an 50% reduction in recognition errors with a moderate 12% rejection rate of correct utterances and a low 1.5% rate of false acceptance.", "Comments": [], "entities": [{"id": 1094687, "label": "ENT", "start_offset": 0, "end_offset": 27}, {"id": 1094688, "label": "ENT", "start_offset": 57, "end_offset": 98}, {"id": 1094689, "label": "ENT", "start_offset": 134, "end_offset": 152}, {"id": 1094690, "label": "ENT", "start_offset": 154, "end_offset": 183}, {"id": 1094691, "label": "ENT", "start_offset": 188, "end_offset": 203}, {"id": 1094692, "label": "ENT", "start_offset": 248, "end_offset": 260}, {"id": 1094693, "label": "ENT", "start_offset": 289, "end_offset": 305}, {"id": 1094694, "label": "ENT", "start_offset": 321, "end_offset": 346}, {"id": 1094695, "label": "ENT", "start_offset": 406, "end_offset": 415}, {"id": 1094696, "label": "ENT", "start_offset": 420, "end_offset": 426}, {"id": 1094697, "label": "ENT", "start_offset": 437, "end_offset": 447}, {"id": 1094698, "label": "ENT", "start_offset": 493, "end_offset": 506}, {"id": 1094699, "label": "ENT", "start_offset": 511, "end_offset": 538}, {"id": 1094700, "label": "ENT", "start_offset": 547, "end_offset": 559}, {"id": 1094701, "label": "ENT", "start_offset": 588, "end_offset": 604}, {"id": 1094702, "label": "ENT", "start_offset": 606, "end_offset": 609}, {"id": 1094703, "label": "ENT", "start_offset": 619, "end_offset": 636}, {"id": 1094704, "label": "ENT", "start_offset": 641, "end_offset": 644}, {"id": 1094705, "label": "ENT", "start_offset": 656, "end_offset": 678}, {"id": 1094706, "label": "ENT", "start_offset": 693, "end_offset": 715}, {"id": 1094707, "label": "ENT", "start_offset": 750, "end_offset": 771}, {"id": 1094708, "label": "ENT", "start_offset": 777, "end_offset": 808}, {"id": 1094709, "label": "ENT", "start_offset": 834, "end_offset": 852}, {"id": 1094710, "label": "ENT", "start_offset": 873, "end_offset": 887}, {"id": 1094711, "label": "ENT", "start_offset": 933, "end_offset": 949}], "relations": [{"id": 177564, "from_id": 1094687, "to_id": 1094688, "type": "HYPONYM-OF"}, {"id": 177565, "from_id": 1094692, "to_id": 1094687, "type": "COREF"}, {"id": 177566, "from_id": 1094693, "to_id": 1094694, "type": "USED-FOR"}, {"id": 177567, "from_id": 1094696, "to_id": 1094695, "type": "CONJUNCTION"}, {"id": 177568, "from_id": 1094697, "to_id": 1094688, "type": "COREF"}, {"id": 177569, "from_id": 1094697, "to_id": 1094698, "type": "USED-FOR"}, {"id": 177570, "from_id": 1094697, "to_id": 1094699, "type": "USED-FOR"}, {"id": 177571, "from_id": 1094700, "to_id": 1094692, "type": "COREF"}, {"id": 177572, "from_id": 1094708, "to_id": 1094707, "type": "FEATURE-OF"}, {"id": 177573, "from_id": 1094709, "to_id": 1094708, "type": "EVALUATE-FOR"}, {"id": 177574, "from_id": 1094702, "to_id": 1094701, "type": "HYPONYM-OF"}, {"id": 177575, "from_id": 1094704, "to_id": 1094701, "type": "HYPONYM-OF"}, {"id": 177576, "from_id": 1094701, "to_id": 1094700, "type": "USED-FOR"}, {"id": 177577, "from_id": 1094703, "to_id": 1094702, "type": "USED-FOR"}, {"id": 177578, "from_id": 1094705, "to_id": 1094704, "type": "USED-FOR"}, {"id": 177579, "from_id": 1094701, "to_id": 1094706, "type": "USED-FOR"}]}
{"id": "CVPR_2014_10_abs", "text": "We present an image set classification algorithm based on unsupervised clustering of labeled training and unla-beled test data where labels are only used in the stopping criterion. The probability distribution of each class over the set of clusters is used to define a true set based similarity measure. To this end, we propose an iterative sparse spectral clustering algorithm. In each iteration, a proximity matrix is efficiently recomputed to better represent the local subspace structure. Initial clusters capture the global data structure and finer clusters at the later stages capture the subtle class differences not visible at the global scale. Image sets are compactly represented with multiple Grass-mannian manifolds which are subsequently embedded in Euclidean space with the proposed spectral clustering algorithm. We also propose an efficient eigenvector solver which not only reduces the computational cost of spectral clustering by many folds but also improves the clustering quality and final classification results. Experiments on five standard datasets and comparison with seven existing techniques show the efficacy of our algorithm.", "Comments": [], "entities": [{"id": 1094712, "label": "ENT", "start_offset": 14, "end_offset": 48}, {"id": 1094713, "label": "ENT", "start_offset": 58, "end_offset": 81}, {"id": 1094714, "label": "ENT", "start_offset": 85, "end_offset": 126}, {"id": 1094715, "label": "ENT", "start_offset": 161, "end_offset": 179}, {"id": 1094716, "label": "ENT", "start_offset": 185, "end_offset": 209}, {"id": 1094717, "label": "ENT", "start_offset": 240, "end_offset": 248}, {"id": 1094718, "label": "ENT", "start_offset": 274, "end_offset": 302}, {"id": 1094719, "label": "ENT", "start_offset": 331, "end_offset": 377}, {"id": 1094720, "label": "ENT", "start_offset": 400, "end_offset": 416}, {"id": 1094721, "label": "ENT", "start_offset": 467, "end_offset": 491}, {"id": 1094722, "label": "ENT", "start_offset": 493, "end_offset": 509}, {"id": 1094723, "label": "ENT", "start_offset": 522, "end_offset": 543}, {"id": 1094724, "label": "ENT", "start_offset": 548, "end_offset": 562}, {"id": 1094725, "label": "ENT", "start_offset": 595, "end_offset": 619}, {"id": 1094726, "label": "ENT", "start_offset": 639, "end_offset": 651}, {"id": 1094727, "label": "ENT", "start_offset": 653, "end_offset": 663}, {"id": 1094728, "label": "ENT", "start_offset": 704, "end_offset": 727}, {"id": 1094729, "label": "ENT", "start_offset": 763, "end_offset": 778}, {"id": 1094730, "label": "ENT", "start_offset": 797, "end_offset": 826}, {"id": 1094731, "label": "ENT", "start_offset": 857, "end_offset": 875}, {"id": 1094732, "label": "ENT", "start_offset": 903, "end_offset": 921}, {"id": 1094733, "label": "ENT", "start_offset": 925, "end_offset": 944}, {"id": 1094734, "label": "ENT", "start_offset": 981, "end_offset": 999}, {"id": 1094735, "label": "ENT", "start_offset": 1010, "end_offset": 1032}, {"id": 1094736, "label": "ENT", "start_offset": 1063, "end_offset": 1071}, {"id": 1094737, "label": "ENT", "start_offset": 1143, "end_offset": 1152}], "relations": [{"id": 177580, "from_id": 1094716, "to_id": 1094718, "type": "USED-FOR"}, {"id": 177581, "from_id": 1094720, "to_id": 1094721, "type": "USED-FOR"}, {"id": 177582, "from_id": 1094722, "to_id": 1094723, "type": "USED-FOR"}, {"id": 177583, "from_id": 1094724, "to_id": 1094725, "type": "USED-FOR"}, {"id": 177584, "from_id": 1094728, "to_id": 1094727, "type": "USED-FOR"}, {"id": 177585, "from_id": 1094730, "to_id": 1094729, "type": "USED-FOR"}, {"id": 177586, "from_id": 1094732, "to_id": 1094733, "type": "EVALUATE-FOR"}, {"id": 177587, "from_id": 1094734, "to_id": 1094733, "type": "EVALUATE-FOR"}, {"id": 177588, "from_id": 1094731, "to_id": 1094733, "type": "USED-FOR"}, {"id": 177589, "from_id": 1094737, "to_id": 1094712, "type": "COREF"}, {"id": 177590, "from_id": 1094730, "to_id": 1094719, "type": "COREF"}, {"id": 177591, "from_id": 1094713, "to_id": 1094712, "type": "USED-FOR"}, {"id": 177592, "from_id": 1094714, "to_id": 1094713, "type": "USED-FOR"}, {"id": 177593, "from_id": 1094735, "to_id": 1094733, "type": "EVALUATE-FOR"}]}
{"id": "P01-1004", "text": " In this paper, we compare the relative effects of  segment order  ,  segmentation  and  segment contiguity  on the  retrieval performance  of a  translation memory system  . We take a selection of both  bag-of-words and segment order-sensitive string comparison methods  , and run each over both  character- and word-segmented data  , in combination with a range of  local segment contiguity models  (in the form of  N-grams  ). Over two distinct  datasets  , we find that  indexing  according to simple  character bigrams  produces a  retrieval accuracy  superior to any of the tested  word N-gram models  . Further,in their optimum  configuration  ,  bag-of-words methods  are shown to be equivalent to  segment order-sensitive methods  in terms of  retrieval accuracy  , but much faster. We also provide evidence that our findings are scalable. ", "Comments": [], "entities": [{"id": 1094738, "label": "ENT", "start_offset": 52, "end_offset": 65}, {"id": 1094739, "label": "ENT", "start_offset": 70, "end_offset": 82}, {"id": 1094740, "label": "ENT", "start_offset": 89, "end_offset": 107}, {"id": 1094741, "label": "ENT", "start_offset": 117, "end_offset": 126}, {"id": 1094742, "label": "ENT", "start_offset": 146, "end_offset": 171}, {"id": 1094743, "label": "ENT", "start_offset": 204, "end_offset": 270}, {"id": 1094744, "label": "ENT", "start_offset": 298, "end_offset": 332}, {"id": 1094745, "label": "ENT", "start_offset": 368, "end_offset": 399}, {"id": 1094746, "label": "ENT", "start_offset": 418, "end_offset": 425}, {"id": 1094747, "label": "ENT", "start_offset": 475, "end_offset": 483}, {"id": 1094748, "label": "ENT", "start_offset": 506, "end_offset": 523}, {"id": 1094749, "label": "ENT", "start_offset": 537, "end_offset": 555}, {"id": 1094750, "label": "ENT", "start_offset": 588, "end_offset": 606}, {"id": 1094751, "label": "ENT", "start_offset": 654, "end_offset": 674}, {"id": 1094752, "label": "ENT", "start_offset": 707, "end_offset": 738}, {"id": 1094753, "label": "ENT", "start_offset": 753, "end_offset": 771}], "relations": [{"id": 177594, "from_id": 1094744, "to_id": 1094743, "type": "USED-FOR"}, {"id": 177595, "from_id": 1094748, "to_id": 1094747, "type": "USED-FOR"}, {"id": 177596, "from_id": 1094751, "to_id": 1094752, "type": "COMPARE"}, {"id": 177597, "from_id": 1094741, "to_id": 1094742, "type": "EVALUATE-FOR"}, {"id": 177598, "from_id": 1094745, "to_id": 1094743, "type": "CONJUNCTION"}, {"id": 177599, "from_id": 1094746, "to_id": 1094745, "type": "FEATURE-OF"}, {"id": 177600, "from_id": 1094738, "to_id": 1094739, "type": "CONJUNCTION"}, {"id": 177601, "from_id": 1094739, "to_id": 1094740, "type": "CONJUNCTION"}, {"id": 177602, "from_id": 1094738, "to_id": 1094742, "type": "USED-FOR"}, {"id": 177603, "from_id": 1094739, "to_id": 1094742, "type": "USED-FOR"}, {"id": 177604, "from_id": 1094740, "to_id": 1094742, "type": "USED-FOR"}, {"id": 177605, "from_id": 1094748, "to_id": 1094750, "type": "COMPARE"}, {"id": 177606, "from_id": 1094753, "to_id": 1094752, "type": "EVALUATE-FOR"}, {"id": 177607, "from_id": 1094753, "to_id": 1094751, "type": "EVALUATE-FOR"}, {"id": 177608, "from_id": 1094753, "to_id": 1094749, "type": "COREF"}, {"id": 177609, "from_id": 1094749, "to_id": 1094750, "type": "EVALUATE-FOR"}, {"id": 177610, "from_id": 1094749, "to_id": 1094748, "type": "EVALUATE-FOR"}]}
{"id": "C00-2123", "text": " In this paper, we describe a search procedure for  statistical machine translation (MT)  based on  dynamic programming (DP) . Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible  word reordering  between  source and target language  in order to achieve an efficient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the  Verbmobil task  (German-English, 8000-word vocabulary), which is a  limited-domain spoken-language task . ", "Comments": [], "entities": [{"id": 1094765, "label": "ENT", "start_offset": 30, "end_offset": 46}, {"id": 1094766, "label": "ENT", "start_offset": 52, "end_offset": 88}, {"id": 1094767, "label": "ENT", "start_offset": 100, "end_offset": 124}, {"id": 1094768, "label": "ENT", "start_offset": 143, "end_offset": 160}, {"id": 1094769, "label": "ENT", "start_offset": 168, "end_offset": 194}, {"id": 1094770, "label": "ENT", "start_offset": 215, "end_offset": 224}, {"id": 1094771, "label": "ENT", "start_offset": 251, "end_offset": 266}, {"id": 1094772, "label": "ENT", "start_offset": 338, "end_offset": 354}, {"id": 1094773, "label": "ENT", "start_offset": 509, "end_offset": 523}, {"id": 1094774, "label": "ENT", "start_offset": 526, "end_offset": 540}, {"id": 1094775, "label": "ENT", "start_offset": 577, "end_offset": 612}], "relations": [{"id": 177616, "from_id": 1094767, "to_id": 1094766, "type": "USED-FOR"}, {"id": 177617, "from_id": 1094765, "to_id": 1094766, "type": "USED-FOR"}, {"id": 177618, "from_id": 1094773, "to_id": 1094775, "type": "HYPONYM-OF"}, {"id": 177619, "from_id": 1094770, "to_id": 1094772, "type": "USED-FOR"}]}
{"id": "A97-1052", "text": " We describe a novel technique and implemented system for constructing a  subcategorization dictionary  from  textual corpora . Each  dictionary entry  encodes the  relative frequency of occurrence  of a comprehensive set of  subcategorization classes  for  English . An initial experiment, on a sample of 14  verbs  which exhibit  multiple complementation patterns , demonstrates that the technique achieves  accuracy  comparable to previous approaches, which are all limited to a highly restricted set of  subcategorization classes . We also demonstrate that a  subcategorization dictionary  built with the system improves the  accuracy  of a  parser  by an appreciable amount", "Comments": [], "entities": [{"id": 1094776, "label": "ENT", "start_offset": 47, "end_offset": 53}, {"id": 1094777, "label": "ENT", "start_offset": 74, "end_offset": 102}, {"id": 1094778, "label": "ENT", "start_offset": 110, "end_offset": 125}, {"id": 1094779, "label": "ENT", "start_offset": 165, "end_offset": 197}, {"id": 1094780, "label": "ENT", "start_offset": 564, "end_offset": 592}, {"id": 1094781, "label": "ENT", "start_offset": 609, "end_offset": 615}, {"id": 1094782, "label": "ENT", "start_offset": 630, "end_offset": 638}, {"id": 1094783, "label": "ENT", "start_offset": 646, "end_offset": 652}], "relations": [{"id": 177620, "from_id": 1094783, "to_id": 1094780, "type": "EVALUATE-FOR"}, {"id": 177621, "from_id": 1094776, "to_id": 1094777, "type": "USED-FOR"}, {"id": 177622, "from_id": 1094781, "to_id": 1094776, "type": "COREF"}, {"id": 177623, "from_id": 1094778, "to_id": 1094776, "type": "USED-FOR"}, {"id": 177624, "from_id": 1094782, "to_id": 1094783, "type": "EVALUATE-FOR"}, {"id": 177625, "from_id": 1094781, "to_id": 1094780, "type": "USED-FOR"}]}
{"id": "ICCV_2001_50_abs", "text": "We present a new model-based bundle adjustment algorithm to recover the 3D model of a scene/object from a sequence of images with unknown motions. Instead of representing scene/object by a collection of isolated 3D features (usually points), our algorithm uses a surface controlled by a small set of parameters. Compared with previous model-based approaches, our approach has the following advantages. First, instead of using the model space as a regular-izer, we directly use it as our search space, thus resulting in a more elegant formulation with fewer unknowns and fewer equations. Second, our algorithm automatically associates tracked points with their correct locations on the surfaces, thereby eliminating the need for a prior 2D-to-3D association. Third, regarding face modeling, we use a very small set of face metrics (meaningful deformations) to parame-terize the face geometry, resulting in a smaller search space and a better posed system. Experiments with both synthetic and real data show that this new algorithm is faster, more accurate and more stable than existing ones.", "Comments": [], "entities": [{"id": 1094829, "label": "ENT", "start_offset": 17, "end_offset": 56}, {"id": 1094830, "label": "ENT", "start_offset": 72, "end_offset": 80}, {"id": 1094831, "label": "ENT", "start_offset": 118, "end_offset": 124}, {"id": 1094832, "label": "ENT", "start_offset": 130, "end_offset": 145}, {"id": 1094833, "label": "ENT", "start_offset": 203, "end_offset": 223}, {"id": 1094834, "label": "ENT", "start_offset": 246, "end_offset": 255}, {"id": 1094835, "label": "ENT", "start_offset": 263, "end_offset": 270}, {"id": 1094836, "label": "ENT", "start_offset": 335, "end_offset": 357}, {"id": 1094837, "label": "ENT", "start_offset": 363, "end_offset": 371}, {"id": 1094838, "label": "ENT", "start_offset": 430, "end_offset": 441}, {"id": 1094839, "label": "ENT", "start_offset": 447, "end_offset": 459}, {"id": 1094840, "label": "ENT", "start_offset": 477, "end_offset": 479}, {"id": 1094841, "label": "ENT", "start_offset": 487, "end_offset": 499}, {"id": 1094842, "label": "ENT", "start_offset": 599, "end_offset": 608}, {"id": 1094843, "label": "ENT", "start_offset": 685, "end_offset": 693}, {"id": 1094844, "label": "ENT", "start_offset": 730, "end_offset": 756}, {"id": 1094845, "label": "ENT", "start_offset": 775, "end_offset": 788}, {"id": 1094846, "label": "ENT", "start_offset": 817, "end_offset": 829}, {"id": 1094847, "label": "ENT", "start_offset": 877, "end_offset": 890}, {"id": 1094848, "label": "ENT", "start_offset": 915, "end_offset": 927}, {"id": 1094849, "label": "ENT", "start_offset": 941, "end_offset": 953}, {"id": 1094850, "label": "ENT", "start_offset": 977, "end_offset": 1000}, {"id": 1094851, "label": "ENT", "start_offset": 1020, "end_offset": 1029}, {"id": 1094852, "label": "ENT", "start_offset": 1085, "end_offset": 1089}], "relations": [{"id": 177659, "from_id": 1094832, "to_id": 1094831, "type": "PART-OF"}, {"id": 177660, "from_id": 1094831, "to_id": 1094830, "type": "USED-FOR"}, {"id": 177661, "from_id": 1094829, "to_id": 1094830, "type": "USED-FOR"}, {"id": 177662, "from_id": 1094829, "to_id": 1094834, "type": "COREF"}, {"id": 177663, "from_id": 1094835, "to_id": 1094834, "type": "USED-FOR"}, {"id": 177664, "from_id": 1094837, "to_id": 1094834, "type": "COREF"}, {"id": 177665, "from_id": 1094836, "to_id": 1094837, "type": "COMPARE"}, {"id": 177666, "from_id": 1094838, "to_id": 1094840, "type": "COREF"}, {"id": 177667, "from_id": 1094840, "to_id": 1094841, "type": "USED-FOR"}, {"id": 177668, "from_id": 1094838, "to_id": 1094839, "type": "USED-FOR"}, {"id": 177669, "from_id": 1094839, "to_id": 1094841, "type": "COMPARE"}, {"id": 177670, "from_id": 1094837, "to_id": 1094842, "type": "COREF"}, {"id": 177671, "from_id": 1094842, "to_id": 1094851, "type": "COREF"}, {"id": 177672, "from_id": 1094850, "to_id": 1094851, "type": "EVALUATE-FOR"}, {"id": 177673, "from_id": 1094836, "to_id": 1094852, "type": "COREF"}, {"id": 177674, "from_id": 1094851, "to_id": 1094852, "type": "COMPARE"}, {"id": 177675, "from_id": 1094850, "to_id": 1094852, "type": "EVALUATE-FOR"}, {"id": 177676, "from_id": 1094846, "to_id": 1094845, "type": "USED-FOR"}, {"id": 177677, "from_id": 1094846, "to_id": 1094847, "type": "USED-FOR"}, {"id": 177678, "from_id": 1094846, "to_id": 1094848, "type": "USED-FOR"}, {"id": 177679, "from_id": 1094846, "to_id": 1094849, "type": "USED-FOR"}]}
{"id": "C04-1024", "text": " An efficient  bit-vector-based CKY-style parser  for  context-free parsing  is presented. The  parser  computes a compact  parse forest representation  of the complete set of possible  analyses for large treebank grammars  and long  input sentences . The  parser  uses  bit-vector operations  to parallelise the  basic parsing operations . The  parser  is particularly useful when all analyses are needed rather than just the most probable one. ", "Comments": [], "entities": [{"id": 1094862, "label": "ENT", "start_offset": 15, "end_offset": 48}, {"id": 1094863, "label": "ENT", "start_offset": 55, "end_offset": 75}, {"id": 1094864, "label": "ENT", "start_offset": 96, "end_offset": 102}, {"id": 1094865, "label": "ENT", "start_offset": 124, "end_offset": 151}, {"id": 1094866, "label": "ENT", "start_offset": 199, "end_offset": 222}, {"id": 1094867, "label": "ENT", "start_offset": 257, "end_offset": 263}, {"id": 1094868, "label": "ENT", "start_offset": 271, "end_offset": 292}, {"id": 1094869, "label": "ENT", "start_offset": 346, "end_offset": 352}], "relations": [{"id": 177687, "from_id": 1094862, "to_id": 1094863, "type": "USED-FOR"}, {"id": 177688, "from_id": 1094868, "to_id": 1094867, "type": "USED-FOR"}, {"id": 177689, "from_id": 1094864, "to_id": 1094862, "type": "COREF"}, {"id": 177690, "from_id": 1094867, "to_id": 1094864, "type": "COREF"}, {"id": 177691, "from_id": 1094869, "to_id": 1094867, "type": "COREF"}, {"id": 177692, "from_id": 1094865, "to_id": 1094866, "type": "USED-FOR"}, {"id": 177693, "from_id": 1094864, "to_id": 1094865, "type": "USED-FOR"}]}
{"id": "E85-1037", "text": "  Systemic grammar  has been used for  AI text generation  work in the past, but the  implementations  have tended be ad hoc or inefficient. This paper presents an approach to systemic  text generation  where  AI problem solving techniques  are applied directly to an unadulterated  systemic grammar . This  approach  is made possible by a special relationship between  systemic grammar  and  problem solving : both are organized primarily as choosing from alternatives. The result is simple, efficient  text generation  firmly based in a  linguistic theory . ", "Comments": [], "entities": [{"id": 1094870, "label": "ENT", "start_offset": 2, "end_offset": 18}, {"id": 1094871, "label": "ENT", "start_offset": 39, "end_offset": 57}, {"id": 1094872, "label": "ENT", "start_offset": 86, "end_offset": 101}, {"id": 1094873, "label": "ENT", "start_offset": 164, "end_offset": 172}, {"id": 1094874, "label": "ENT", "start_offset": 186, "end_offset": 201}, {"id": 1094875, "label": "ENT", "start_offset": 210, "end_offset": 239}, {"id": 1094876, "label": "ENT", "start_offset": 283, "end_offset": 299}, {"id": 1094877, "label": "ENT", "start_offset": 308, "end_offset": 316}, {"id": 1094878, "label": "ENT", "start_offset": 370, "end_offset": 386}, {"id": 1094879, "label": "ENT", "start_offset": 393, "end_offset": 408}, {"id": 1094880, "label": "ENT", "start_offset": 504, "end_offset": 519}, {"id": 1094881, "label": "ENT", "start_offset": 540, "end_offset": 557}], "relations": [{"id": 177694, "from_id": 1094870, "to_id": 1094871, "type": "USED-FOR"}, {"id": 177695, "from_id": 1094875, "to_id": 1094876, "type": "USED-FOR"}, {"id": 177696, "from_id": 1094881, "to_id": 1094880, "type": "USED-FOR"}, {"id": 177697, "from_id": 1094872, "to_id": 1094870, "type": "COREF"}, {"id": 177698, "from_id": 1094873, "to_id": 1094874, "type": "USED-FOR"}, {"id": 177699, "from_id": 1094874, "to_id": 1094871, "type": "COREF"}, {"id": 177700, "from_id": 1094875, "to_id": 1094873, "type": "COREF"}, {"id": 177701, "from_id": 1094877, "to_id": 1094873, "type": "COREF"}, {"id": 177702, "from_id": 1094878, "to_id": 1094879, "type": "CONJUNCTION"}, {"id": 177703, "from_id": 1094880, "to_id": 1094874, "type": "COREF"}, {"id": 177704, "from_id": 1094879, "to_id": 1094875, "type": "COREF"}]}
{"id": "H92-1026", "text": "   We describe a  generative probabilistic model of natural language  , which we call  HBG  , that takes advantage of detailed  linguistic information  to resolve  ambiguity  .   HBG  incorporates  lexical, syntactic, semantic, and structural information  from the  parse tree  into the  disambiguation process  in a novel way. We use a  corpus of bracketed sentences  , called a  Treebank  , in combination with  decision tree building  to tease out the relevant aspects of a  parse tree  that will determine the correct  parse  of a  sentence  . This stands in contrast to the usual approach of further  grammar  tailoring via the usual  linguistic introspection  in the hope of generating the correct  parse  . In  head-to-head tests  against one of the best existing robust  probabilistic parsing models  , which we call  P-CFG , the  HBG model  significantly outperforms  P-CFG  , increasing the  parsing accuracy  rate from 60% to 75%, a 37% reduction in error. ", "Comments": [], "entities": [{"id": 1094882, "label": "ENT", "start_offset": 18, "end_offset": 68}, {"id": 1094883, "label": "ENT", "start_offset": 87, "end_offset": 90}, {"id": 1094884, "label": "ENT", "start_offset": 128, "end_offset": 150}, {"id": 1094885, "label": "ENT", "start_offset": 164, "end_offset": 173}, {"id": 1094886, "label": "ENT", "start_offset": 179, "end_offset": 182}, {"id": 1094887, "label": "ENT", "start_offset": 198, "end_offset": 254}, {"id": 1094888, "label": "ENT", "start_offset": 266, "end_offset": 276}, {"id": 1094889, "label": "ENT", "start_offset": 288, "end_offset": 310}, {"id": 1094890, "label": "ENT", "start_offset": 338, "end_offset": 367}, {"id": 1094891, "label": "ENT", "start_offset": 381, "end_offset": 389}, {"id": 1094892, "label": "ENT", "start_offset": 414, "end_offset": 436}, {"id": 1094893, "label": "ENT", "start_offset": 478, "end_offset": 488}, {"id": 1094894, "label": "ENT", "start_offset": 523, "end_offset": 528}, {"id": 1094895, "label": "ENT", "start_offset": 606, "end_offset": 624}, {"id": 1094896, "label": "ENT", "start_offset": 640, "end_offset": 664}, {"id": 1094897, "label": "ENT", "start_offset": 705, "end_offset": 710}, {"id": 1094898, "label": "ENT", "start_offset": 718, "end_offset": 736}, {"id": 1094899, "label": "ENT", "start_offset": 771, "end_offset": 807}, {"id": 1094900, "label": "ENT", "start_offset": 826, "end_offset": 831}, {"id": 1094901, "label": "ENT", "start_offset": 839, "end_offset": 848}, {"id": 1094902, "label": "ENT", "start_offset": 877, "end_offset": 882}, {"id": 1094903, "label": "ENT", "start_offset": 902, "end_offset": 924}], "relations": [{"id": 177705, "from_id": 1094884, "to_id": 1094885, "type": "USED-FOR"}, {"id": 177706, "from_id": 1094887, "to_id": 1094886, "type": "USED-FOR"}, {"id": 177707, "from_id": 1094901, "to_id": 1094902, "type": "COMPARE"}, {"id": 177708, "from_id": 1094883, "to_id": 1094882, "type": "COREF"}, {"id": 177709, "from_id": 1094886, "to_id": 1094883, "type": "COREF"}, {"id": 177710, "from_id": 1094886, "to_id": 1094889, "type": "USED-FOR"}, {"id": 177711, "from_id": 1094891, "to_id": 1094890, "type": "COREF"}, {"id": 177712, "from_id": 1094890, "to_id": 1094892, "type": "CONJUNCTION"}, {"id": 177713, "from_id": 1094892, "to_id": 1094893, "type": "USED-FOR"}, {"id": 177714, "from_id": 1094900, "to_id": 1094902, "type": "COREF"}, {"id": 177715, "from_id": 1094901, "to_id": 1094886, "type": "COREF"}, {"id": 177716, "from_id": 1094896, "to_id": 1094895, "type": "USED-FOR"}, {"id": 177717, "from_id": 1094895, "to_id": 1094897, "type": "USED-FOR"}, {"id": 177718, "from_id": 1094890, "to_id": 1094893, "type": "USED-FOR"}, {"id": 177719, "from_id": 1094893, "to_id": 1094894, "type": "USED-FOR"}, {"id": 177720, "from_id": 1094903, "to_id": 1094901, "type": "EVALUATE-FOR"}, {"id": 177721, "from_id": 1094900, "to_id": 1094899, "type": "HYPONYM-OF"}]}
{"id": "C86-1132", "text": "   This paper describes a system (  RAREAS  ) which synthesizes marine weather forecasts directly from  formatted weather data  . Such  synthesis  appears feasible in certain  natural sublanguages  with  stereotyped text structure  .  RAREAS  draws on several kinds of  linguistic and non-linguistic knowledge  and mirrors a forecaster's apparent tendency to ascribe less precise  temporal adverbs  to more remote meteorological events. The approach can easily be adapted to synthesize  bilingual or multi-lingual texts  . ", "Comments": [], "entities": [{"id": 1094923, "label": "ENT", "start_offset": 26, "end_offset": 32}, {"id": 1094924, "label": "ENT", "start_offset": 36, "end_offset": 42}, {"id": 1094925, "label": "ENT", "start_offset": 64, "end_offset": 88}, {"id": 1094926, "label": "ENT", "start_offset": 104, "end_offset": 126}, {"id": 1094927, "label": "ENT", "start_offset": 136, "end_offset": 145}, {"id": 1094928, "label": "ENT", "start_offset": 176, "end_offset": 230}, {"id": 1094929, "label": "ENT", "start_offset": 235, "end_offset": 241}, {"id": 1094930, "label": "ENT", "start_offset": 270, "end_offset": 309}, {"id": 1094931, "label": "ENT", "start_offset": 441, "end_offset": 449}, {"id": 1094932, "label": "ENT", "start_offset": 487, "end_offset": 519}], "relations": [{"id": 177732, "from_id": 1094930, "to_id": 1094929, "type": "USED-FOR"}, {"id": 177733, "from_id": 1094924, "to_id": 1094923, "type": "COREF"}, {"id": 177734, "from_id": 1094927, "to_id": 1094923, "type": "COREF"}, {"id": 177735, "from_id": 1094929, "to_id": 1094927, "type": "COREF"}, {"id": 177736, "from_id": 1094931, "to_id": 1094929, "type": "COREF"}, {"id": 177737, "from_id": 1094932, "to_id": 1094931, "type": "USED-FOR"}, {"id": 177738, "from_id": 1094923, "to_id": 1094925, "type": "USED-FOR"}, {"id": 177739, "from_id": 1094926, "to_id": 1094923, "type": "USED-FOR"}, {"id": 177740, "from_id": 1094928, "to_id": 1094927, "type": "USED-FOR"}]}
{"id": "ICML_2006_122_abs", "text": "In the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or \"simulator\") of the Markov decision process. However, for high-dimensional continuous-state tasks, it can be extremely difficult to build an accurate model, and thus often the algorithm returns a policy that works in simulation but not in real-life. The other extreme, model-free RL, tends to require infeasibly large numbers of real-life trials. In this paper, we present a hybrid algorithm that requires only an approximate model, and only a small number of real-life trials. The key idea is to successively \"ground\" the policy evaluations using real-life trials, but to rely on the approximate model to suggest local changes. Our theoretical results show that this algorithm achieves near-optimal performance in the real system, even when the model is only approximate. Empirical results also demonstrate that---when given only a crude model and a small number of real-life trials---our algorithm can obtain near-optimal performance in the real system.", "Comments": [], "entities": [{"id": 1094986, "label": "ENT", "start_offset": 7, "end_offset": 41}, {"id": 1094987, "label": "ENT", "start_offset": 45, "end_offset": 72}, {"id": 1094988, "label": "ENT", "start_offset": 74, "end_offset": 82}, {"id": 1094989, "label": "ENT", "start_offset": 131, "end_offset": 154}, {"id": 1094990, "label": "ENT", "start_offset": 169, "end_offset": 208}, {"id": 1094991, "label": "ENT", "start_offset": 261, "end_offset": 266}, {"id": 1094992, "label": "ENT", "start_offset": 287, "end_offset": 296}, {"id": 1094993, "label": "ENT", "start_offset": 307, "end_offset": 313}, {"id": 1094994, "label": "ENT", "start_offset": 380, "end_offset": 393}, {"id": 1094995, "label": "ENT", "start_offset": 440, "end_offset": 456}, {"id": 1094996, "label": "ENT", "start_offset": 486, "end_offset": 502}, {"id": 1094997, "label": "ENT", "start_offset": 525, "end_offset": 542}, {"id": 1094998, "label": "ENT", "start_offset": 571, "end_offset": 587}, {"id": 1094999, "label": "ENT", "start_offset": 634, "end_offset": 652}, {"id": 1095000, "label": "ENT", "start_offset": 659, "end_offset": 675}, {"id": 1095001, "label": "ENT", "start_offset": 696, "end_offset": 713}, {"id": 1095002, "label": "ENT", "start_offset": 779, "end_offset": 788}, {"id": 1095003, "label": "ENT", "start_offset": 798, "end_offset": 822}, {"id": 1095004, "label": "ENT", "start_offset": 944, "end_offset": 955}, {"id": 1095005, "label": "ENT", "start_offset": 978, "end_offset": 994}, {"id": 1095006, "label": "ENT", "start_offset": 1001, "end_offset": 1010}, {"id": 1095007, "label": "ENT", "start_offset": 1022, "end_offset": 1046}], "relations": [{"id": 177777, "from_id": 1094986, "to_id": 1094987, "type": "USED-FOR"}, {"id": 177778, "from_id": 1094989, "to_id": 1094988, "type": "USED-FOR"}, {"id": 177779, "from_id": 1094998, "to_id": 1094997, "type": "USED-FOR"}, {"id": 177780, "from_id": 1094997, "to_id": 1094996, "type": "USED-FOR"}, {"id": 177781, "from_id": 1095000, "to_id": 1094999, "type": "USED-FOR"}, {"id": 177782, "from_id": 1095002, "to_id": 1095006, "type": "COREF"}, {"id": 177783, "from_id": 1095005, "to_id": 1095006, "type": "USED-FOR"}, {"id": 177784, "from_id": 1095004, "to_id": 1095006, "type": "USED-FOR"}, {"id": 177785, "from_id": 1094995, "to_id": 1094994, "type": "USED-FOR"}, {"id": 177786, "from_id": 1094992, "to_id": 1094993, "type": "USED-FOR"}, {"id": 177787, "from_id": 1094991, "to_id": 1094990, "type": "USED-FOR"}, {"id": 177788, "from_id": 1094991, "to_id": 1094992, "type": "COREF"}, {"id": 177789, "from_id": 1095004, "to_id": 1095005, "type": "CONJUNCTION"}]}
{"id": "C02-1120", "text": " This paper describes an  unsupervised learning method  for  associative relationships between verb phrases , which is important in developing reliable  Q&A systems . Consider the situation that a user gives a  query  \"How much petrol was imported to Japan from Saudi Arabia?\" to a  Q&A system , but the  text  given to the system includes only the  description  \"X tonnes of petrol was conveyed to Japan from Saudi Arabia\". We think that the  description  is a good clue to find the answer for our  query , \"X tonnes\". But there is no  large-scale database  that provides the  associative relationship  between \"imported\" and \"conveyed\". Our aim is to develop an  unsupervised learning method  that can obtain such an  associative relationship , which we call  scenario consistency . The method we are currently working on uses an  expectation-maximization (EM) based word-clustering algorithm , and we have evaluated the effectiveness of this method using  Japanese verb phrases .  ", "Comments": [], "entities": [{"id": 1095008, "label": "ENT", "start_offset": 26, "end_offset": 54}, {"id": 1095009, "label": "ENT", "start_offset": 61, "end_offset": 107}, {"id": 1095010, "label": "ENT", "start_offset": 153, "end_offset": 164}, {"id": 1095011, "label": "ENT", "start_offset": 283, "end_offset": 293}, {"id": 1095012, "label": "ENT", "start_offset": 537, "end_offset": 557}, {"id": 1095013, "label": "ENT", "start_offset": 578, "end_offset": 602}, {"id": 1095014, "label": "ENT", "start_offset": 665, "end_offset": 693}, {"id": 1095015, "label": "ENT", "start_offset": 720, "end_offset": 744}, {"id": 1095016, "label": "ENT", "start_offset": 762, "end_offset": 782}, {"id": 1095017, "label": "ENT", "start_offset": 789, "end_offset": 795}, {"id": 1095018, "label": "ENT", "start_offset": 833, "end_offset": 894}, {"id": 1095019, "label": "ENT", "start_offset": 945, "end_offset": 951}, {"id": 1095020, "label": "ENT", "start_offset": 959, "end_offset": 980}], "relations": [{"id": 177790, "from_id": 1095008, "to_id": 1095009, "type": "USED-FOR"}, {"id": 177791, "from_id": 1095014, "to_id": 1095015, "type": "USED-FOR"}, {"id": 177792, "from_id": 1095009, "to_id": 1095010, "type": "USED-FOR"}, {"id": 177793, "from_id": 1095017, "to_id": 1095014, "type": "COREF"}, {"id": 177794, "from_id": 1095018, "to_id": 1095017, "type": "USED-FOR"}, {"id": 177795, "from_id": 1095019, "to_id": 1095017, "type": "COREF"}, {"id": 177796, "from_id": 1095020, "to_id": 1095019, "type": "USED-FOR"}, {"id": 177797, "from_id": 1095015, "to_id": 1095013, "type": "COREF"}, {"id": 177798, "from_id": 1095016, "to_id": 1095015, "type": "COREF"}]}
{"id": "AAAI_2015_21_abs", "text": "Semantic Web documents that encode facts about entities on the Web have been growing rapidly in size and evolving over time. Creating summaries on lengthy Semantic Web documents for quick identification of the corresponding entity has been of great contemporary interest. In this paper, we explore automatic summa-rization techniques that characterize and enable identification of an entity and create summaries that are human friendly. Specifically, we highlight the importance of diversified (faceted) summaries by combining three dimensions: diversity, uniqueness, and popularity. Our novel diversity-aware entity summarization approach mimics human conceptual clustering techniques to group facts, and picks representative facts from each group to form concise (i.e., short) and comprehensive (i.e., improved coverage through diversity) summaries. We evaluate our approach against the state-of-the-art techniques and show that our work improves both the quality and the efficiency of entity summarization.", "Comments": [], "entities": [{"id": 1095021, "label": "ENT", "start_offset": 0, "end_offset": 22}, {"id": 1095022, "label": "ENT", "start_offset": 125, "end_offset": 143}, {"id": 1095023, "label": "ENT", "start_offset": 147, "end_offset": 177}, {"id": 1095024, "label": "ENT", "start_offset": 188, "end_offset": 230}, {"id": 1095025, "label": "ENT", "start_offset": 298, "end_offset": 333}, {"id": 1095026, "label": "ENT", "start_offset": 482, "end_offset": 513}, {"id": 1095027, "label": "ENT", "start_offset": 545, "end_offset": 554}, {"id": 1095028, "label": "ENT", "start_offset": 556, "end_offset": 566}, {"id": 1095029, "label": "ENT", "start_offset": 572, "end_offset": 582}, {"id": 1095030, "label": "ENT", "start_offset": 594, "end_offset": 639}, {"id": 1095031, "label": "ENT", "start_offset": 647, "end_offset": 685}, {"id": 1095032, "label": "ENT", "start_offset": 868, "end_offset": 876}, {"id": 1095033, "label": "ENT", "start_offset": 889, "end_offset": 916}, {"id": 1095034, "label": "ENT", "start_offset": 958, "end_offset": 965}, {"id": 1095035, "label": "ENT", "start_offset": 974, "end_offset": 984}, {"id": 1095036, "label": "ENT", "start_offset": 988, "end_offset": 1008}], "relations": [{"id": 177799, "from_id": 1095023, "to_id": 1095022, "type": "USED-FOR"}, {"id": 177800, "from_id": 1095022, "to_id": 1095024, "type": "USED-FOR"}, {"id": 177801, "from_id": 1095027, "to_id": 1095026, "type": "FEATURE-OF"}, {"id": 177802, "from_id": 1095028, "to_id": 1095026, "type": "FEATURE-OF"}, {"id": 177803, "from_id": 1095029, "to_id": 1095026, "type": "FEATURE-OF"}, {"id": 177804, "from_id": 1095027, "to_id": 1095028, "type": "CONJUNCTION"}, {"id": 177805, "from_id": 1095028, "to_id": 1095029, "type": "CONJUNCTION"}, {"id": 177806, "from_id": 1095030, "to_id": 1095032, "type": "COREF"}, {"id": 177807, "from_id": 1095033, "to_id": 1095032, "type": "COMPARE"}, {"id": 177808, "from_id": 1095035, "to_id": 1095036, "type": "EVALUATE-FOR"}, {"id": 177809, "from_id": 1095034, "to_id": 1095036, "type": "EVALUATE-FOR"}, {"id": 177810, "from_id": 1095032, "to_id": 1095036, "type": "USED-FOR"}, {"id": 177811, "from_id": 1095033, "to_id": 1095036, "type": "USED-FOR"}, {"id": 177812, "from_id": 1095031, "to_id": 1095030, "type": "USED-FOR"}]}
{"id": "J86-3001", "text": "   In this paper we explore a new  theory of discourse structure  that stresses the role of  purpose  and  processing  in  discourse  . In this theory,  discourse structure  is composed of three separate but interrelated components: the structure of the sequence of  utterances  (called the  linguistic structure  ), a structure of  purposes  (called the  intentional structure  ), and the state of  focus of attention  (called the  attentional state  ). The  linguistic structure  consists of segments of the  discourse  into which the  utterances  naturally aggregate. The  intentional structure  captures the  discourse-relevant purposes  , expressed in each of the  linguistic segments  as well as relationships among them. The  attentional state  is an abstraction of the  focus of attention  of the  participants  as the  discourse  unfolds. The  attentional state  , being dynamic, records the objects, properties, and relations that are salient at each point of the  discourse  . The distinction among these components is essential to provide an adequate explanation of such  discourse phenomena  as  cue phrases  ,  referring expressions  , and  interruptions  . The  theory of attention, intention, and aggregation of utterances  is illustrated in the paper with a number of example  discourses  . Various properties of  discourse  are described, and explanations for the behaviour of  cue phrases  ,  referring expressions  , and  interruptions  are explored. This  theory  provides a framework for describing the processing of  utterances  in a  discourse  .  Discourse processing  requires recognizing how the  utterances  of the  discourse  aggregate into  segments  , recognizing the  intentions  expressed in the  discourse  and the relationships among  intentions  , and tracking the  discourse  through the operation of the mechanisms associated with  attentional state  . This processing description specifies in these  recognition tasks  the role of information from the  discourse  and from the  participants  ' knowledge of the domain. ", "Comments": [], "entities": [{"id": 1095067, "label": "ENT", "start_offset": 35, "end_offset": 64}, {"id": 1095068, "label": "ENT", "start_offset": 123, "end_offset": 132}, {"id": 1095069, "label": "ENT", "start_offset": 144, "end_offset": 150}, {"id": 1095070, "label": "ENT", "start_offset": 153, "end_offset": 172}, {"id": 1095071, "label": "ENT", "start_offset": 221, "end_offset": 231}, {"id": 1095072, "label": "ENT", "start_offset": 292, "end_offset": 312}, {"id": 1095073, "label": "ENT", "start_offset": 356, "end_offset": 377}, {"id": 1095074, "label": "ENT", "start_offset": 433, "end_offset": 450}, {"id": 1095075, "label": "ENT", "start_offset": 460, "end_offset": 480}, {"id": 1095076, "label": "ENT", "start_offset": 511, "end_offset": 520}, {"id": 1095077, "label": "ENT", "start_offset": 576, "end_offset": 597}, {"id": 1095078, "label": "ENT", "start_offset": 613, "end_offset": 640}, {"id": 1095079, "label": "ENT", "start_offset": 733, "end_offset": 750}, {"id": 1095080, "label": "ENT", "start_offset": 828, "end_offset": 837}, {"id": 1095081, "label": "ENT", "start_offset": 853, "end_offset": 870}, {"id": 1095082, "label": "ENT", "start_offset": 975, "end_offset": 984}, {"id": 1095083, "label": "ENT", "start_offset": 1084, "end_offset": 1103}, {"id": 1095084, "label": "ENT", "start_offset": 1109, "end_offset": 1120}, {"id": 1095085, "label": "ENT", "start_offset": 1125, "end_offset": 1146}, {"id": 1095086, "label": "ENT", "start_offset": 1155, "end_offset": 1168}, {"id": 1095087, "label": "ENT", "start_offset": 1177, "end_offset": 1238}, {"id": 1095088, "label": "ENT", "start_offset": 1294, "end_offset": 1304}, {"id": 1095089, "label": "ENT", "start_offset": 1331, "end_offset": 1340}, {"id": 1095090, "label": "ENT", "start_offset": 1396, "end_offset": 1407}, {"id": 1095091, "label": "ENT", "start_offset": 1412, "end_offset": 1433}, {"id": 1095092, "label": "ENT", "start_offset": 1442, "end_offset": 1455}, {"id": 1095093, "label": "ENT", "start_offset": 1477, "end_offset": 1483}, {"id": 1095094, "label": "ENT", "start_offset": 1558, "end_offset": 1567}, {"id": 1095095, "label": "ENT", "start_offset": 1572, "end_offset": 1592}, {"id": 1095096, "label": "ENT", "start_offset": 1644, "end_offset": 1653}, {"id": 1095097, "label": "ENT", "start_offset": 1730, "end_offset": 1739}, {"id": 1095098, "label": "ENT", "start_offset": 1802, "end_offset": 1811}, {"id": 1095099, "label": "ENT", "start_offset": 1870, "end_offset": 1887}, {"id": 1095100, "label": "ENT", "start_offset": 1896, "end_offset": 1906}, {"id": 1095101, "label": "ENT", "start_offset": 1939, "end_offset": 1956}, {"id": 1095102, "label": "ENT", "start_offset": 1992, "end_offset": 2001}], "relations": [{"id": 177833, "from_id": 1095077, "to_id": 1095078, "type": "USED-FOR"}, {"id": 177834, "from_id": 1095071, "to_id": 1095070, "type": "PART-OF"}, {"id": 177835, "from_id": 1095072, "to_id": 1095071, "type": "PART-OF"}, {"id": 177836, "from_id": 1095073, "to_id": 1095071, "type": "PART-OF"}, {"id": 177837, "from_id": 1095074, "to_id": 1095071, "type": "PART-OF"}, {"id": 177838, "from_id": 1095072, "to_id": 1095073, "type": "CONJUNCTION"}, {"id": 177839, "from_id": 1095073, "to_id": 1095074, "type": "CONJUNCTION"}, {"id": 177840, "from_id": 1095075, "to_id": 1095072, "type": "COREF"}, {"id": 177841, "from_id": 1095077, "to_id": 1095073, "type": "COREF"}, {"id": 177842, "from_id": 1095079, "to_id": 1095074, "type": "COREF"}, {"id": 177843, "from_id": 1095081, "to_id": 1095079, "type": "COREF"}, {"id": 177844, "from_id": 1095084, "to_id": 1095083, "type": "HYPONYM-OF"}, {"id": 177845, "from_id": 1095085, "to_id": 1095083, "type": "HYPONYM-OF"}, {"id": 177846, "from_id": 1095086, "to_id": 1095083, "type": "HYPONYM-OF"}, {"id": 177847, "from_id": 1095084, "to_id": 1095085, "type": "CONJUNCTION"}, {"id": 177848, "from_id": 1095085, "to_id": 1095086, "type": "CONJUNCTION"}, {"id": 177849, "from_id": 1095093, "to_id": 1095087, "type": "COREF"}, {"id": 177850, "from_id": 1095100, "to_id": 1095095, "type": "COREF"}, {"id": 177851, "from_id": 1095069, "to_id": 1095067, "type": "COREF"}]}
{"id": "N03-2006", "text": " In order to boost the  translation quality  of  EBMT  based on a small-sized  bilingual corpus  , we use an out-of-domain  bilingual corpus  and, in addition, the  language model  of an in-domain  monolingual corpus  . We conducted experiments with an  EBMT system  . The two  evaluation measures  of the  BLEU score  and the  NIST score  demonstrated the effect of using an out-of-domain  bilingual corpus  and the possibility of using the  language model  . ", "Comments": [], "entities": [{"id": 1095103, "label": "ENT", "start_offset": 49, "end_offset": 53}, {"id": 1095104, "label": "ENT", "start_offset": 66, "end_offset": 95}, {"id": 1095105, "label": "ENT", "start_offset": 109, "end_offset": 140}, {"id": 1095106, "label": "ENT", "start_offset": 165, "end_offset": 179}, {"id": 1095107, "label": "ENT", "start_offset": 187, "end_offset": 216}, {"id": 1095108, "label": "ENT", "start_offset": 254, "end_offset": 265}, {"id": 1095109, "label": "ENT", "start_offset": 278, "end_offset": 297}, {"id": 1095110, "label": "ENT", "start_offset": 307, "end_offset": 317}, {"id": 1095111, "label": "ENT", "start_offset": 328, "end_offset": 338}, {"id": 1095112, "label": "ENT", "start_offset": 376, "end_offset": 407}, {"id": 1095113, "label": "ENT", "start_offset": 443, "end_offset": 457}], "relations": [{"id": 177852, "from_id": 1095106, "to_id": 1095103, "type": "USED-FOR"}, {"id": 177853, "from_id": 1095108, "to_id": 1095103, "type": "COREF"}, {"id": 177854, "from_id": 1095110, "to_id": 1095109, "type": "HYPONYM-OF"}, {"id": 177855, "from_id": 1095111, "to_id": 1095109, "type": "HYPONYM-OF"}, {"id": 177856, "from_id": 1095110, "to_id": 1095111, "type": "CONJUNCTION"}, {"id": 177857, "from_id": 1095109, "to_id": 1095113, "type": "EVALUATE-FOR"}, {"id": 177858, "from_id": 1095113, "to_id": 1095106, "type": "COREF"}, {"id": 177859, "from_id": 1095104, "to_id": 1095103, "type": "USED-FOR"}, {"id": 177860, "from_id": 1095107, "to_id": 1095106, "type": "USED-FOR"}, {"id": 177861, "from_id": 1095105, "to_id": 1095103, "type": "USED-FOR"}, {"id": 177862, "from_id": 1095109, "to_id": 1095112, "type": "USED-FOR"}, {"id": 177863, "from_id": 1095112, "to_id": 1095105, "type": "COREF"}]}
{"id": "C86-1081", "text": "    Determiners  play an important role in conveying the  meaning  of an  utterance  , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the  global meaning  of a  sentence  , even if not in a precise way. Another problem with  determiners  is their inherent  ambiguity  . In this paper we propose a  logical formalism  , which, among other things, is suitable for representing  determiners  without forcing a particular  interpretation  when their  meaning  is still not clear. ", "Comments": [], "entities": [{"id": 1095124, "label": "ENT", "start_offset": 4, "end_offset": 15}, {"id": 1095125, "label": "ENT", "start_offset": 284, "end_offset": 295}, {"id": 1095126, "label": "ENT", "start_offset": 316, "end_offset": 325}, {"id": 1095127, "label": "ENT", "start_offset": 357, "end_offset": 374}, {"id": 1095128, "label": "ENT", "start_offset": 435, "end_offset": 446}], "relations": [{"id": 177871, "from_id": 1095126, "to_id": 1095125, "type": "FEATURE-OF"}, {"id": 177872, "from_id": 1095127, "to_id": 1095128, "type": "USED-FOR"}, {"id": 177873, "from_id": 1095128, "to_id": 1095125, "type": "COREF"}, {"id": 177874, "from_id": 1095125, "to_id": 1095124, "type": "COREF"}]}
{"id": "E89-1040", "text": " Theoretical research in the area of  machine translation  usually involves the search for and creation of an appropriate  formalism . An important issue in this respect is the way in which the  compositionality  of  translation  is to be defined. In this paper, we will introduce the  anaphoric component  of the  Mimo formalism . It makes the definition and  translation  of  anaphoric relations  possible,  relations  which are usually problematic for systems that adhere to  strict compositionality . In  Mimo , the  translation  of  anaphoric relations  is compositional. The  anaphoric component  is used to define  linguistic phenomena  such as  wh-movement , the  passive  and the  binding of reflexives and pronouns  mono-lingually. The actual working of the component will be shown in this paper by means of a detailed discussion of  wh-movement . ", "Comments": [], "entities": [{"id": 1095216, "label": "ENT", "start_offset": 38, "end_offset": 57}, {"id": 1095217, "label": "ENT", "start_offset": 123, "end_offset": 132}, {"id": 1095218, "label": "ENT", "start_offset": 217, "end_offset": 228}, {"id": 1095219, "label": "ENT", "start_offset": 286, "end_offset": 305}, {"id": 1095220, "label": "ENT", "start_offset": 315, "end_offset": 329}, {"id": 1095221, "label": "ENT", "start_offset": 361, "end_offset": 397}, {"id": 1095222, "label": "ENT", "start_offset": 479, "end_offset": 502}, {"id": 1095223, "label": "ENT", "start_offset": 509, "end_offset": 513}, {"id": 1095224, "label": "ENT", "start_offset": 521, "end_offset": 557}, {"id": 1095225, "label": "ENT", "start_offset": 582, "end_offset": 601}, {"id": 1095226, "label": "ENT", "start_offset": 622, "end_offset": 642}, {"id": 1095227, "label": "ENT", "start_offset": 653, "end_offset": 664}, {"id": 1095228, "label": "ENT", "start_offset": 667, "end_offset": 724}, {"id": 1095229, "label": "ENT", "start_offset": 844, "end_offset": 855}], "relations": [{"id": 177939, "from_id": 1095217, "to_id": 1095216, "type": "USED-FOR"}, {"id": 177940, "from_id": 1095225, "to_id": 1095226, "type": "USED-FOR"}, {"id": 177941, "from_id": 1095220, "to_id": 1095217, "type": "COREF"}, {"id": 177942, "from_id": 1095223, "to_id": 1095220, "type": "COREF"}, {"id": 177943, "from_id": 1095225, "to_id": 1095219, "type": "COREF"}, {"id": 177944, "from_id": 1095227, "to_id": 1095226, "type": "HYPONYM-OF"}, {"id": 177945, "from_id": 1095229, "to_id": 1095227, "type": "COREF"}, {"id": 177946, "from_id": 1095219, "to_id": 1095220, "type": "PART-OF"}, {"id": 177947, "from_id": 1095224, "to_id": 1095221, "type": "COREF"}, {"id": 177948, "from_id": 1095223, "to_id": 1095224, "type": "USED-FOR"}, {"id": 177949, "from_id": 1095228, "to_id": 1095226, "type": "HYPONYM-OF"}, {"id": 177950, "from_id": 1095227, "to_id": 1095228, "type": "CONJUNCTION"}]}
{"id": "ICML_1995_38_abs", "text": "An important area of learning in autonomous agents is the ability to learn domain-speciic models of actions to be used by planning systems. In this paper, we present methods by which an agent learns action models from its own experience and from its observation of a domain expert. These methods diier from previous work in the area in two ways: the use of an action model formalism which is better suited to the needs of a re-active agent, and successful implementation of noise-handling mechanisms. Training instances are generated from experience and observation, and a variant of GOLEM is used to learn action models from these instances. The integrated learning system has been experimentally validated in simulated construction and ooce domains.", "Comments": [], "entities": [{"id": 1095230, "label": "ENT", "start_offset": 21, "end_offset": 50}, {"id": 1095231, "label": "ENT", "start_offset": 75, "end_offset": 107}, {"id": 1095232, "label": "ENT", "start_offset": 122, "end_offset": 138}, {"id": 1095233, "label": "ENT", "start_offset": 166, "end_offset": 173}, {"id": 1095234, "label": "ENT", "start_offset": 199, "end_offset": 212}, {"id": 1095235, "label": "ENT", "start_offset": 267, "end_offset": 280}, {"id": 1095236, "label": "ENT", "start_offset": 288, "end_offset": 295}, {"id": 1095237, "label": "ENT", "start_offset": 360, "end_offset": 382}, {"id": 1095238, "label": "ENT", "start_offset": 424, "end_offset": 439}, {"id": 1095239, "label": "ENT", "start_offset": 474, "end_offset": 499}, {"id": 1095240, "label": "ENT", "start_offset": 584, "end_offset": 589}, {"id": 1095241, "label": "ENT", "start_offset": 607, "end_offset": 620}, {"id": 1095242, "label": "ENT", "start_offset": 647, "end_offset": 673}, {"id": 1095243, "label": "ENT", "start_offset": 711, "end_offset": 733}, {"id": 1095244, "label": "ENT", "start_offset": 738, "end_offset": 750}], "relations": [{"id": 177951, "from_id": 1095233, "to_id": 1095236, "type": "COREF"}, {"id": 177952, "from_id": 1095237, "to_id": 1095238, "type": "USED-FOR"}, {"id": 177953, "from_id": 1095240, "to_id": 1095241, "type": "USED-FOR"}, {"id": 177954, "from_id": 1095241, "to_id": 1095234, "type": "COREF"}, {"id": 177955, "from_id": 1095230, "to_id": 1095231, "type": "USED-FOR"}, {"id": 177956, "from_id": 1095232, "to_id": 1095231, "type": "USED-FOR"}, {"id": 177957, "from_id": 1095243, "to_id": 1095244, "type": "CONJUNCTION"}, {"id": 177958, "from_id": 1095243, "to_id": 1095242, "type": "EVALUATE-FOR"}, {"id": 177959, "from_id": 1095244, "to_id": 1095242, "type": "EVALUATE-FOR"}, {"id": 177960, "from_id": 1095237, "to_id": 1095236, "type": "USED-FOR"}, {"id": 177961, "from_id": 1095239, "to_id": 1095236, "type": "USED-FOR"}, {"id": 177962, "from_id": 1095236, "to_id": 1095242, "type": "COREF"}, {"id": 177963, "from_id": 1095234, "to_id": 1095231, "type": "COREF"}]}
{"id": "CVPR_2011_10_abs", "text": "Reflections in image sequences consist of several layers superimposed over each other. This phenomenon causes many image processing techniques to fail as they assume the presence of only one layer at each examined site e.g. motion estimation and object recognition. This work presents an automated technique for detecting reflections in image sequences by analyzing motion trajectories of feature points. It models reflection as regions containing two different layers moving over each other. We present a strong detector based on combining a set of weak detectors. We use novel priors, generate sparse and dense detection maps and our results show high detection rate with rejection to pathological motion and occlusion.", "Comments": [], "entities": [{"id": 1095245, "label": "ENT", "start_offset": 0, "end_offset": 11}, {"id": 1095246, "label": "ENT", "start_offset": 15, "end_offset": 30}, {"id": 1095247, "label": "ENT", "start_offset": 115, "end_offset": 142}, {"id": 1095248, "label": "ENT", "start_offset": 224, "end_offset": 241}, {"id": 1095249, "label": "ENT", "start_offset": 246, "end_offset": 264}, {"id": 1095250, "label": "ENT", "start_offset": 298, "end_offset": 307}, {"id": 1095251, "label": "ENT", "start_offset": 312, "end_offset": 352}, {"id": 1095252, "label": "ENT", "start_offset": 366, "end_offset": 385}, {"id": 1095253, "label": "ENT", "start_offset": 389, "end_offset": 403}, {"id": 1095254, "label": "ENT", "start_offset": 405, "end_offset": 407}, {"id": 1095255, "label": "ENT", "start_offset": 415, "end_offset": 425}, {"id": 1095256, "label": "ENT", "start_offset": 513, "end_offset": 521}, {"id": 1095257, "label": "ENT", "start_offset": 555, "end_offset": 564}, {"id": 1095258, "label": "ENT", "start_offset": 579, "end_offset": 585}, {"id": 1095259, "label": "ENT", "start_offset": 596, "end_offset": 627}, {"id": 1095260, "label": "ENT", "start_offset": 654, "end_offset": 668}, {"id": 1095261, "label": "ENT", "start_offset": 687, "end_offset": 706}, {"id": 1095262, "label": "ENT", "start_offset": 711, "end_offset": 720}], "relations": [{"id": 177964, "from_id": 1095248, "to_id": 1095249, "type": "CONJUNCTION"}, {"id": 177965, "from_id": 1095252, "to_id": 1095250, "type": "USED-FOR"}, {"id": 177966, "from_id": 1095253, "to_id": 1095252, "type": "FEATURE-OF"}, {"id": 177967, "from_id": 1095254, "to_id": 1095250, "type": "COREF"}, {"id": 177968, "from_id": 1095254, "to_id": 1095255, "type": "USED-FOR"}, {"id": 177969, "from_id": 1095258, "to_id": 1095259, "type": "USED-FOR"}, {"id": 177970, "from_id": 1095261, "to_id": 1095262, "type": "CONJUNCTION"}, {"id": 177971, "from_id": 1095250, "to_id": 1095251, "type": "USED-FOR"}, {"id": 177972, "from_id": 1095254, "to_id": 1095256, "type": "COREF"}, {"id": 177973, "from_id": 1095257, "to_id": 1095256, "type": "USED-FOR"}]}
{"id": "C04-1035", "text": " This paper presents a  machine learning approach  to  bare slice disambiguation  in  dialogue . We extract a set of  heuristic principles  from a  corpus-based sample  and formulate them as  probabilistic Horn clauses . We then use the predicates of such  clauses  to create a set of  domain independent features  to annotate an  input dataset , and run two different  machine learning algorithms  : SLIPPER, a  rule-based learning algorithm , and TiMBL, a  memory-based system . Both learners perform well, yielding similar  success rates  of approx 90%. The results show that the  features  in terms of which we formulate our  heuristic principles  have significant predictive power, and that  rules  that closely resemble our  Horn clauses  can be learnt automatically from these  features . ", "Comments": [], "entities": [{"id": 1095263, "label": "ENT", "start_offset": 24, "end_offset": 49}, {"id": 1095264, "label": "ENT", "start_offset": 55, "end_offset": 80}, {"id": 1095265, "label": "ENT", "start_offset": 86, "end_offset": 94}, {"id": 1095266, "label": "ENT", "start_offset": 118, "end_offset": 138}, {"id": 1095267, "label": "ENT", "start_offset": 148, "end_offset": 167}, {"id": 1095268, "label": "ENT", "start_offset": 192, "end_offset": 218}, {"id": 1095269, "label": "ENT", "start_offset": 257, "end_offset": 264}, {"id": 1095270, "label": "ENT", "start_offset": 286, "end_offset": 313}, {"id": 1095271, "label": "ENT", "start_offset": 370, "end_offset": 397}, {"id": 1095272, "label": "ENT", "start_offset": 401, "end_offset": 408}, {"id": 1095273, "label": "ENT", "start_offset": 413, "end_offset": 442}, {"id": 1095274, "label": "ENT", "start_offset": 449, "end_offset": 454}, {"id": 1095275, "label": "ENT", "start_offset": 459, "end_offset": 478}, {"id": 1095276, "label": "ENT", "start_offset": 527, "end_offset": 540}, {"id": 1095277, "label": "ENT", "start_offset": 584, "end_offset": 592}, {"id": 1095278, "label": "ENT", "start_offset": 630, "end_offset": 650}, {"id": 1095279, "label": "ENT", "start_offset": 697, "end_offset": 702}, {"id": 1095280, "label": "ENT", "start_offset": 731, "end_offset": 743}, {"id": 1095281, "label": "ENT", "start_offset": 785, "end_offset": 793}], "relations": [{"id": 177974, "from_id": 1095263, "to_id": 1095264, "type": "USED-FOR"}, {"id": 177975, "from_id": 1095268, "to_id": 1095266, "type": "FEATURE-OF"}, {"id": 177976, "from_id": 1095273, "to_id": 1095275, "type": "COMPARE"}, {"id": 177977, "from_id": 1095277, "to_id": 1095278, "type": "FEATURE-OF"}, {"id": 177978, "from_id": 1095265, "to_id": 1095264, "type": "USED-FOR"}, {"id": 177979, "from_id": 1095273, "to_id": 1095271, "type": "PART-OF"}, {"id": 177980, "from_id": 1095275, "to_id": 1095271, "type": "PART-OF"}, {"id": 177981, "from_id": 1095274, "to_id": 1095275, "type": "HYPONYM-OF"}, {"id": 177982, "from_id": 1095272, "to_id": 1095273, "type": "HYPONYM-OF"}, {"id": 177983, "from_id": 1095271, "to_id": 1095263, "type": "COREF"}, {"id": 177984, "from_id": 1095278, "to_id": 1095266, "type": "COREF"}, {"id": 177985, "from_id": 1095280, "to_id": 1095268, "type": "COREF"}, {"id": 177986, "from_id": 1095267, "to_id": 1095266, "type": "USED-FOR"}, {"id": 177987, "from_id": 1095269, "to_id": 1095268, "type": "COREF"}, {"id": 177988, "from_id": 1095281, "to_id": 1095277, "type": "COREF"}]}
{"id": "P80-1019", "text": "   Current  natural language interfaces  have concentrated largely on determining the literal  meaning  of  input  from their  users  . While such  decoding  is an essential underpinning, much recent work suggests that  natural language interfaces  will never appear cooperative or graceful unless they also incorporate numerous  non-literal aspects of communication  , such as robust  communication procedures  . This paper defends that view, but claims that direct imitation of human performance is not the best way to implement many of these  non-literal aspects of communication  ; that the new technology of powerful  personal computers  with integral  graphics displays  offers techniques superior to those of humans for these aspects, while still satisfying  human communication needs  . The paper proposes  interfaces  based on a judicious mixture of these techniques and the still valuable methods of more traditional  natural language interfaces . ", "Comments": [], "entities": [{"id": 1095295, "label": "ENT", "start_offset": 12, "end_offset": 39}, {"id": 1095296, "label": "ENT", "start_offset": 148, "end_offset": 156}, {"id": 1095297, "label": "ENT", "start_offset": 220, "end_offset": 247}, {"id": 1095298, "label": "ENT", "start_offset": 298, "end_offset": 302}, {"id": 1095299, "label": "ENT", "start_offset": 330, "end_offset": 366}, {"id": 1095300, "label": "ENT", "start_offset": 378, "end_offset": 410}, {"id": 1095301, "label": "ENT", "start_offset": 546, "end_offset": 582}, {"id": 1095302, "label": "ENT", "start_offset": 623, "end_offset": 641}, {"id": 1095303, "label": "ENT", "start_offset": 658, "end_offset": 675}, {"id": 1095304, "label": "ENT", "start_offset": 928, "end_offset": 955}], "relations": [{"id": 178002, "from_id": 1095303, "to_id": 1095302, "type": "PART-OF"}, {"id": 178003, "from_id": 1095297, "to_id": 1095298, "type": "COREF"}, {"id": 178004, "from_id": 1095299, "to_id": 1095298, "type": "PART-OF"}, {"id": 178005, "from_id": 1095300, "to_id": 1095299, "type": "HYPONYM-OF"}]}
{"id": "CVPR_2005_18_abs", "text": "We study and compare two novel embedding methods for segmenting feature points of piece-wise planar structures from two (uncalibrated) perspective images. We show that a set of different homographies can be embedded in different ways to a higher-dimensional real or complex space, so that each homography corresponds to either a complex bilinear form or a real quadratic form. Each embedding reveals different algebraic properties and relations of homo-graphies. We give a closed-form segmentation solution for each case by utilizing these properties based on subspace-segmentation methods. These theoretical results show that one can intrinsically segment a piece-wise planar scene from 2-D images without explicitly performing any 3-D reconstruction. The resulting segmentation may make subsequent 3-D reconstruction much better-conditioned. We demonstrate the proposed methods with some convincing experimental results.", "Comments": [], "entities": [{"id": 1095330, "label": "ENT", "start_offset": 31, "end_offset": 48}, {"id": 1095331, "label": "ENT", "start_offset": 53, "end_offset": 110}, {"id": 1095332, "label": "ENT", "start_offset": 187, "end_offset": 199}, {"id": 1095333, "label": "ENT", "start_offset": 239, "end_offset": 279}, {"id": 1095334, "label": "ENT", "start_offset": 294, "end_offset": 304}, {"id": 1095335, "label": "ENT", "start_offset": 329, "end_offset": 350}, {"id": 1095336, "label": "ENT", "start_offset": 356, "end_offset": 375}, {"id": 1095337, "label": "ENT", "start_offset": 448, "end_offset": 461}, {"id": 1095338, "label": "ENT", "start_offset": 473, "end_offset": 506}, {"id": 1095339, "label": "ENT", "start_offset": 560, "end_offset": 589}, {"id": 1095340, "label": "ENT", "start_offset": 659, "end_offset": 682}, {"id": 1095341, "label": "ENT", "start_offset": 688, "end_offset": 698}, {"id": 1095342, "label": "ENT", "start_offset": 733, "end_offset": 751}, {"id": 1095343, "label": "ENT", "start_offset": 800, "end_offset": 818}], "relations": [{"id": 178026, "from_id": 1095330, "to_id": 1095331, "type": "USED-FOR"}, {"id": 178027, "from_id": 1095339, "to_id": 1095338, "type": "USED-FOR"}, {"id": 178028, "from_id": 1095343, "to_id": 1095342, "type": "COREF"}, {"id": 178029, "from_id": 1095341, "to_id": 1095340, "type": "FEATURE-OF"}, {"id": 178030, "from_id": 1095335, "to_id": 1095334, "type": "FEATURE-OF"}, {"id": 178031, "from_id": 1095336, "to_id": 1095334, "type": "FEATURE-OF"}, {"id": 178032, "from_id": 1095335, "to_id": 1095336, "type": "CONJUNCTION"}, {"id": 178033, "from_id": 1095333, "to_id": 1095332, "type": "FEATURE-OF"}]}
{"id": "H92-1036", "text": "   We discuss  maximum a posteriori estimation  of  continuous density hidden Markov models (CDHMM)  . The classical  MLE reestimation algorithms  , namely the  forward-backward algorithm  and the  segmental k-means algorithm  , are expanded and  reestimation formulas  are given for  HMM with Gaussian mixture observation densities  . Because of its adaptive nature,  Bayesian learning  serves as a unified approach for the following four  speech recognition  applications, namely  parameter smoothing  ,  speaker adaptation  ,  speaker group modeling  and  corrective training  . New experimental results on all four applications are provided to show the effectiveness of the  MAP estimation approach  . ", "Comments": [], "entities": [{"id": 1095360, "label": "ENT", "start_offset": 15, "end_offset": 46}, {"id": 1095361, "label": "ENT", "start_offset": 52, "end_offset": 99}, {"id": 1095362, "label": "ENT", "start_offset": 118, "end_offset": 145}, {"id": 1095363, "label": "ENT", "start_offset": 161, "end_offset": 187}, {"id": 1095364, "label": "ENT", "start_offset": 198, "end_offset": 225}, {"id": 1095365, "label": "ENT", "start_offset": 247, "end_offset": 268}, {"id": 1095366, "label": "ENT", "start_offset": 285, "end_offset": 332}, {"id": 1095367, "label": "ENT", "start_offset": 369, "end_offset": 386}, {"id": 1095368, "label": "ENT", "start_offset": 441, "end_offset": 473}, {"id": 1095369, "label": "ENT", "start_offset": 483, "end_offset": 502}, {"id": 1095370, "label": "ENT", "start_offset": 507, "end_offset": 525}, {"id": 1095371, "label": "ENT", "start_offset": 530, "end_offset": 552}, {"id": 1095372, "label": "ENT", "start_offset": 559, "end_offset": 578}, {"id": 1095373, "label": "ENT", "start_offset": 619, "end_offset": 631}, {"id": 1095374, "label": "ENT", "start_offset": 679, "end_offset": 702}], "relations": [{"id": 178051, "from_id": 1095365, "to_id": 1095366, "type": "USED-FOR"}, {"id": 178052, "from_id": 1095363, "to_id": 1095362, "type": "HYPONYM-OF"}, {"id": 178053, "from_id": 1095364, "to_id": 1095362, "type": "HYPONYM-OF"}, {"id": 178054, "from_id": 1095364, "to_id": 1095363, "type": "CONJUNCTION"}, {"id": 178055, "from_id": 1095360, "to_id": 1095361, "type": "USED-FOR"}, {"id": 178056, "from_id": 1095367, "to_id": 1095368, "type": "USED-FOR"}, {"id": 178057, "from_id": 1095369, "to_id": 1095368, "type": "HYPONYM-OF"}, {"id": 178058, "from_id": 1095370, "to_id": 1095368, "type": "HYPONYM-OF"}, {"id": 178059, "from_id": 1095371, "to_id": 1095368, "type": "HYPONYM-OF"}, {"id": 178060, "from_id": 1095372, "to_id": 1095368, "type": "HYPONYM-OF"}, {"id": 178061, "from_id": 1095369, "to_id": 1095370, "type": "CONJUNCTION"}, {"id": 178062, "from_id": 1095370, "to_id": 1095371, "type": "CONJUNCTION"}, {"id": 178063, "from_id": 1095371, "to_id": 1095372, "type": "CONJUNCTION"}, {"id": 178064, "from_id": 1095373, "to_id": 1095368, "type": "COREF"}, {"id": 178065, "from_id": 1095374, "to_id": 1095360, "type": "COREF"}, {"id": 178066, "from_id": 1095373, "to_id": 1095374, "type": "EVALUATE-FOR"}]}
{"id": "N06-2009", "text": "   State-of-the-art  Question Answering (QA) systems  are very sensitive to variations in the phrasing of an  information need  . Finding the preferred  language  for such a  need  is a valuable task. We investigate that claim by adopting a simple  MT-based paraphrasing technique  and evaluating  QA system  performance on  paraphrased questions  . We found a potential increase of 35% in  MRR  with respect to the original  question  . ", "Comments": [], "entities": [{"id": 1095410, "label": "ENT", "start_offset": 21, "end_offset": 52}, {"id": 1095411, "label": "ENT", "start_offset": 249, "end_offset": 280}, {"id": 1095412, "label": "ENT", "start_offset": 298, "end_offset": 307}, {"id": 1095413, "label": "ENT", "start_offset": 325, "end_offset": 346}, {"id": 1095414, "label": "ENT", "start_offset": 391, "end_offset": 394}], "relations": [{"id": 178093, "from_id": 1095411, "to_id": 1095412, "type": "USED-FOR"}, {"id": 178094, "from_id": 1095412, "to_id": 1095410, "type": "COREF"}, {"id": 178095, "from_id": 1095413, "to_id": 1095412, "type": "EVALUATE-FOR"}]}
{"id": "A97-1042", "text": " This paper addresses the problem of identifying likely  topics  of  texts  by their position in the  text . It describes the automated  training  and evaluation of an  Optimal Position Policy , a method of locating the likely positions of  topic-bearing sentences  based on  genre-specific regularities  of  discourse structure . This method can be used in applications such as  information retrieval ,  routing , and  text summarization . ", "Comments": [], "entities": [{"id": 1095415, "label": "ENT", "start_offset": 169, "end_offset": 192}, {"id": 1095416, "label": "ENT", "start_offset": 197, "end_offset": 203}, {"id": 1095417, "label": "ENT", "start_offset": 227, "end_offset": 264}, {"id": 1095418, "label": "ENT", "start_offset": 276, "end_offset": 328}, {"id": 1095419, "label": "ENT", "start_offset": 336, "end_offset": 342}, {"id": 1095420, "label": "ENT", "start_offset": 358, "end_offset": 370}, {"id": 1095421, "label": "ENT", "start_offset": 380, "end_offset": 401}, {"id": 1095422, "label": "ENT", "start_offset": 405, "end_offset": 412}, {"id": 1095423, "label": "ENT", "start_offset": 420, "end_offset": 438}], "relations": [{"id": 178096, "from_id": 1095416, "to_id": 1095415, "type": "COREF"}, {"id": 178097, "from_id": 1095419, "to_id": 1095416, "type": "COREF"}, {"id": 178098, "from_id": 1095421, "to_id": 1095422, "type": "CONJUNCTION"}, {"id": 178099, "from_id": 1095422, "to_id": 1095423, "type": "CONJUNCTION"}, {"id": 178100, "from_id": 1095421, "to_id": 1095420, "type": "HYPONYM-OF"}, {"id": 178101, "from_id": 1095422, "to_id": 1095420, "type": "HYPONYM-OF"}, {"id": 178102, "from_id": 1095423, "to_id": 1095420, "type": "HYPONYM-OF"}, {"id": 178103, "from_id": 1095419, "to_id": 1095420, "type": "USED-FOR"}, {"id": 178104, "from_id": 1095416, "to_id": 1095417, "type": "USED-FOR"}, {"id": 178105, "from_id": 1095418, "to_id": 1095416, "type": "USED-FOR"}]}
{"id": "I05-5008", "text": " We propose a method that automatically generates  paraphrase  sets from  seed sentences  to be used as  reference sets  in objective  machine translation evaluation measures  like  BLEU  and  NIST  . We measured the quality of the  paraphrases  produced in an experiment, i.e., (i) their  grammaticality  : at least 99% correct  sentences  ; (ii) their  equivalence in meaning  : at least 96% correct  paraphrases  either by  meaning equivalence  or  entailment  ; and, (iii) the amount of internal  lexical and syntactical variation  in a set of  paraphrases  : slightly superior to that of  hand-produced sets  . The  paraphrase  sets produced by this method thus seem adequate as  reference sets  to be used for  MT evaluation  . ", "Comments": [], "entities": [{"id": 1095438, "label": "ENT", "start_offset": 14, "end_offset": 20}, {"id": 1095439, "label": "ENT", "start_offset": 51, "end_offset": 61}, {"id": 1095440, "label": "ENT", "start_offset": 135, "end_offset": 174}, {"id": 1095441, "label": "ENT", "start_offset": 182, "end_offset": 186}, {"id": 1095442, "label": "ENT", "start_offset": 193, "end_offset": 197}, {"id": 1095443, "label": "ENT", "start_offset": 233, "end_offset": 244}, {"id": 1095444, "label": "ENT", "start_offset": 290, "end_offset": 304}, {"id": 1095445, "label": "ENT", "start_offset": 355, "end_offset": 377}, {"id": 1095446, "label": "ENT", "start_offset": 403, "end_offset": 414}, {"id": 1095447, "label": "ENT", "start_offset": 427, "end_offset": 446}, {"id": 1095448, "label": "ENT", "start_offset": 452, "end_offset": 462}, {"id": 1095449, "label": "ENT", "start_offset": 491, "end_offset": 534}, {"id": 1095450, "label": "ENT", "start_offset": 549, "end_offset": 560}, {"id": 1095451, "label": "ENT", "start_offset": 594, "end_offset": 612}, {"id": 1095452, "label": "ENT", "start_offset": 621, "end_offset": 631}, {"id": 1095453, "label": "ENT", "start_offset": 655, "end_offset": 661}, {"id": 1095454, "label": "ENT", "start_offset": 717, "end_offset": 730}], "relations": [{"id": 178116, "from_id": 1095438, "to_id": 1095439, "type": "USED-FOR"}, {"id": 178117, "from_id": 1095441, "to_id": 1095440, "type": "HYPONYM-OF"}, {"id": 178118, "from_id": 1095442, "to_id": 1095440, "type": "HYPONYM-OF"}, {"id": 178119, "from_id": 1095441, "to_id": 1095442, "type": "CONJUNCTION"}, {"id": 178120, "from_id": 1095439, "to_id": 1095440, "type": "USED-FOR"}, {"id": 178121, "from_id": 1095445, "to_id": 1095444, "type": "CONJUNCTION"}, {"id": 178122, "from_id": 1095447, "to_id": 1095446, "type": "USED-FOR"}, {"id": 178123, "from_id": 1095448, "to_id": 1095446, "type": "USED-FOR"}, {"id": 178124, "from_id": 1095447, "to_id": 1095448, "type": "CONJUNCTION"}, {"id": 178125, "from_id": 1095450, "to_id": 1095451, "type": "COMPARE"}, {"id": 178126, "from_id": 1095453, "to_id": 1095438, "type": "COREF"}, {"id": 178127, "from_id": 1095453, "to_id": 1095452, "type": "USED-FOR"}, {"id": 178128, "from_id": 1095449, "to_id": 1095445, "type": "CONJUNCTION"}]}
{"id": "E93-1023", "text": " One of the major problems one is faced with when decomposing  words  into their  constituent parts  is  ambiguity : the  generation  of multiple  analyses  for one  input word , many of which are implausible. In order to deal with  ambiguity , the  MORphological PArser MORPA  is provided with a  probabilistic context-free grammar (PCFG) , i.e. it combines a  \"conventional\" context-free morphological grammar  to filter out  ungrammatical segmentations  with a  probability-based scoring function  which determines the likelihood of each successful  parse . Consequently, remaining  analyses  can be ordered along a scale of plausibility. Test performance data will show that a  PCFG  yields good results in  morphological parsing .  MORPA  is a fully implemented  parser  developed for use in a  text-to-speech conversion system . ", "Comments": [], "entities": [{"id": 1095455, "label": "ENT", "start_offset": 105, "end_offset": 114}, {"id": 1095456, "label": "ENT", "start_offset": 122, "end_offset": 132}, {"id": 1095457, "label": "ENT", "start_offset": 233, "end_offset": 242}, {"id": 1095458, "label": "ENT", "start_offset": 250, "end_offset": 276}, {"id": 1095459, "label": "ENT", "start_offset": 298, "end_offset": 339}, {"id": 1095460, "label": "ENT", "start_offset": 347, "end_offset": 349}, {"id": 1095461, "label": "ENT", "start_offset": 362, "end_offset": 411}, {"id": 1095462, "label": "ENT", "start_offset": 428, "end_offset": 455}, {"id": 1095463, "label": "ENT", "start_offset": 465, "end_offset": 499}, {"id": 1095464, "label": "ENT", "start_offset": 553, "end_offset": 558}, {"id": 1095465, "label": "ENT", "start_offset": 682, "end_offset": 686}, {"id": 1095466, "label": "ENT", "start_offset": 712, "end_offset": 733}, {"id": 1095467, "label": "ENT", "start_offset": 737, "end_offset": 742}, {"id": 1095468, "label": "ENT", "start_offset": 768, "end_offset": 774}, {"id": 1095469, "label": "ENT", "start_offset": 800, "end_offset": 832}], "relations": [{"id": 178129, "from_id": 1095459, "to_id": 1095458, "type": "USED-FOR"}, {"id": 178130, "from_id": 1095463, "to_id": 1095461, "type": "CONJUNCTION"}, {"id": 178131, "from_id": 1095468, "to_id": 1095469, "type": "USED-FOR"}, {"id": 178132, "from_id": 1095460, "to_id": 1095458, "type": "COREF"}, {"id": 178133, "from_id": 1095458, "to_id": 1095457, "type": "USED-FOR"}, {"id": 178134, "from_id": 1095461, "to_id": 1095462, "type": "USED-FOR"}, {"id": 178135, "from_id": 1095463, "to_id": 1095464, "type": "USED-FOR"}, {"id": 178136, "from_id": 1095465, "to_id": 1095459, "type": "COREF"}, {"id": 178137, "from_id": 1095465, "to_id": 1095466, "type": "USED-FOR"}, {"id": 178138, "from_id": 1095467, "to_id": 1095458, "type": "COREF"}, {"id": 178139, "from_id": 1095467, "to_id": 1095468, "type": "HYPONYM-OF"}, {"id": 178140, "from_id": 1095467, "to_id": 1095469, "type": "USED-FOR"}, {"id": 178141, "from_id": 1095461, "to_id": 1095460, "type": "USED-FOR"}, {"id": 178142, "from_id": 1095463, "to_id": 1095460, "type": "USED-FOR"}]}
{"id": "N03-2015", "text": " We describe a simple  unsupervised technique  for learning  morphology  by identifying  hubs  in an  automaton  . For our purposes, a  hub  is a  node  in a  graph  with  in-degree  greater than one and  out-degree  greater than one. We create a  word-trie  , transform it into a  minimal DFA  , then identify  hubs  . Those  hubs  mark the boundary between  root  and  suffix  , achieving similar  performance  to more complex mixtures of techniques. ", "Comments": [], "entities": [{"id": 1095489, "label": "ENT", "start_offset": 23, "end_offset": 45}, {"id": 1095490, "label": "ENT", "start_offset": 61, "end_offset": 71}, {"id": 1095491, "label": "ENT", "start_offset": 89, "end_offset": 93}, {"id": 1095492, "label": "ENT", "start_offset": 102, "end_offset": 111}, {"id": 1095493, "label": "ENT", "start_offset": 136, "end_offset": 139}, {"id": 1095494, "label": "ENT", "start_offset": 147, "end_offset": 151}, {"id": 1095495, "label": "ENT", "start_offset": 159, "end_offset": 164}, {"id": 1095496, "label": "ENT", "start_offset": 248, "end_offset": 257}, {"id": 1095497, "label": "ENT", "start_offset": 271, "end_offset": 273}, {"id": 1095498, "label": "ENT", "start_offset": 282, "end_offset": 293}, {"id": 1095499, "label": "ENT", "start_offset": 312, "end_offset": 316}, {"id": 1095500, "label": "ENT", "start_offset": 327, "end_offset": 331}, {"id": 1095501, "label": "ENT", "start_offset": 360, "end_offset": 364}, {"id": 1095502, "label": "ENT", "start_offset": 371, "end_offset": 377}], "relations": [{"id": 178162, "from_id": 1095489, "to_id": 1095490, "type": "USED-FOR"}, {"id": 178163, "from_id": 1095491, "to_id": 1095489, "type": "USED-FOR"}, {"id": 178164, "from_id": 1095493, "to_id": 1095491, "type": "COREF"}, {"id": 178165, "from_id": 1095493, "to_id": 1095494, "type": "HYPONYM-OF"}, {"id": 178166, "from_id": 1095494, "to_id": 1095495, "type": "PART-OF"}, {"id": 178167, "from_id": 1095496, "to_id": 1095499, "type": "USED-FOR"}, {"id": 178168, "from_id": 1095498, "to_id": 1095499, "type": "USED-FOR"}, {"id": 178169, "from_id": 1095499, "to_id": 1095493, "type": "COREF"}, {"id": 178170, "from_id": 1095500, "to_id": 1095499, "type": "COREF"}, {"id": 178171, "from_id": 1095491, "to_id": 1095492, "type": "PART-OF"}, {"id": 178172, "from_id": 1095497, "to_id": 1095496, "type": "COREF"}]}
{"id": "INTERSPEECH_2015_16_abs", "text": "Recently, Stacked Auto-Encoders (SAE) have been successfully used for learning imbalanced datasets. In this paper, for the first time, we propose to use a Neural Network classifier furnished by an SAE structure for detecting the errors made by a strong Automatic Speech Recognition (ASR) system. Error detection on an automatic transcription provided by a \" strong \" ASR system, i.e. exhibiting a small word error rate, is difficult due to the limited number of \" positive \" examples (i.e. words erroneously recognized) available for training a binary classi-fier. In this paper we investigate and compare different types of classifiers for automatically detecting ASR errors, including the one based on a stacked auto-encoder architecture. We show the effectiveness of the latter by measuring and comparing performance on the automatic transcriptions of an English corpus collected from TED talks. Performance of each investigated classifier is evaluated both via receiving operating curve and via a measure, called mean absolute error, related to the quality in predicting the corresponding word error rate. The results demonstrates that the classifier based on SAE detects the ASR errors better than the other classification methods.", "Comments": [], "entities": [{"id": 1095503, "label": "ENT", "start_offset": 10, "end_offset": 37}, {"id": 1095504, "label": "ENT", "start_offset": 70, "end_offset": 98}, {"id": 1095505, "label": "ENT", "start_offset": 155, "end_offset": 180}, {"id": 1095506, "label": "ENT", "start_offset": 197, "end_offset": 210}, {"id": 1095507, "label": "ENT", "start_offset": 253, "end_offset": 294}, {"id": 1095508, "label": "ENT", "start_offset": 296, "end_offset": 311}, {"id": 1095509, "label": "ENT", "start_offset": 318, "end_offset": 341}, {"id": 1095510, "label": "ENT", "start_offset": 367, "end_offset": 377}, {"id": 1095511, "label": "ENT", "start_offset": 403, "end_offset": 418}, {"id": 1095512, "label": "ENT", "start_offset": 545, "end_offset": 563}, {"id": 1095513, "label": "ENT", "start_offset": 625, "end_offset": 636}, {"id": 1095514, "label": "ENT", "start_offset": 641, "end_offset": 675}, {"id": 1095515, "label": "ENT", "start_offset": 691, "end_offset": 694}, {"id": 1095516, "label": "ENT", "start_offset": 706, "end_offset": 739}, {"id": 1095517, "label": "ENT", "start_offset": 827, "end_offset": 851}, {"id": 1095518, "label": "ENT", "start_offset": 858, "end_offset": 872}, {"id": 1095519, "label": "ENT", "start_offset": 888, "end_offset": 897}, {"id": 1095520, "label": "ENT", "start_offset": 932, "end_offset": 942}, {"id": 1095521, "label": "ENT", "start_offset": 965, "end_offset": 990}, {"id": 1095522, "label": "ENT", "start_offset": 1001, "end_offset": 1008}, {"id": 1095523, "label": "ENT", "start_offset": 1017, "end_offset": 1036}, {"id": 1095524, "label": "ENT", "start_offset": 1093, "end_offset": 1108}, {"id": 1095525, "label": "ENT", "start_offset": 1144, "end_offset": 1154}, {"id": 1095526, "label": "ENT", "start_offset": 1164, "end_offset": 1167}, {"id": 1095527, "label": "ENT", "start_offset": 1180, "end_offset": 1190}, {"id": 1095528, "label": "ENT", "start_offset": 1213, "end_offset": 1235}], "relations": [{"id": 178173, "from_id": 1095503, "to_id": 1095504, "type": "USED-FOR"}, {"id": 178174, "from_id": 1095506, "to_id": 1095503, "type": "COREF"}, {"id": 178175, "from_id": 1095506, "to_id": 1095505, "type": "USED-FOR"}, {"id": 178176, "from_id": 1095505, "to_id": 1095507, "type": "USED-FOR"}, {"id": 178177, "from_id": 1095508, "to_id": 1095509, "type": "USED-FOR"}, {"id": 178178, "from_id": 1095510, "to_id": 1095507, "type": "COREF"}, {"id": 178179, "from_id": 1095513, "to_id": 1095514, "type": "USED-FOR"}, {"id": 178180, "from_id": 1095515, "to_id": 1095513, "type": "HYPONYM-OF"}, {"id": 178181, "from_id": 1095516, "to_id": 1095515, "type": "USED-FOR"}, {"id": 178182, "from_id": 1095516, "to_id": 1095503, "type": "COREF"}, {"id": 178183, "from_id": 1095519, "to_id": 1095518, "type": "USED-FOR"}, {"id": 178184, "from_id": 1095518, "to_id": 1095517, "type": "FEATURE-OF"}, {"id": 178185, "from_id": 1095523, "to_id": 1095522, "type": "COREF"}, {"id": 178186, "from_id": 1095521, "to_id": 1095520, "type": "EVALUATE-FOR"}, {"id": 178187, "from_id": 1095522, "to_id": 1095520, "type": "EVALUATE-FOR"}, {"id": 178188, "from_id": 1095521, "to_id": 1095522, "type": "CONJUNCTION"}, {"id": 178189, "from_id": 1095526, "to_id": 1095516, "type": "COREF"}, {"id": 178190, "from_id": 1095526, "to_id": 1095525, "type": "USED-FOR"}, {"id": 178191, "from_id": 1095525, "to_id": 1095527, "type": "USED-FOR"}, {"id": 178192, "from_id": 1095525, "to_id": 1095528, "type": "COMPARE"}, {"id": 178193, "from_id": 1095528, "to_id": 1095527, "type": "USED-FOR"}]}
{"id": "P05-3025", "text": "   This paper describes a method of  interactively visualizing and directing the process  of  translating a sentence  . The method allows a  user  to explore a  model  of  syntax-based statistical machine translation (MT)  , to understand the  model  's strengths and weaknesses, and to compare it to other  MT systems  . Using this  visualization method  , we can find and address conceptual and practical problems in an  MT system  . In our demonstration at  ACL  , new  users  of our tool will drive a  syntax-based decoder  for themselves. ", "Comments": [], "entities": [{"id": 1095597, "label": "ENT", "start_offset": 26, "end_offset": 32}, {"id": 1095598, "label": "ENT", "start_offset": 37, "end_offset": 105}, {"id": 1095599, "label": "ENT", "start_offset": 124, "end_offset": 130}, {"id": 1095600, "label": "ENT", "start_offset": 161, "end_offset": 166}, {"id": 1095601, "label": "ENT", "start_offset": 172, "end_offset": 221}, {"id": 1095602, "label": "ENT", "start_offset": 244, "end_offset": 249}, {"id": 1095603, "label": "ENT", "start_offset": 295, "end_offset": 297}, {"id": 1095604, "label": "ENT", "start_offset": 308, "end_offset": 318}, {"id": 1095605, "label": "ENT", "start_offset": 334, "end_offset": 354}, {"id": 1095606, "label": "ENT", "start_offset": 423, "end_offset": 432}, {"id": 1095607, "label": "ENT", "start_offset": 506, "end_offset": 526}], "relations": [{"id": 178250, "from_id": 1095605, "to_id": 1095606, "type": "USED-FOR"}, {"id": 178251, "from_id": 1095597, "to_id": 1095598, "type": "USED-FOR"}, {"id": 178252, "from_id": 1095599, "to_id": 1095597, "type": "COREF"}, {"id": 178253, "from_id": 1095600, "to_id": 1095601, "type": "USED-FOR"}, {"id": 178254, "from_id": 1095599, "to_id": 1095600, "type": "USED-FOR"}, {"id": 178255, "from_id": 1095602, "to_id": 1095603, "type": "COREF"}, {"id": 178256, "from_id": 1095603, "to_id": 1095604, "type": "COMPARE"}, {"id": 178257, "from_id": 1095600, "to_id": 1095602, "type": "COREF"}]}
{"id": "C08-1118", "text": " This paper shows that it is very often possible to identify the  source language  of medium-length speeches in the  EUROPARL corpus  on the basis of  frequency counts  of  word n-grams  (87.2%-96.7%  accuracy  depending on  classification method ). The paper also examines in detail which  positive markers  are most powerful and identifies a number of linguistic aspects as well as culture- and domain-related ones. ", "Comments": [], "entities": [{"id": 1095639, "label": "ENT", "start_offset": 86, "end_offset": 108}, {"id": 1095640, "label": "ENT", "start_offset": 117, "end_offset": 132}, {"id": 1095641, "label": "ENT", "start_offset": 151, "end_offset": 185}, {"id": 1095642, "label": "ENT", "start_offset": 201, "end_offset": 209}, {"id": 1095643, "label": "ENT", "start_offset": 225, "end_offset": 246}, {"id": 1095644, "label": "ENT", "start_offset": 291, "end_offset": 307}], "relations": [{"id": 178281, "from_id": 1095639, "to_id": 1095640, "type": "PART-OF"}, {"id": 178282, "from_id": 1095642, "to_id": 1095643, "type": "EVALUATE-FOR"}]}
{"id": "P80-1026", "text": "  When people use  natural language  in natural settings, they often use it ungrammatically, missing out or repeating words, breaking-off and restarting, speaking in fragments, etc.. Their  human listeners  are usually able to cope with these deviations with little difficulty. If a  computer system  wishes to accept  natural language input  from its  users  on a routine basis, it must display a similar indifference. In this paper, we outline a set of  parsing flexibilities  that such a system should provide. We go, on to describe  FlexP  , a  bottom-up pattern-matching parser  that we have designed and implemented to provide these flexibilities for  restricted natural language  input to a limited-domain computer system. ", "Comments": [], "entities": [{"id": 1095645, "label": "ENT", "start_offset": 19, "end_offset": 35}, {"id": 1095646, "label": "ENT", "start_offset": 73, "end_offset": 75}, {"id": 1095647, "label": "ENT", "start_offset": 284, "end_offset": 299}, {"id": 1095648, "label": "ENT", "start_offset": 319, "end_offset": 341}, {"id": 1095649, "label": "ENT", "start_offset": 456, "end_offset": 477}, {"id": 1095650, "label": "ENT", "start_offset": 491, "end_offset": 497}, {"id": 1095651, "label": "ENT", "start_offset": 537, "end_offset": 542}, {"id": 1095652, "label": "ENT", "start_offset": 549, "end_offset": 582}, {"id": 1095653, "label": "ENT", "start_offset": 639, "end_offset": 652}, {"id": 1095654, "label": "ENT", "start_offset": 658, "end_offset": 685}, {"id": 1095655, "label": "ENT", "start_offset": 698, "end_offset": 728}], "relations": [{"id": 178283, "from_id": 1095645, "to_id": 1095646, "type": "COREF"}, {"id": 178284, "from_id": 1095647, "to_id": 1095650, "type": "COREF"}, {"id": 178285, "from_id": 1095651, "to_id": 1095652, "type": "HYPONYM-OF"}, {"id": 178286, "from_id": 1095649, "to_id": 1095653, "type": "COREF"}, {"id": 178287, "from_id": 1095653, "to_id": 1095654, "type": "FEATURE-OF"}, {"id": 178288, "from_id": 1095652, "to_id": 1095653, "type": "USED-FOR"}, {"id": 178289, "from_id": 1095654, "to_id": 1095652, "type": "USED-FOR"}, {"id": 178290, "from_id": 1095653, "to_id": 1095655, "type": "PART-OF"}]}
{"id": "C04-1022", "text": "  Statistical language modeling  remains a challenging task, in particular for  morphologically rich languages . Recently, new approaches based on  factored language models  have been developed to address this problem. These  models  provide principled ways of including additional  conditioning variables  other than the  preceding words , such as  morphological or syntactic features . However, the number of possible choices for  model parameters  creates a  large space of models  that cannot be searched exhaustively. This paper presents an  entirely data-driven model selection procedure  based on  genetic search , which is shown to outperform both  knowledge-based and random selection procedures  on two different  language modeling tasks  ( Arabic  and  Turkish ). ", "Comments": [], "entities": [{"id": 1095693, "label": "ENT", "start_offset": 2, "end_offset": 31}, {"id": 1095694, "label": "ENT", "start_offset": 55, "end_offset": 59}, {"id": 1095695, "label": "ENT", "start_offset": 80, "end_offset": 110}, {"id": 1095696, "label": "ENT", "start_offset": 127, "end_offset": 137}, {"id": 1095697, "label": "ENT", "start_offset": 148, "end_offset": 172}, {"id": 1095698, "label": "ENT", "start_offset": 226, "end_offset": 232}, {"id": 1095699, "label": "ENT", "start_offset": 283, "end_offset": 305}, {"id": 1095700, "label": "ENT", "start_offset": 350, "end_offset": 385}, {"id": 1095701, "label": "ENT", "start_offset": 433, "end_offset": 449}, {"id": 1095702, "label": "ENT", "start_offset": 547, "end_offset": 593}, {"id": 1095703, "label": "ENT", "start_offset": 605, "end_offset": 619}, {"id": 1095704, "label": "ENT", "start_offset": 657, "end_offset": 704}, {"id": 1095705, "label": "ENT", "start_offset": 724, "end_offset": 747}, {"id": 1095706, "label": "ENT", "start_offset": 751, "end_offset": 757}, {"id": 1095707, "label": "ENT", "start_offset": 764, "end_offset": 771}], "relations": [{"id": 178327, "from_id": 1095703, "to_id": 1095702, "type": "USED-FOR"}, {"id": 178328, "from_id": 1095704, "to_id": 1095705, "type": "USED-FOR"}, {"id": 178329, "from_id": 1095698, "to_id": 1095696, "type": "COREF"}, {"id": 178330, "from_id": 1095697, "to_id": 1095696, "type": "USED-FOR"}, {"id": 178331, "from_id": 1095702, "to_id": 1095704, "type": "COMPARE"}, {"id": 178332, "from_id": 1095700, "to_id": 1095699, "type": "HYPONYM-OF"}, {"id": 178333, "from_id": 1095706, "to_id": 1095705, "type": "HYPONYM-OF"}, {"id": 178334, "from_id": 1095707, "to_id": 1095705, "type": "HYPONYM-OF"}, {"id": 178335, "from_id": 1095706, "to_id": 1095707, "type": "CONJUNCTION"}, {"id": 178336, "from_id": 1095694, "to_id": 1095693, "type": "COREF"}, {"id": 178337, "from_id": 1095694, "to_id": 1095695, "type": "USED-FOR"}]}
{"id": "J87-1003", "text": "  English  is shown to be trans-context-free on the basis of  coordinations  of the respectively type that involve  strictly syntactic cross-serial agreement . The  agreement  in question involves  number  in  nouns  and  reflexive pronouns  and is syntactic rather than semantic in nature because  grammatical number  in  English , like  grammatical gender  in  languages  such as  French , is partly arbitrary. The formal proof, which makes crucial use of the  Interchange Lemma  of Ogden et al., is so constructed as to be valid even if  English  is presumed to contain  grammatical sentences  in which respectively operates across a pair of  coordinate phrases  one of whose members has fewer  conjuncts  than the other; it thus goes through whatever the facts may be regarding  constructions  with unequal numbers of  conjuncts  in the  scope  of respectively, whereas other  arguments  have foundered on this problem. ", "Comments": [], "entities": [{"id": 1095708, "label": "ENT", "start_offset": 2, "end_offset": 9}, {"id": 1095709, "label": "ENT", "start_offset": 62, "end_offset": 75}, {"id": 1095710, "label": "ENT", "start_offset": 116, "end_offset": 157}, {"id": 1095711, "label": "ENT", "start_offset": 165, "end_offset": 174}, {"id": 1095712, "label": "ENT", "start_offset": 210, "end_offset": 215}, {"id": 1095713, "label": "ENT", "start_offset": 222, "end_offset": 240}, {"id": 1095714, "label": "ENT", "start_offset": 299, "end_offset": 317}, {"id": 1095715, "label": "ENT", "start_offset": 323, "end_offset": 330}, {"id": 1095716, "label": "ENT", "start_offset": 339, "end_offset": 357}, {"id": 1095717, "label": "ENT", "start_offset": 363, "end_offset": 372}, {"id": 1095718, "label": "ENT", "start_offset": 383, "end_offset": 389}, {"id": 1095719, "label": "ENT", "start_offset": 463, "end_offset": 480}, {"id": 1095720, "label": "ENT", "start_offset": 541, "end_offset": 548}], "relations": [{"id": 178338, "from_id": 1095711, "to_id": 1095710, "type": "COREF"}, {"id": 178339, "from_id": 1095718, "to_id": 1095717, "type": "HYPONYM-OF"}, {"id": 178340, "from_id": 1095712, "to_id": 1095713, "type": "CONJUNCTION"}, {"id": 178341, "from_id": 1095716, "to_id": 1095717, "type": "FEATURE-OF"}]}
{"id": "E99-1034", "text": " This paper explores the issue of using different  co-occurrence similarities  between  terms  for separating  query terms  that are useful for  retrieval  from those that are harmful. The hypothesis under examination is that  useful terms  tend to be more similar to each other than to other  query terms . Preliminary experiments with similarities computed using  first-order and second-order co-occurrence  seem to confirm the hypothesis.  Term similarities  could then be used for determining which  query terms  are useful and best reflect the user's information need. A possible application would be to use this source of evidence for tuning the  weights  of the  query terms . ", "Comments": [], "entities": [{"id": 1095763, "label": "ENT", "start_offset": 51, "end_offset": 77}, {"id": 1095764, "label": "ENT", "start_offset": 111, "end_offset": 122}, {"id": 1095765, "label": "ENT", "start_offset": 145, "end_offset": 154}, {"id": 1095766, "label": "ENT", "start_offset": 161, "end_offset": 166}, {"id": 1095767, "label": "ENT", "start_offset": 227, "end_offset": 239}, {"id": 1095768, "label": "ENT", "start_offset": 294, "end_offset": 305}, {"id": 1095769, "label": "ENT", "start_offset": 337, "end_offset": 349}, {"id": 1095770, "label": "ENT", "start_offset": 366, "end_offset": 408}, {"id": 1095771, "label": "ENT", "start_offset": 443, "end_offset": 460}], "relations": [{"id": 178376, "from_id": 1095764, "to_id": 1095765, "type": "USED-FOR"}, {"id": 178377, "from_id": 1095767, "to_id": 1095768, "type": "COMPARE"}, {"id": 178378, "from_id": 1095763, "to_id": 1095764, "type": "USED-FOR"}, {"id": 178379, "from_id": 1095770, "to_id": 1095769, "type": "USED-FOR"}, {"id": 178380, "from_id": 1095771, "to_id": 1095769, "type": "COREF"}, {"id": 178381, "from_id": 1095766, "to_id": 1095764, "type": "COMPARE"}]}
{"id": "H91-1067", "text": " This paper describes an implemented program that takes a  tagged text corpus  and generates a partial list of the  subcategorization frames  in which each  verb  occurs. The completeness of the output list increases monotonically with the total  occurrences  of each  verb  in the  training corpus .  False positive rates  are one to three percent. Five  subcategorization frames  are currently detected and we foresee no impediment to detecting many more. Ultimately, we expect to provide a large  subcategorization dictionary  to the  NLP community  and to train  dictionaries  for specific  corpora . ", "Comments": [], "entities": [{"id": 1095795, "label": "ENT", "start_offset": 37, "end_offset": 44}, {"id": 1095796, "label": "ENT", "start_offset": 59, "end_offset": 77}, {"id": 1095797, "label": "ENT", "start_offset": 116, "end_offset": 140}, {"id": 1095798, "label": "ENT", "start_offset": 302, "end_offset": 322}, {"id": 1095799, "label": "ENT", "start_offset": 356, "end_offset": 380}, {"id": 1095800, "label": "ENT", "start_offset": 500, "end_offset": 528}], "relations": [{"id": 178402, "from_id": 1095796, "to_id": 1095795, "type": "EVALUATE-FOR"}]}
{"id": "J86-1002", "text": "   A method for  error correction  of  ill-formed input  is described that acquires  dialogue patterns  in typical usage and uses these  patterns  to predict new inputs.  Error correction  is done by strongly biasing  parsing  toward expected  meanings  unless clear evidence from the input shows the current  sentence  is not expected. A  dialogue acquisition and tracking algorithm  is presented along with a description of its  implementation  in a  voice interactive system  . A series of tests are described that show the power of the  error correction methodology  when  stereotypic dialogue  occurs. ", "Comments": [], "entities": [{"id": 1095801, "label": "ENT", "start_offset": 5, "end_offset": 11}, {"id": 1095802, "label": "ENT", "start_offset": 17, "end_offset": 33}, {"id": 1095803, "label": "ENT", "start_offset": 39, "end_offset": 55}, {"id": 1095804, "label": "ENT", "start_offset": 85, "end_offset": 102}, {"id": 1095805, "label": "ENT", "start_offset": 137, "end_offset": 145}, {"id": 1095806, "label": "ENT", "start_offset": 171, "end_offset": 187}, {"id": 1095807, "label": "ENT", "start_offset": 218, "end_offset": 225}, {"id": 1095808, "label": "ENT", "start_offset": 340, "end_offset": 383}, {"id": 1095809, "label": "ENT", "start_offset": 453, "end_offset": 477}, {"id": 1095810, "label": "ENT", "start_offset": 541, "end_offset": 569}, {"id": 1095811, "label": "ENT", "start_offset": 577, "end_offset": 597}], "relations": [{"id": 178403, "from_id": 1095803, "to_id": 1095802, "type": "USED-FOR"}, {"id": 178404, "from_id": 1095808, "to_id": 1095809, "type": "USED-FOR"}, {"id": 178405, "from_id": 1095801, "to_id": 1095802, "type": "USED-FOR"}, {"id": 178406, "from_id": 1095805, "to_id": 1095804, "type": "COREF"}, {"id": 178407, "from_id": 1095806, "to_id": 1095802, "type": "COREF"}, {"id": 178408, "from_id": 1095811, "to_id": 1095810, "type": "USED-FOR"}, {"id": 178409, "from_id": 1095810, "to_id": 1095801, "type": "COREF"}]}
{"id": "NIPS_2002_10_abs", "text": "Identity uncertainty is a pervasive problem in real-world data analysis. It arises whenever objects are not labeled with unique identifiers or when those identifiers may not be perceived perfectly. In such cases, two observations may or may not correspond to the same object. In this paper, we consider the problem in the context of citation matching\u2014the problem of deciding which citations correspond to the same publication. Our approach is based on the use of a relational probability model to define a generative model for the domain, including models of author and title corruption and a probabilistic citation grammar. Identity uncertainty is handled by extending standard models to incorporate probabilities over the possible mappings between terms in the language and objects in the domain. Inference is based on Markov chain Monte Carlo, augmented with specific methods for generating efficient proposals when the domain contains many objects. Results on several citation data sets show that the method outperforms current algorithms for citation matching. The declarative, relational nature of the model also means that our algorithm can determine object characteristics such as author names by combining multiple citations of multiple papers.", "Comments": [], "entities": [{"id": 1095812, "label": "ENT", "start_offset": 0, "end_offset": 20}, {"id": 1095813, "label": "ENT", "start_offset": 47, "end_offset": 71}, {"id": 1095814, "label": "ENT", "start_offset": 128, "end_offset": 139}, {"id": 1095815, "label": "ENT", "start_offset": 154, "end_offset": 165}, {"id": 1095816, "label": "ENT", "start_offset": 307, "end_offset": 314}, {"id": 1095817, "label": "ENT", "start_offset": 333, "end_offset": 350}, {"id": 1095818, "label": "ENT", "start_offset": 381, "end_offset": 390}, {"id": 1095819, "label": "ENT", "start_offset": 414, "end_offset": 425}, {"id": 1095820, "label": "ENT", "start_offset": 431, "end_offset": 439}, {"id": 1095821, "label": "ENT", "start_offset": 465, "end_offset": 493}, {"id": 1095822, "label": "ENT", "start_offset": 506, "end_offset": 522}, {"id": 1095823, "label": "ENT", "start_offset": 531, "end_offset": 537}, {"id": 1095824, "label": "ENT", "start_offset": 549, "end_offset": 586}, {"id": 1095825, "label": "ENT", "start_offset": 593, "end_offset": 623}, {"id": 1095826, "label": "ENT", "start_offset": 625, "end_offset": 645}, {"id": 1095827, "label": "ENT", "start_offset": 679, "end_offset": 685}, {"id": 1095828, "label": "ENT", "start_offset": 733, "end_offset": 741}, {"id": 1095829, "label": "ENT", "start_offset": 791, "end_offset": 797}, {"id": 1095830, "label": "ENT", "start_offset": 799, "end_offset": 808}, {"id": 1095831, "label": "ENT", "start_offset": 821, "end_offset": 845}, {"id": 1095832, "label": "ENT", "start_offset": 871, "end_offset": 878}, {"id": 1095833, "label": "ENT", "start_offset": 972, "end_offset": 990}, {"id": 1095834, "label": "ENT", "start_offset": 1005, "end_offset": 1011}, {"id": 1095835, "label": "ENT", "start_offset": 1024, "end_offset": 1042}, {"id": 1095836, "label": "ENT", "start_offset": 1047, "end_offset": 1064}, {"id": 1095837, "label": "ENT", "start_offset": 1108, "end_offset": 1113}, {"id": 1095838, "label": "ENT", "start_offset": 1134, "end_offset": 1143}, {"id": 1095839, "label": "ENT", "start_offset": 1158, "end_offset": 1180}, {"id": 1095840, "label": "ENT", "start_offset": 1189, "end_offset": 1201}, {"id": 1095841, "label": "ENT", "start_offset": 1224, "end_offset": 1233}], "relations": [{"id": 178410, "from_id": 1095812, "to_id": 1095813, "type": "HYPONYM-OF"}, {"id": 178411, "from_id": 1095821, "to_id": 1095822, "type": "USED-FOR"}, {"id": 178412, "from_id": 1095827, "to_id": 1095826, "type": "USED-FOR"}, {"id": 178413, "from_id": 1095834, "to_id": 1095835, "type": "COMPARE"}, {"id": 178414, "from_id": 1095834, "to_id": 1095836, "type": "USED-FOR"}, {"id": 178415, "from_id": 1095835, "to_id": 1095836, "type": "USED-FOR"}, {"id": 178416, "from_id": 1095821, "to_id": 1095820, "type": "USED-FOR"}, {"id": 178417, "from_id": 1095824, "to_id": 1095821, "type": "PART-OF"}, {"id": 178418, "from_id": 1095825, "to_id": 1095821, "type": "PART-OF"}, {"id": 178419, "from_id": 1095824, "to_id": 1095825, "type": "CONJUNCTION"}, {"id": 178420, "from_id": 1095822, "to_id": 1095823, "type": "USED-FOR"}, {"id": 178421, "from_id": 1095823, "to_id": 1095817, "type": "COREF"}, {"id": 178422, "from_id": 1095816, "to_id": 1095812, "type": "COREF"}, {"id": 178423, "from_id": 1095815, "to_id": 1095814, "type": "COREF"}, {"id": 178424, "from_id": 1095826, "to_id": 1095812, "type": "COREF"}, {"id": 178425, "from_id": 1095831, "to_id": 1095830, "type": "USED-FOR"}, {"id": 178426, "from_id": 1095833, "to_id": 1095834, "type": "EVALUATE-FOR"}, {"id": 178427, "from_id": 1095836, "to_id": 1095817, "type": "COREF"}, {"id": 178428, "from_id": 1095834, "to_id": 1095820, "type": "COREF"}, {"id": 178429, "from_id": 1095837, "to_id": 1095834, "type": "COREF"}, {"id": 178430, "from_id": 1095838, "to_id": 1095837, "type": "COREF"}, {"id": 178431, "from_id": 1095840, "to_id": 1095839, "type": "HYPONYM-OF"}, {"id": 178432, "from_id": 1095838, "to_id": 1095839, "type": "USED-FOR"}, {"id": 178433, "from_id": 1095832, "to_id": 1095830, "type": "USED-FOR"}]}
{"id": "P81-1033", "text": " A  flexible parser  can deal with input that deviates from its  grammar , in addition to input that conforms to it. Ideally, such a  parser  will correct the deviant input: sometimes, it will be unable to correct it at all; at other times,  correction  will be possible, but only to within a range of ambiguous possibilities. This paper is concerned with such ambiguous situations, and with making it as easy as possible for the  ambiguity  to be resolved through consultation with the user of the  parser  - we presume interactive use. We show the importance of asking the user for clarification in as focused a way as possible.  Focused interaction  of this kind is facilitated by a  construction-specific approach  to  flexible parsing , with  specialized parsing techniques  for each type of  construction , and specialized  ambiguity representations  for each type of  ambiguity  that a particular  construction  can give rise to. A  construction-specific approach  also aids in  task-specific language development  by allowing a  language definition  that is natural in terms of the  task domain  to be interpreted directly without compilation into a  uniform grammar formalism , thus greatly speeding the  testing  of changes to the  language definition . ", "Comments": [], "entities": [{"id": 1095842, "label": "ENT", "start_offset": 4, "end_offset": 19}, {"id": 1095843, "label": "ENT", "start_offset": 113, "end_offset": 115}, {"id": 1095844, "label": "ENT", "start_offset": 134, "end_offset": 140}, {"id": 1095845, "label": "ENT", "start_offset": 431, "end_offset": 440}, {"id": 1095846, "label": "ENT", "start_offset": 500, "end_offset": 506}, {"id": 1095847, "label": "ENT", "start_offset": 687, "end_offset": 717}, {"id": 1095848, "label": "ENT", "start_offset": 723, "end_offset": 739}, {"id": 1095849, "label": "ENT", "start_offset": 748, "end_offset": 778}, {"id": 1095850, "label": "ENT", "start_offset": 798, "end_offset": 810}, {"id": 1095851, "label": "ENT", "start_offset": 830, "end_offset": 855}, {"id": 1095852, "label": "ENT", "start_offset": 875, "end_offset": 884}, {"id": 1095853, "label": "ENT", "start_offset": 905, "end_offset": 917}, {"id": 1095854, "label": "ENT", "start_offset": 940, "end_offset": 970}, {"id": 1095855, "label": "ENT", "start_offset": 986, "end_offset": 1020}, {"id": 1095856, "label": "ENT", "start_offset": 1159, "end_offset": 1184}], "relations": [{"id": 178434, "from_id": 1095847, "to_id": 1095848, "type": "USED-FOR"}, {"id": 178435, "from_id": 1095849, "to_id": 1095850, "type": "USED-FOR"}, {"id": 178436, "from_id": 1095851, "to_id": 1095852, "type": "USED-FOR"}, {"id": 178437, "from_id": 1095854, "to_id": 1095855, "type": "USED-FOR"}, {"id": 178438, "from_id": 1095842, "to_id": 1095843, "type": "COREF"}, {"id": 178439, "from_id": 1095842, "to_id": 1095844, "type": "COREF"}, {"id": 178440, "from_id": 1095844, "to_id": 1095846, "type": "COREF"}, {"id": 178441, "from_id": 1095847, "to_id": 1095849, "type": "CONJUNCTION"}, {"id": 178442, "from_id": 1095849, "to_id": 1095851, "type": "CONJUNCTION"}]}
{"id": "P05-3030", "text": " We propose a method of organizing reading materials for  vocabulary learning . It enables us to select a concise set of reading  texts  (from a  target corpus ) that contains all the  target vocabulary  to be learned. We used a specialized  vocabulary  for an English certification test as the  target vocabulary  and used  English Wikipedia , a free-content encyclopedia, as the  target corpus . The organized reading materials would enable learners not only to study the  target vocabulary  efficiently but also to gain a variety of knowledge through reading. The reading materials are available on our web site. ", "Comments": [], "entities": [{"id": 1095857, "label": "ENT", "start_offset": 14, "end_offset": 20}, {"id": 1095858, "label": "ENT", "start_offset": 24, "end_offset": 52}, {"id": 1095859, "label": "ENT", "start_offset": 58, "end_offset": 77}, {"id": 1095860, "label": "ENT", "start_offset": 80, "end_offset": 82}, {"id": 1095861, "label": "ENT", "start_offset": 185, "end_offset": 202}, {"id": 1095862, "label": "ENT", "start_offset": 325, "end_offset": 342}, {"id": 1095863, "label": "ENT", "start_offset": 347, "end_offset": 372}], "relations": [{"id": 178443, "from_id": 1095857, "to_id": 1095858, "type": "USED-FOR"}, {"id": 178444, "from_id": 1095858, "to_id": 1095859, "type": "USED-FOR"}, {"id": 178445, "from_id": 1095857, "to_id": 1095860, "type": "COREF"}, {"id": 178446, "from_id": 1095862, "to_id": 1095863, "type": "HYPONYM-OF"}]}
{"id": "P06-1112", "text": " In this paper, we explore correlation of  dependency relation paths  to rank candidate answers in  answer extraction . Using the  correlation measure , we compare  dependency relations  of a candidate answer and mapped  question phrases  in  sentence  with the corresponding  relations  in question. Different from previous studies, we propose an  approximate phrase mapping algorithm  and incorporate the  mapping score  into the  correlation measure . The correlations are further incorporated into a  Maximum Entropy-based ranking model  which estimates  path weights  from training. Experimental results show that our method significantly outperforms state-of-the-art  syntactic relation-based methods  by up to 20% in  MRR . ", "Comments": [], "entities": [{"id": 1095864, "label": "ENT", "start_offset": 27, "end_offset": 68}, {"id": 1095865, "label": "ENT", "start_offset": 100, "end_offset": 117}, {"id": 1095866, "label": "ENT", "start_offset": 131, "end_offset": 150}, {"id": 1095867, "label": "ENT", "start_offset": 165, "end_offset": 185}, {"id": 1095868, "label": "ENT", "start_offset": 349, "end_offset": 385}, {"id": 1095869, "label": "ENT", "start_offset": 408, "end_offset": 421}, {"id": 1095870, "label": "ENT", "start_offset": 433, "end_offset": 452}, {"id": 1095871, "label": "ENT", "start_offset": 459, "end_offset": 471}, {"id": 1095872, "label": "ENT", "start_offset": 505, "end_offset": 540}, {"id": 1095873, "label": "ENT", "start_offset": 623, "end_offset": 629}, {"id": 1095874, "label": "ENT", "start_offset": 674, "end_offset": 706}, {"id": 1095875, "label": "ENT", "start_offset": 725, "end_offset": 728}], "relations": [{"id": 178447, "from_id": 1095869, "to_id": 1095870, "type": "PART-OF"}, {"id": 178448, "from_id": 1095864, "to_id": 1095865, "type": "USED-FOR"}, {"id": 178449, "from_id": 1095866, "to_id": 1095867, "type": "USED-FOR"}, {"id": 178450, "from_id": 1095864, "to_id": 1095871, "type": "COREF"}, {"id": 178451, "from_id": 1095871, "to_id": 1095872, "type": "PART-OF"}, {"id": 178452, "from_id": 1095873, "to_id": 1095874, "type": "COMPARE"}, {"id": 178453, "from_id": 1095875, "to_id": 1095874, "type": "EVALUATE-FOR"}, {"id": 178454, "from_id": 1095875, "to_id": 1095873, "type": "EVALUATE-FOR"}, {"id": 178455, "from_id": 1095868, "to_id": 1095873, "type": "COREF"}]}
{"id": "P03-1050", "text": " This paper presents an  unsupervised learning approach  to building a  non-English (Arabic) stemmer  . The  stemming model  is based on  statistical machine translation  and it uses an  English stemmer  and a small (10K sentences)  parallel corpus  as its sole  training resources  . No  parallel text  is needed after the  training phase  .  Monolingual, unannotated text  can be used to further improve the  stemmer  by allowing it to adapt to a desired  domain  or  genre  . Examples and results will be given for  Arabic  , but the approach is applicable to any  language  that needs  affix removal  . Our  resource-frugal approach  results in 87.5%  agreement  with a state of the art, proprietary  Arabic stemmer  built using  rules  ,  affix lists  , and  human annotated text  , in addition to an  unsupervised component  .  Task-based evaluation  using  Arabic information retrieval  indicates an improvement of 22-38% in  average precision  over  unstemmed text  , and 96% of the performance of the proprietary  stemmer  above. ", "Comments": [], "entities": [{"id": 1095954, "label": "ENT", "start_offset": 25, "end_offset": 55}, {"id": 1095955, "label": "ENT", "start_offset": 72, "end_offset": 100}, {"id": 1095956, "label": "ENT", "start_offset": 109, "end_offset": 123}, {"id": 1095957, "label": "ENT", "start_offset": 138, "end_offset": 169}, {"id": 1095958, "label": "ENT", "start_offset": 175, "end_offset": 177}, {"id": 1095959, "label": "ENT", "start_offset": 187, "end_offset": 202}, {"id": 1095960, "label": "ENT", "start_offset": 233, "end_offset": 248}, {"id": 1095961, "label": "ENT", "start_offset": 289, "end_offset": 302}, {"id": 1095962, "label": "ENT", "start_offset": 344, "end_offset": 373}, {"id": 1095963, "label": "ENT", "start_offset": 411, "end_offset": 418}, {"id": 1095964, "label": "ENT", "start_offset": 432, "end_offset": 434}, {"id": 1095965, "label": "ENT", "start_offset": 519, "end_offset": 525}, {"id": 1095966, "label": "ENT", "start_offset": 537, "end_offset": 545}, {"id": 1095967, "label": "ENT", "start_offset": 590, "end_offset": 603}, {"id": 1095968, "label": "ENT", "start_offset": 612, "end_offset": 636}, {"id": 1095969, "label": "ENT", "start_offset": 656, "end_offset": 665}, {"id": 1095970, "label": "ENT", "start_offset": 705, "end_offset": 719}, {"id": 1095971, "label": "ENT", "start_offset": 734, "end_offset": 739}, {"id": 1095972, "label": "ENT", "start_offset": 744, "end_offset": 755}, {"id": 1095973, "label": "ENT", "start_offset": 764, "end_offset": 784}, {"id": 1095974, "label": "ENT", "start_offset": 807, "end_offset": 829}, {"id": 1095975, "label": "ENT", "start_offset": 834, "end_offset": 855}, {"id": 1095976, "label": "ENT", "start_offset": 864, "end_offset": 892}, {"id": 1095977, "label": "ENT", "start_offset": 933, "end_offset": 950}, {"id": 1095978, "label": "ENT", "start_offset": 958, "end_offset": 972}, {"id": 1095979, "label": "ENT", "start_offset": 1023, "end_offset": 1030}], "relations": [{"id": 178516, "from_id": 1095954, "to_id": 1095955, "type": "USED-FOR"}, {"id": 178517, "from_id": 1095957, "to_id": 1095956, "type": "USED-FOR"}, {"id": 178518, "from_id": 1095956, "to_id": 1095958, "type": "COREF"}, {"id": 178519, "from_id": 1095959, "to_id": 1095958, "type": "USED-FOR"}, {"id": 178520, "from_id": 1095960, "to_id": 1095958, "type": "USED-FOR"}, {"id": 178521, "from_id": 1095962, "to_id": 1095963, "type": "USED-FOR"}, {"id": 178522, "from_id": 1095963, "to_id": 1095964, "type": "COREF"}, {"id": 178523, "from_id": 1095955, "to_id": 1095956, "type": "COREF"}, {"id": 178524, "from_id": 1095958, "to_id": 1095963, "type": "COREF"}, {"id": 178525, "from_id": 1095954, "to_id": 1095966, "type": "COREF"}, {"id": 178526, "from_id": 1095971, "to_id": 1095970, "type": "USED-FOR"}, {"id": 178527, "from_id": 1095972, "to_id": 1095970, "type": "USED-FOR"}, {"id": 178528, "from_id": 1095973, "to_id": 1095970, "type": "USED-FOR"}, {"id": 178529, "from_id": 1095971, "to_id": 1095972, "type": "CONJUNCTION"}, {"id": 178530, "from_id": 1095972, "to_id": 1095973, "type": "CONJUNCTION"}, {"id": 178531, "from_id": 1095974, "to_id": 1095970, "type": "USED-FOR"}, {"id": 178532, "from_id": 1095973, "to_id": 1095974, "type": "CONJUNCTION"}, {"id": 178533, "from_id": 1095968, "to_id": 1095970, "type": "COMPARE"}, {"id": 178534, "from_id": 1095969, "to_id": 1095970, "type": "EVALUATE-FOR"}, {"id": 178535, "from_id": 1095969, "to_id": 1095968, "type": "EVALUATE-FOR"}, {"id": 178536, "from_id": 1095966, "to_id": 1095968, "type": "COREF"}, {"id": 178537, "from_id": 1095970, "to_id": 1095979, "type": "COREF"}, {"id": 178538, "from_id": 1095976, "to_id": 1095975, "type": "USED-FOR"}, {"id": 178539, "from_id": 1095977, "to_id": 1095975, "type": "EVALUATE-FOR"}, {"id": 178540, "from_id": 1095977, "to_id": 1095978, "type": "EVALUATE-FOR"}]}
{"id": "CVPR_2009_21_abs", "text": "Many computer vision applications, such as image classification and video indexing, are usually multi-label classification problems in which an instance can be assigned to more than one category. In this paper, we present a novel multi-label classification approach with hypergraph regu-larization that addresses the correlations among different categories. First, a hypergraph is constructed to capture the correlations among different categories, in which each vertex represents one training instance and each hyperedge for one category contains all the instances belonging to the same category. Then, an improved SVM like learning system incorporating the hypergraph regularization, called Rank-HLapSVM, is proposed to handle the multi-label classification problems. We find that the corresponding optimization problem can be efficiently solved by the dual coordinate descent method. Many promising experimental results on the real datasets including ImageCLEF and Me-diaMill demonstrate the effectiveness and efficiency of the proposed algorithm.", "Comments": [], "entities": [{"id": 1095980, "label": "ENT", "start_offset": 5, "end_offset": 33}, {"id": 1095981, "label": "ENT", "start_offset": 43, "end_offset": 63}, {"id": 1095982, "label": "ENT", "start_offset": 68, "end_offset": 82}, {"id": 1095983, "label": "ENT", "start_offset": 96, "end_offset": 131}, {"id": 1095984, "label": "ENT", "start_offset": 230, "end_offset": 265}, {"id": 1095985, "label": "ENT", "start_offset": 271, "end_offset": 297}, {"id": 1095986, "label": "ENT", "start_offset": 367, "end_offset": 377}, {"id": 1095987, "label": "ENT", "start_offset": 616, "end_offset": 640}, {"id": 1095988, "label": "ENT", "start_offset": 659, "end_offset": 684}, {"id": 1095989, "label": "ENT", "start_offset": 693, "end_offset": 705}, {"id": 1095990, "label": "ENT", "start_offset": 733, "end_offset": 768}, {"id": 1095991, "label": "ENT", "start_offset": 801, "end_offset": 821}, {"id": 1095992, "label": "ENT", "start_offset": 855, "end_offset": 885}, {"id": 1095993, "label": "ENT", "start_offset": 930, "end_offset": 943}, {"id": 1095994, "label": "ENT", "start_offset": 954, "end_offset": 963}, {"id": 1095995, "label": "ENT", "start_offset": 968, "end_offset": 978}, {"id": 1095996, "label": "ENT", "start_offset": 1040, "end_offset": 1049}], "relations": [{"id": 178541, "from_id": 1095981, "to_id": 1095982, "type": "CONJUNCTION"}, {"id": 178542, "from_id": 1095981, "to_id": 1095980, "type": "HYPONYM-OF"}, {"id": 178543, "from_id": 1095982, "to_id": 1095980, "type": "HYPONYM-OF"}, {"id": 178544, "from_id": 1095983, "to_id": 1095980, "type": "USED-FOR"}, {"id": 178545, "from_id": 1095985, "to_id": 1095984, "type": "FEATURE-OF"}, {"id": 178546, "from_id": 1095988, "to_id": 1095987, "type": "PART-OF"}, {"id": 178547, "from_id": 1095989, "to_id": 1095987, "type": "HYPONYM-OF"}, {"id": 178548, "from_id": 1095987, "to_id": 1095990, "type": "USED-FOR"}, {"id": 178549, "from_id": 1095990, "to_id": 1095983, "type": "COREF"}, {"id": 178550, "from_id": 1095984, "to_id": 1095987, "type": "COREF"}, {"id": 178551, "from_id": 1095992, "to_id": 1095991, "type": "USED-FOR"}, {"id": 178552, "from_id": 1095987, "to_id": 1095996, "type": "COREF"}, {"id": 178553, "from_id": 1095994, "to_id": 1095993, "type": "HYPONYM-OF"}, {"id": 178554, "from_id": 1095995, "to_id": 1095993, "type": "HYPONYM-OF"}, {"id": 178555, "from_id": 1095994, "to_id": 1095995, "type": "CONJUNCTION"}, {"id": 178556, "from_id": 1095993, "to_id": 1095996, "type": "EVALUATE-FOR"}]}
{"id": "ICCV_2015_50_abs", "text": "Detecting fine-grained subtle changes among a scene is critically important in practice. Previous change detection methods, focusing on detecting large-scale significant changes, cannot do this well. This paper proposes a feasible end-to-end approach to this challenging problem. We start from active camera relocation that quickly relocates camera to nearly the same pose and position of the last time observation. To guarantee detection sensitivity and accuracy of minute changes, in an observation, we capture a group of images under multiple illuminations, which need only to be roughly aligned to the last time lighting conditions. Given two times observations, we formulate fine-grained change detection as a joint optimization problem of three related factors, i.e., normal-aware lighting difference, camera geometry correction flow, and real scene change mask. We solve the three factors in a coarse-to-fine manner and achieve reliable change decision by rank minimization. We build three real-world datasets to benchmark fine-grained change detection of misaligned scenes under varied multiple lighting conditions. Extensive experiments show the superior performance of our approach over state-of-the-art change detection methods and its ability to distinguish real scene changes from false ones caused by lighting variations.", "Comments": [], "entities": [{"id": 1095997, "label": "ENT", "start_offset": 0, "end_offset": 51}, {"id": 1095998, "label": "ENT", "start_offset": 98, "end_offset": 122}, {"id": 1095999, "label": "ENT", "start_offset": 136, "end_offset": 177}, {"id": 1096000, "label": "ENT", "start_offset": 231, "end_offset": 250}, {"id": 1096001, "label": "ENT", "start_offset": 271, "end_offset": 278}, {"id": 1096002, "label": "ENT", "start_offset": 294, "end_offset": 318}, {"id": 1096003, "label": "ENT", "start_offset": 429, "end_offset": 450}, {"id": 1096004, "label": "ENT", "start_offset": 455, "end_offset": 463}, {"id": 1096005, "label": "ENT", "start_offset": 546, "end_offset": 559}, {"id": 1096006, "label": "ENT", "start_offset": 680, "end_offset": 709}, {"id": 1096007, "label": "ENT", "start_offset": 715, "end_offset": 741}, {"id": 1096008, "label": "ENT", "start_offset": 759, "end_offset": 766}, {"id": 1096009, "label": "ENT", "start_offset": 774, "end_offset": 806}, {"id": 1096010, "label": "ENT", "start_offset": 808, "end_offset": 839}, {"id": 1096011, "label": "ENT", "start_offset": 845, "end_offset": 867}, {"id": 1096012, "label": "ENT", "start_offset": 888, "end_offset": 895}, {"id": 1096013, "label": "ENT", "start_offset": 901, "end_offset": 922}, {"id": 1096014, "label": "ENT", "start_offset": 944, "end_offset": 959}, {"id": 1096015, "label": "ENT", "start_offset": 963, "end_offset": 980}, {"id": 1096016, "label": "ENT", "start_offset": 997, "end_offset": 1016}, {"id": 1096017, "label": "ENT", "start_offset": 1030, "end_offset": 1080}, {"id": 1096018, "label": "ENT", "start_offset": 1087, "end_offset": 1122}, {"id": 1096019, "label": "ENT", "start_offset": 1183, "end_offset": 1191}, {"id": 1096020, "label": "ENT", "start_offset": 1214, "end_offset": 1238}, {"id": 1096021, "label": "ENT", "start_offset": 1270, "end_offset": 1288}, {"id": 1096022, "label": "ENT", "start_offset": 1315, "end_offset": 1334}], "relations": [{"id": 178557, "from_id": 1095998, "to_id": 1095999, "type": "USED-FOR"}, {"id": 178558, "from_id": 1095999, "to_id": 1096001, "type": "COREF"}, {"id": 178559, "from_id": 1096000, "to_id": 1096001, "type": "USED-FOR"}, {"id": 178560, "from_id": 1096007, "to_id": 1096006, "type": "USED-FOR"}, {"id": 178561, "from_id": 1096009, "to_id": 1096008, "type": "HYPONYM-OF"}, {"id": 178562, "from_id": 1096010, "to_id": 1096008, "type": "HYPONYM-OF"}, {"id": 178563, "from_id": 1096011, "to_id": 1096008, "type": "HYPONYM-OF"}, {"id": 178564, "from_id": 1096008, "to_id": 1096007, "type": "FEATURE-OF"}, {"id": 178565, "from_id": 1096008, "to_id": 1096012, "type": "COREF"}, {"id": 178566, "from_id": 1096013, "to_id": 1096012, "type": "USED-FOR"}, {"id": 178567, "from_id": 1096015, "to_id": 1096014, "type": "USED-FOR"}, {"id": 178568, "from_id": 1096018, "to_id": 1096017, "type": "FEATURE-OF"}, {"id": 178569, "from_id": 1096016, "to_id": 1096017, "type": "EVALUATE-FOR"}, {"id": 178570, "from_id": 1096019, "to_id": 1096020, "type": "COMPARE"}, {"id": 178571, "from_id": 1096019, "to_id": 1096021, "type": "USED-FOR"}, {"id": 178572, "from_id": 1096009, "to_id": 1096010, "type": "CONJUNCTION"}, {"id": 178573, "from_id": 1096010, "to_id": 1096011, "type": "CONJUNCTION"}, {"id": 178574, "from_id": 1096000, "to_id": 1096019, "type": "COREF"}]}
{"id": "L08-1540", "text": " In this paper we deal with a recently developed  large Czech MWE database  containing at the moment 160 000  MWEs  (treated as  lexical units ). It was compiled from various resources such as  encyclopedias  and  dictionaries , public  databases  of  proper names  and  toponyms ,  collocations  obtained from  Czech WordNet , lists of  botanical and zoological terms  and others. We describe the structure of the  database  and give basic types of  MWEs  according to domains they belong to. We compare the built  MWEs database  with the  corpus data  from  Czech National Corpus  (approx. 100 mil. tokens) and present results of this comparison in the paper. These  MWEs  have not been obtained from the  corpus  since their frequencies in it are rather low. To obtain a more complete list of  MWEs  we propose and use a technique exploiting the  Word Sketch Engine , which allows us to work with  statistical parameters  such as frequency of  MWEs  and their components as well as with the  salience  for the whole  MWEs . We also discuss exploitation of the  database  for working out a more adequate  tagging  and  lemmatization . The final goal is to be able to recognize  MWEs  in  corpus text  and lemmatize them as complete  lexical units , i. e. to make  tagging  and  lemmatization  more adequate. ", "Comments": [], "entities": [{"id": 1096023, "label": "ENT", "start_offset": 50, "end_offset": 74}, {"id": 1096024, "label": "ENT", "start_offset": 110, "end_offset": 114}, {"id": 1096025, "label": "ENT", "start_offset": 129, "end_offset": 142}, {"id": 1096026, "label": "ENT", "start_offset": 146, "end_offset": 148}, {"id": 1096027, "label": "ENT", "start_offset": 194, "end_offset": 207}, {"id": 1096028, "label": "ENT", "start_offset": 214, "end_offset": 226}, {"id": 1096029, "label": "ENT", "start_offset": 229, "end_offset": 279}, {"id": 1096030, "label": "ENT", "start_offset": 283, "end_offset": 295}, {"id": 1096031, "label": "ENT", "start_offset": 312, "end_offset": 325}, {"id": 1096032, "label": "ENT", "start_offset": 328, "end_offset": 368}, {"id": 1096033, "label": "ENT", "start_offset": 416, "end_offset": 424}, {"id": 1096034, "label": "ENT", "start_offset": 451, "end_offset": 455}, {"id": 1096035, "label": "ENT", "start_offset": 516, "end_offset": 529}, {"id": 1096036, "label": "ENT", "start_offset": 560, "end_offset": 581}, {"id": 1096037, "label": "ENT", "start_offset": 669, "end_offset": 673}, {"id": 1096038, "label": "ENT", "start_offset": 708, "end_offset": 714}, {"id": 1096039, "label": "ENT", "start_offset": 797, "end_offset": 801}, {"id": 1096040, "label": "ENT", "start_offset": 824, "end_offset": 833}, {"id": 1096041, "label": "ENT", "start_offset": 850, "end_offset": 868}, {"id": 1096042, "label": "ENT", "start_offset": 901, "end_offset": 923}, {"id": 1096043, "label": "ENT", "start_offset": 947, "end_offset": 951}, {"id": 1096044, "label": "ENT", "start_offset": 1020, "end_offset": 1024}, {"id": 1096045, "label": "ENT", "start_offset": 1064, "end_offset": 1072}, {"id": 1096046, "label": "ENT", "start_offset": 1107, "end_offset": 1114}, {"id": 1096047, "label": "ENT", "start_offset": 1121, "end_offset": 1134}, {"id": 1096048, "label": "ENT", "start_offset": 1180, "end_offset": 1184}, {"id": 1096049, "label": "ENT", "start_offset": 1217, "end_offset": 1221}, {"id": 1096050, "label": "ENT", "start_offset": 1235, "end_offset": 1248}, {"id": 1096051, "label": "ENT", "start_offset": 1266, "end_offset": 1273}, {"id": 1096052, "label": "ENT", "start_offset": 1280, "end_offset": 1293}], "relations": [{"id": 178575, "from_id": 1096031, "to_id": 1096030, "type": "COREF"}, {"id": 178576, "from_id": 1096042, "to_id": 1096041, "type": "FEATURE-OF"}, {"id": 178577, "from_id": 1096045, "to_id": 1096046, "type": "USED-FOR"}, {"id": 178578, "from_id": 1096026, "to_id": 1096023, "type": "COREF"}, {"id": 178579, "from_id": 1096027, "to_id": 1096026, "type": "USED-FOR"}, {"id": 178580, "from_id": 1096028, "to_id": 1096026, "type": "USED-FOR"}, {"id": 178581, "from_id": 1096030, "to_id": 1096026, "type": "USED-FOR"}, {"id": 178582, "from_id": 1096027, "to_id": 1096028, "type": "CONJUNCTION"}, {"id": 178583, "from_id": 1096033, "to_id": 1096026, "type": "COREF"}, {"id": 178584, "from_id": 1096035, "to_id": 1096033, "type": "COREF"}, {"id": 178585, "from_id": 1096038, "to_id": 1096036, "type": "COREF"}, {"id": 178586, "from_id": 1096039, "to_id": 1096037, "type": "COREF"}, {"id": 178587, "from_id": 1096040, "to_id": 1096041, "type": "USED-FOR"}, {"id": 178588, "from_id": 1096044, "to_id": 1096043, "type": "COREF"}, {"id": 178589, "from_id": 1096046, "to_id": 1096047, "type": "CONJUNCTION"}, {"id": 178590, "from_id": 1096045, "to_id": 1096047, "type": "USED-FOR"}, {"id": 178591, "from_id": 1096049, "to_id": 1096048, "type": "COREF"}, {"id": 178592, "from_id": 1096037, "to_id": 1096034, "type": "COREF"}, {"id": 178593, "from_id": 1096045, "to_id": 1096035, "type": "COREF"}, {"id": 178594, "from_id": 1096048, "to_id": 1096051, "type": "USED-FOR"}, {"id": 178595, "from_id": 1096048, "to_id": 1096052, "type": "USED-FOR"}, {"id": 178596, "from_id": 1096051, "to_id": 1096052, "type": "CONJUNCTION"}, {"id": 178597, "from_id": 1096029, "to_id": 1096026, "type": "USED-FOR"}, {"id": 178598, "from_id": 1096032, "to_id": 1096026, "type": "USED-FOR"}, {"id": 178599, "from_id": 1096032, "to_id": 1096030, "type": "CONJUNCTION"}, {"id": 178600, "from_id": 1096036, "to_id": 1096035, "type": "USED-FOR"}, {"id": 178601, "from_id": 1096051, "to_id": 1096046, "type": "COREF"}, {"id": 178602, "from_id": 1096052, "to_id": 1096047, "type": "COREF"}]}
{"id": "N04-1008", "text": " In this paper we describe and evaluate a  Question Answering system  that goes beyond answering factoid questions. We focus on  FAQ-like questions and answers  , and build our system around a  noisy-channel architecture  which exploits both a  language model  for  answers  and a  transformation model  for  answer/question terms , trained on a  corpus  of 1 million  question/answer pairs  collected from the Web. ", "Comments": [], "entities": [{"id": 1096053, "label": "ENT", "start_offset": 43, "end_offset": 68}, {"id": 1096054, "label": "ENT", "start_offset": 129, "end_offset": 159}, {"id": 1096055, "label": "ENT", "start_offset": 177, "end_offset": 183}, {"id": 1096056, "label": "ENT", "start_offset": 194, "end_offset": 220}, {"id": 1096057, "label": "ENT", "start_offset": 245, "end_offset": 259}, {"id": 1096058, "label": "ENT", "start_offset": 282, "end_offset": 302}, {"id": 1096059, "label": "ENT", "start_offset": 411, "end_offset": 414}], "relations": [{"id": 178603, "from_id": 1096055, "to_id": 1096053, "type": "COREF"}, {"id": 178604, "from_id": 1096055, "to_id": 1096054, "type": "USED-FOR"}, {"id": 178605, "from_id": 1096056, "to_id": 1096057, "type": "USED-FOR"}, {"id": 178606, "from_id": 1096056, "to_id": 1096058, "type": "USED-FOR"}, {"id": 178607, "from_id": 1096056, "to_id": 1096055, "type": "USED-FOR"}]}
